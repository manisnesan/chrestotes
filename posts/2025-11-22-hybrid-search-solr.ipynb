{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yaml-frontmatter",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Hybrid Search in Apache Solr - Learning Notes\"\n",
    "date: \"2025-11-22\"\n",
    "categories:\n",
    "  - information-retrieval\n",
    "  - apache-solr\n",
    "  - search\n",
    "description: \"Understanding hybrid search and reranking strategies in Apache Solr\"\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f2915",
   "metadata": {
    "time_run": "2025-11-22T14:15:11.013320+00:00"
   },
   "source": [
    "# Hybrid Search in Apache Solr - Learning Notes\n",
    "\n",
    "## What is This About?\n",
    "\n",
    "This post explores **hybrid search** and **reranking** in Apache Solr. If you're new to these concepts:\n",
    "\n",
    "- **Keyword search** (also called lexical search) finds documents by matching exact words or phrases\n",
    "- **Vector search** (also called semantic search) finds documents by understanding meaning and similarity\n",
    "- **Hybrid search** combines both approaches to get the best of both worlds\n",
    "- **Reranking** is a technique where you first retrieve candidates using one method, then reorder them using another method\n",
    "\n",
    "## Why Reranking Matters\n",
    "\n",
    "Imagine you're searching for \"how to fix memory leaks in Kubernetes\". \n",
    "\n",
    "- **Keyword search alone** might miss relevant docs that use different terminology (e.g., \"memory management\" instead of \"memory leaks\")\n",
    "- **Vector search alone** might return semantically similar but irrelevant docs (e.g., general memory management articles)\n",
    "- **Reranking** lets you use keyword search to find relevant candidates, then use vector search to surface the most semantically relevant ones\n",
    "\n",
    "## Context & Goal\n",
    "- **Background:** Search practitioner, intermediate Python coder, familiar with lexical search in Solr\n",
    "- **Goal:** Understand hybrid search and re-ranking features in Solr\n",
    "- **Application:** Lightspeed core implementation for OpenShift documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1415f6",
   "metadata": {},
   "source": [
    "## Understanding the Reranking Approach\n",
    "\n",
    "This implementation uses a **keyword-first hybrid search** strategy. Let's break down what that means and how it works.\n",
    "\n",
    "### The Two-Stage Process\n",
    "\n",
    "#### Stage 1: Keyword Retrieval (Broad Cast)\n",
    "\n",
    "- Use traditional keyword search to find candidate documents\n",
    "- Retrieve `k*2` documents (twice as many as you need)\n",
    "- This acts as a filter: only documents matching your keywords are considered\n",
    "\n",
    "#### Stage 2: Semantic Reranking (Refinement)\n",
    "\n",
    "- Take those `k*2` candidates from Stage 1\n",
    "- Use vector/semantic similarity to reorder them\n",
    "- Return the top `k` documents based on the combined score\n",
    "\n",
    "### High-Level Flow\n",
    "```\n",
    "User Query: \"how to deploy nodejs on openshift\"\n",
    "    ↓\n",
    "Stage 1: Keyword Search\n",
    "    → Find top k*2 documents matching \"deploy\", \"nodejs\", \"openshift\"\n",
    "    → Example: Gets 20 documents (if k=10)\n",
    "    ↓\n",
    "Stage 2: Semantic Reranking  \n",
    "    → Calculate semantic similarity for those 20 documents\n",
    "    → Reorder by combining keyword score + semantic score\n",
    "    ↓\n",
    "Final Results: Top k documents (10 in this case)\n",
    "```\n",
    "\n",
    "### Why Retrieve k*2 First?\n",
    "\n",
    "Retrieving `k*2` candidates gives the reranker a larger pool to work with. This is important because:\n",
    "\n",
    "- The keyword search might rank documents highly that aren't semantically the best match\n",
    "- The reranker can \"rescue\" semantically relevant documents that ranked lower in keyword search\n",
    "- It's a balance: too few candidates = missed opportunities, too many = slower performance\n",
    "\n",
    "### Librarian Analogy\n",
    "\n",
    "Imagine you're asking two librarians to help you find books:\n",
    "\n",
    "- **Librarian #1 (Keyword Search):** \n",
    "  - You ask: \"Find books about deploying applications\"\n",
    "  - They search the catalog by keywords and bring you 20 books\n",
    "  - They put them on a table, roughly sorted by how many times \"deploy\" and \"application\" appear\n",
    "  \n",
    "- **Librarian #2 (Vector Reranker):**\n",
    "  - Takes those same 20 books from the table\n",
    "  - Reads through them to understand the actual content and meaning\n",
    "  - Reorders them based on how well they match what you're really looking for\n",
    "  - Gives you the top 10 most relevant books\n",
    "\n",
    "The key insight: Librarian #2 can only work with what Librarian #1 found. If a book doesn't match the keywords, it never makes it to the table.\n",
    "\n",
    "### Reference Implementation\n",
    "\n",
    "- Lightspeed implementation: [solr_vector_io/solr.py](https://github.com/mwcz/lightspeed-providers/blob/anx/solr-provider-ls-0.2/lightspeed_stack_providers/providers/remote/solr_vector_io/solr_vector_io/src/solr_vector_io/solr.py#L386-L395)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa0c38",
   "metadata": {},
   "source": [
    "## Code Implementation Details\n",
    "\n",
    "Now let's look at how this is actually implemented in code.\n",
    "\n",
    "### The Function Signature\n",
    "\n",
    "```python\n",
    "async def query_hybrid(\n",
    "    embedding: NDArray,           # Query vector (converted from text to numbers)\n",
    "    query_string: str,             # Original query text for keyword search\n",
    "    k: int,                        # Final number of results wanted\n",
    "    score_threshold: float,        # Minimum score to include a result\n",
    "    reranker_type: str,           # Type of reranking strategy\n",
    "    reranker_params: dict          # Contains boost values (reRankWeight, etc.)\n",
    ")\n",
    "```\n",
    "\n",
    "**Key inputs:**\n",
    "\n",
    "- `embedding`: The query converted to a vector (array of numbers) that represents its meaning\n",
    "- `query_string`: The original text query for keyword matching\n",
    "- `k`: How many final results you want (e.g., 10)\n",
    "- `reranker_params`: Configuration like `reRankWeight` that controls how much semantic similarity matters\n",
    "\n",
    "### Solr Query Parameters Explained\n",
    "\n",
    "Here's what gets sent to Solr:\n",
    "\n",
    "```python\n",
    "data_params = {\n",
    "    # Stage 1: Initial keyword retrieval\n",
    "    \"q\": query_string,                    # Your keyword query (e.g., \"deploy nodejs\")\n",
    "    \"defType\": \"edismax\",                 # Extended DisMax parser (flexible keyword matching)\n",
    "    \"rows\": k,                            # Final result count (but we'll rerank k*2 first)\n",
    "    \n",
    "    # Stage 2: Reranking configuration\n",
    "    \"rq\": \"{{!rerank reRankQuery=$rqq reRankDocs={k*2} reRankWeight={vector_boost}}}\",\n",
    "    # rq = rerank query instruction\n",
    "    # reRankQuery=$rqq = use the query defined in rqq parameter\n",
    "    # reRankDocs={k*2} = rerank the top k*2 documents from keyword search\n",
    "    # reRankWeight={vector_boost} = how much to weight semantic score vs keyword score\n",
    "    \n",
    "    \"rqq\": \"{{!knn f={vector_field} topK={k*2}}}{vector_str}\",\n",
    "    # rqq = the actual rerank query (KNN = K-Nearest Neighbors, a vector similarity search)\n",
    "    # f={vector_field} = which field contains the document vectors\n",
    "    # topK={k*2} = consider top k*2 candidates\n",
    "    # {vector_str} = the query vector as a string\n",
    "    \n",
    "    # Other parameters\n",
    "    \"fl\": \"*, score\",                     # Return all fields + relevance score\n",
    "    \"fq\": [\"product:*openshift*\"],       # Filter query (only OpenShift docs)\n",
    "    \"wt\": \"json\"                          # Response format (JSON)\n",
    "}\n",
    "```\n",
    "\n",
    "### Understanding the Key Parameters\n",
    "\n",
    "| Parameter | What It Does | Example Value | Why It Matters |\n",
    "|-----------|--------------|---------------|----------------|\n",
    "| `q` | The keyword search query | `\"deploy nodejs openshift\"` | Finds initial candidates based on word matches |\n",
    "| `rq` | Rerank instruction | `\"{!rerank ...}\"` | Tells Solr to rerank results |\n",
    "| `reRankDocs` | How many docs to rerank | `20` (if k=10) | Larger pool = better reranking, but slower |\n",
    "| `reRankQuery` | What to use for reranking | `$rqq` (references rqq param) | Points to the vector similarity query |\n",
    "| `reRankWeight` | Semantic score importance | `5.0` (medium) | Controls balance: low = keyword wins, high = semantic wins |\n",
    "| `rqq` | The vector similarity query | `\"{!knn f=vector topK=20}...\"` | Performs semantic search on candidates |\n",
    "\n",
    "### How reRankWeight Works\n",
    "\n",
    "The `reRankWeight` parameter is crucial. It controls how the final score is calculated:\n",
    "\n",
    "```\n",
    "final_score = keyword_score + (reRankWeight × semantic_score)\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- `reRankWeight = 1`: Semantic score has equal weight to keyword score\n",
    "- `reRankWeight = 5`: Semantic score is 5× more important (balanced approach)\n",
    "- `reRankWeight = 20`: Semantic score dominates (for conceptual queries)\n",
    "\n",
    "**Why this matters:** Different types of queries need different balances. A query like \"CVE-2024-1234\" needs exact keyword matching (low weight), while \"how to improve security\" benefits from semantic understanding (high weight).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f484d",
   "metadata": {},
   "source": [
    "## Choosing the Right Reranking Strategy\n",
    "\n",
    "One of the key insights from this implementation is that **different query types need different reranking strategies**. You can't use the same `reRankWeight` for everything.\n",
    "\n",
    "### Why One Size Doesn't Fit All\n",
    "\n",
    "Consider these three queries:\n",
    "1. `\"CVE-2024-1234\"` - You want the exact security advisory\n",
    "2. `\"how to improve application performance\"` - You want conceptually relevant guides\n",
    "3. `\"how to patch CVE-2024-1234\"` - You need both the exact CVE and conceptual guidance\n",
    "\n",
    "Each needs a different balance between keyword matching and semantic understanding.\n",
    "\n",
    "### Strategy 1: Exact Technical Queries (Low Semantic Weight)\n",
    "\n",
    "**When to use:** Queries that require precise keyword matching\n",
    "\n",
    "**Examples:**\n",
    "- `\"CVE-2024-1234\"` - Specific security advisory ID\n",
    "- `\"error code 404\"` - Exact error code\n",
    "- `\"kubectl get pods\"` - Specific command syntax\n",
    "- `\"API endpoint /v1/users\"` - Exact API path\n",
    "\n",
    "**Strategy:** Low `reRankWeight` (1-2)\n",
    "\n",
    "**Why:**\n",
    "\n",
    "- These queries have very specific, unambiguous intent\n",
    "- Exact keyword matches are more important than semantic similarity\n",
    "- You don't want semantic search to \"helpfully\" return similar but different CVEs or error codes\n",
    "- The keyword search already finds the right documents; reranking should only make minor adjustments\n",
    "\n",
    "**Example scenario:**\n",
    "```\n",
    "Query: \"CVE-2024-1234\"\n",
    "Keyword search finds: Document about CVE-2024-1234 (score: 10.0)\n",
    "                      Document about CVE-2024-1235 (score: 8.0)  # Similar but wrong!\n",
    "                      \n",
    "With low reRankWeight (1.0):\n",
    "- CVE-2024-1234 stays on top (keyword score dominates)\n",
    "- CVE-2024-1235 stays lower (even if semantically similar)\n",
    "\n",
    "With high reRankWeight (20.0):\n",
    "- Risk: CVE-2024-1235 might jump ahead if it's semantically similar\n",
    "- Problem: User gets wrong CVE!\n",
    "```\n",
    "\n",
    "### Strategy 2: Conceptual Queries (High Semantic Weight)\n",
    "\n",
    "**When to use:** Queries about concepts, best practices, or \"how-to\" questions\n",
    "\n",
    "**Examples:**\n",
    "- `\"how to improve performance\"` - Broad conceptual question\n",
    "- `\"best practices for security\"` - General guidance\n",
    "- `\"troubleshooting slow deployments\"` - Problem-solving query\n",
    "- `\"scaling applications\"` - Conceptual topic\n",
    "\n",
    "**Strategy:** High `reRankWeight` (15-20)\n",
    "\n",
    "**Why:**\n",
    "\n",
    "- These queries are about concepts, not exact terms\n",
    "- Users might use different words than the documentation\n",
    "- Semantic understanding helps find relevant content even if terminology differs\n",
    "- Keyword search might miss relevant docs that use synonyms or related terms\n",
    "\n",
    "**Example scenario:**\n",
    "```\n",
    "Query: \"how to improve performance\"\n",
    "Keyword search finds: Doc mentioning \"improve performance\" (score: 9.0)\n",
    "                      Doc about \"optimization techniques\" (score: 6.0)  # Relevant but different words!\n",
    "                      \n",
    "With low reRankWeight (1.0):\n",
    "- \"improve performance\" doc stays on top\n",
    "- \"optimization techniques\" stays lower (missed opportunity)\n",
    "\n",
    "With high reRankWeight (20.0):\n",
    "- \"optimization techniques\" jumps ahead (semantically very relevant)\n",
    "- User gets better results!\n",
    "```\n",
    "\n",
    "### Strategy 3: Mixed Queries (Balanced Weight)\n",
    "\n",
    "**When to use:** Queries that combine specific terms with conceptual needs\n",
    "\n",
    "**Examples:**\n",
    "- `\"how to patch CVE-2024-1234\"` - Specific CVE + general patching guidance\n",
    "- `\"deploy nodejs on kubernetes\"` - Specific technologies + deployment concept\n",
    "- `\"troubleshoot openshift authentication errors\"` - Specific product + general troubleshooting\n",
    "- `\"configure SSL for nginx\"` - Specific tech + configuration concept\n",
    "\n",
    "**Strategy:** Medium `reRankWeight` (5-8)\n",
    "\n",
    "**Why:**\n",
    "\n",
    "- Need to match specific keywords (technology names, product names, error codes)\n",
    "- But also benefit from semantic understanding of the action/concept\n",
    "- Balance ensures specific terms are matched while still finding conceptually relevant content\n",
    "\n",
    "**Example scenario:**\n",
    "```\n",
    "Query: \"deploy nodejs on kubernetes\"\n",
    "Keyword search finds: \"Deploying Node.js on Kubernetes\" (score: 10.0)\n",
    "                      \"Running Node.js apps in K8s\" (score: 7.0)  # Different words, same concept\n",
    "                      \n",
    "With medium reRankWeight (6.0):\n",
    "- Both documents are considered\n",
    "- Exact match stays high, but semantic match can surface if very relevant\n",
    "- Good balance between precision and recall\n",
    "```\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "When choosing `reRankWeight`, ask yourself:\n",
    "\n",
    "1. **Is this query about a specific, unambiguous thing?** (CVE, error code, exact command)\n",
    "   - → Use **low weight (1-2)**\n",
    "\n",
    "2. **Is this query about a concept or general topic?** (how-to, best practices, troubleshooting)\n",
    "   - → Use **high weight (15-20)**\n",
    "\n",
    "3. **Does it combine specific terms with concepts?** (specific tech + general action)\n",
    "   - → Use **medium weight (5-8)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a9065",
   "metadata": {
    "time_run": "2025-11-22T14:16:13.398742+00:00"
   },
   "source": [
    "## Putting It All Together: A Practical Implementation Plan\n",
    "\n",
    "Now that we understand the concepts, let's see how to implement this in practice.\n",
    "\n",
    "### The Three-Tier Classification System\n",
    "\n",
    "Instead of trying to pick the perfect `reRankWeight` for every query, we can classify queries into three tiers:\n",
    "\n",
    "| Tier | Query Characteristics | reRankWeight | When to Use |\n",
    "|------|----------------------|--------------|-------------|\n",
    "| **Exact Match Critical** | Security IDs (CVE, Errata), error codes, exact commands | 1-2 | Queries that must match exact keywords |\n",
    "| **Balanced** | Technology + action combinations, mixed queries | 5-8 | Default for most queries (covers majority of cases) |\n",
    "| **Semantic Heavy** | Questions, how-to guides, best practices, troubleshooting | 15-20 | Conceptual queries where meaning matters most |\n",
    "\n",
    "### How Classification Works\n",
    "\n",
    "**Example Classification Logic:**\n",
    "```python\n",
    "def classify_query(query: str) -> str:\n",
    "    # Exact match critical: CVE, Errata, specific error codes\n",
    "    if re.search(r'CVE-\\d{4}-\\d+', query) or 'errata' in query.lower():\n",
    "        return \"exact_match\"\n",
    "    \n",
    "    # Semantic heavy: questions, how-to, best practices\n",
    "    if query.lower().startswith(('how', 'what', 'why', 'when')) or \\\n",
    "       'best practice' in query.lower() or 'troubleshoot' in query.lower():\n",
    "        return \"semantic_heavy\"\n",
    "    \n",
    "    # Default: balanced\n",
    "    return \"balanced\"\n",
    "\n",
    "# Map to reRankWeight\n",
    "weight_map = {\n",
    "    \"exact_match\": 1.5,\n",
    "    \"balanced\": 6.0,\n",
    "    \"semantic_heavy\": 18.0\n",
    "}\n",
    "```\n",
    "\n",
    "### Implementation Steps\n",
    "\n",
    "1. **Extend intent detection** to classify queries into three tiers\n",
    "   - Use pattern matching (regex, keywords)\n",
    "   - Leverage existing intent detection if available\n",
    "   - Start simple, refine based on data\n",
    "\n",
    "2. **Map each tier to reRankWeight value**\n",
    "   - Start with suggested ranges (1-2, 5-8, 15-20)\n",
    "   - Fine-tune based on your specific use case\n",
    "\n",
    "3. **Test on historical query logs**\n",
    "   - Run queries through both old and new systems\n",
    "   - Compare result quality (relevance, user satisfaction)\n",
    "   - Measure performance impact\n",
    "\n",
    "4. **Monitor and iterate**\n",
    "   - Track which queries get which classification\n",
    "   - Collect user feedback on result quality\n",
    "   - Adjust weights and classification rules based on data\n",
    "\n",
    "### Advantages of This Approach\n",
    "\n",
    "- **Practical starting point:** Three tiers cover most use cases without being too complex\n",
    "- **Data-driven refinement:** Start with defaults, improve based on real queries\n",
    "- **Explainable:** Easy to understand why a query got a certain weight\n",
    "- **Extensible:** Can add more tiers or dynamic weights later\n",
    "\n",
    "### Alternative Approaches (For Future Learning)\n",
    "\n",
    "This implementation uses **keyword-first reranking**, but there are other hybrid search strategies:\n",
    "\n",
    "1. **Union-based:** Run keyword and vector search separately, merge results\n",
    "2. **RRF (Reciprocal Rank Fusion):** Combine rankings from multiple search methods\n",
    "3. **Learning to Rank (LTR):** Use machine learning to automatically optimize weights\n",
    "4. **Dynamic weights:** Adjust `reRankWeight` based on query features (length, term frequency, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Reranking is a two-stage process:** Keyword search finds candidates, semantic search refines the ranking\n",
    "\n",
    "2. **reRankWeight controls the balance:** It determines how much semantic similarity matters vs. keyword matching\n",
    "   - Low (1-2): Keyword matching dominates\n",
    "   - Medium (5-8): Balanced approach\n",
    "   - High (15-20): Semantic similarity dominates\n",
    "\n",
    "3. **Different query types need different strategies:**\n",
    "   - Exact technical queries → Low weight\n",
    "   - Conceptual queries → High weight  \n",
    "   - Mixed queries → Medium weight\n",
    "\n",
    "4. **Start simple, iterate based on data:**\n",
    "   - Three tiers is a practical starting point\n",
    "   - Refine weights and classification rules based on real query performance\n",
    "\n",
    "5. **This is keyword-first hybrid:**\n",
    "   - Only documents matching keywords are considered\n",
    "   - Reranking refines within that set\n",
    "   - This is different from union-based approaches that merge separate results\n",
    "\n",
    "6. **Why retrieve k*2 candidates?**\n",
    "   - Gives reranker a larger pool to work with\n",
    "   - Allows semantically relevant docs to \"rescue\" from lower keyword ranks\n",
    "   - Balance between quality and performance\n",
    "\n",
    "---\n",
    "\n",
    "## Next Learning Topics\n",
    "\n",
    "- **Experiment design:** How to systematically test reranking strategies with query logs\n",
    "- **Alternative hybrid approaches:** Union-based search, RRF (Reciprocal Rank Fusion)\n",
    "- **Dynamic reRankWeight:** Adjusting weights based on query features automatically\n",
    "- **Learning to Rank (LTR):** Using machine learning to optimize reranking weights\n",
    "- **Performance optimization:** Balancing reranking quality with query latency\n",
    "\n",
    "---\n",
    "\n",
    "## Reference Materials\n",
    "\n",
    "- **Sease.io blog:** [Hybrid Search with Apache Solr](https://sease.io/2023/12/hybrid-search-with-apache-solr.html) - Comprehensive guide to hybrid search concepts\n",
    "- **Lightspeed implementation:** [solr_vector_io/solr.py](https://github.com/mwcz/lightspeed-providers/blob/anx/solr-provider-ls-0.2/lightspeed_stack_providers/providers/remote/solr_vector_io/solr_vector_io/src/solr_vector_io/solr.py#L386-L395) - Real-world code example\n",
    "- **Solveit Dialog:** [Hybrid Search in Solr](https://share.solve.it.com/d/165a3e3f0cc54d5aae552edc4da15d78) - Interactive learning resource\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
