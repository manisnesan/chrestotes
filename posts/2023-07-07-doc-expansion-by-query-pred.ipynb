{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for Document Expansion by Query Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Enhancing Search Engine Performance: Addressing Vocabulary Mismatch with Machine Learning\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [information-retrieval, machine-learning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search engines play a vital role in helping users find the information they need. However, a common challenge faced by search engines is the vocabulary mismatch problem. This occurs when the terms used by users in their queries do not precisely match the terms in the documents indexed by the search engine. (Eg: reduce vs lower though semantically mean the same, from the search engine stand point the terms are different). In summary the vocabulary used by users should match the vocabulary in the documents in the index. This is known as **Vocabulary mismatch problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Expansion for Enhanced Matching\n",
    "\n",
    "To predict the user questions on document text, we explore the varios ML models & approaches. Among them we explore popular approaches like Doc2Query, docTTTTTQuery etc. These models offer promising capabilities to generate relevant questions based on the contents of the documents.\n",
    "\n",
    "To enhance the matching process between user queries and indexed documents, we use this technique called **Document Expansion**.By indexing these predicted user questions along with the documents we can augment the search results and improve the relevance. \n",
    "\n",
    "The benefits of the approach is that computationally intensive tasks such as question prediction, are performed offline eliminating any additional overhead during search response times.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools for investigation are as follows\n",
    "\n",
    "- Transformers library to load the model, tokenizers and to generate predictions\n",
    "- HuggingFace Model Hub to load the pretrained models.\n",
    "- PapersWithCode to investigate the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the below are excerpts from this paper for my own reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [Paper](https://arxiv.org/pdf/1904.08375.pdf)\n",
    "\n",
    "Abstract:\n",
    "- One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the\n",
    "documents' content.\n",
    "- From the perspective of a question answering system, this might comprise questions the document can potentially answer.\n",
    "- Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents.\n",
    "- By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks.\n",
    "- In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural\n",
    "re-rankers but are much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![doc-expansion-by-query-pred](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/b092b6b843e9421bf42bf96f57ed4658a3e0bdf7/1-Figure1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage\n",
    "\n",
    "- primary advantage of this approach is that expensive neural inference is pushed to indexing time,\n",
    "- \"bag of words\" queries are against an inverted index built on the augmented document collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important observations\n",
    "- Model tends to copy some words from the input document (e.g., Washington DC, River, chromosome), meaning that it can effectively perform term re-weighting (i.e., increasing the importance of key terms).\n",
    "- Nevertheless, the model also produces words not present in the input document (e.g., weather, relationship), which can be characterized as expansion by synonyms and other related terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quote about T5 for doc expansion and the performance on [BEIR dataset](https://openreview.net/pdf?id=wCu6T5xFjeJ)\n",
    "\n",
    ">  In contrast, document expansion based docT5query is able to add new relevant keywords to a document and performs strong on the BEIR datasets. It outperforms BM25 on\n",
    "11/18 datasets while providing a competitive performance on the remaining datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related\n",
    "\n",
    "- [BEIR](https://github.com/beir-cellar/beir) - Heterogeneous Benchmark for Information Retrieval.\n",
    "- [Improve text ranking with few shot promptin](https://blog.vespa.ai/improving-text-ranking-with-few-shot-prompting/) by Jo Kristen Bergum\n",
    "- https://huggingface.co/blog/how-to-generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers sentencepiece -qqq > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using pretrained T5 model from HuggingFace for document expansion.\n",
    "\n",
    "As quoted in the model page\n",
    "\n",
    "\n",
    "- **Document expansion**: You generate for your paragraphs 20-40 queries and index the paragraphs and the generates queries in a standard BM25 index like Elasticsearch, OpenSearch, or Lucene. The generated queries help to close the lexical gap of lexical search, as the generate queries contain synonyms. Further, it re-weights words giving important words a higher weight even if they appear seldomn in a paragraph. In our BEIR paper we showed that BM25+docT5query is a powerful search engine. In the BEIR repository we have an example how to use docT5query with Pyserini.\n",
    "- **Domain Specific Training Data Generation**: It can be used to generate training data to learn an embedding model. On SBERT.net we have an example how to use the model to generate (query, text) pairs for a given collection of unlabeled texts. These pairs can then be used to train powerful dense embedding models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286f18e4b79a4527b1adb8228eb1f54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c3fe800c0241d1bce35f8122c30667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0371e2f4004a91ab3aecfe2e0045e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cc87df8d2d4643b85b00aa1946e1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca97d6add4448a783fc966d28468167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
      "\n",
      "Generated Queries:\n",
      "1: What is Python's design philosophy?\n",
      "2: What are the characteristics of Python?\n",
      "3: What does \"python's design philosophy\" mean?\n",
      "4: What is the difference between Python and Java?\n",
      "5: What is the logic behind Python's programming languages?\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/doc2query/stackexchange-title-body-t5-small-v1\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = 'doc2query/stackexchange-title-body-t5-small-v1'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "text = \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(text, max_length=384, truncation=True, return_tensors='pt')\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=64,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=5)\n",
    "\n",
    "print(\"Text:\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nGenerated Queries:\")\n",
    "for i in range(len(outputs)):\n",
    "    query = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(f'{i + 1}: {query}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text: str):\n",
    "    input_ids = tokenizer.encode(text, max_length=384, truncation=True, return_tensors='pt')\n",
    "    outputs = model.generate(\n",
    "      input_ids=input_ids,\n",
    "      max_length=64,\n",
    "      do_sample=True,\n",
    "      top_p=0.95,\n",
    "      num_return_sequences=5)\n",
    "\n",
    "    print(\"Text:\")\n",
    "    print(text)\n",
    "\n",
    "    gen_texts = []\n",
    "    print(\"\\nGenerated Queries:\")\n",
    "    for i in range(len(outputs)):\n",
    "        query = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "        #print(f'{i + 1}: {query}')\n",
    "        gen_texts.append(query)\n",
    "    return gen_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "In certain situations it is needed to have source code preprocessed by `gcc` without undergoing the full compilation process. For example, this might be necessary when embedded SQL is included in C or C++ programs and the preprocesssed file will be passed on to another tool which will convert the SQL in native source code.  - Red Hat Enterprise Linux (RHEL)   --- ***Disclaimer:** Links contained herein to an external website(s) are provided for convenience only. Red Hat has not reviewed the links and is not responsible for the content or its availability. The inclusion of any link to an external website does not imply endorsement by Red Hat of the website or their entities, products or services. You agree that Red Hat is not responsible or liable for any loss or expenses that may result due to your use of (or reliance on) the external site or content.*--- To make GCC stop after the preprocessing stage, use the option `-E`, as explained in [GNU- GCC options](https://gcc.gnu.org/onlinedocs/gcc/Overall-Options.html#Overall-Options). In other words, Using the `-E` parameter with `gcc` or `g++` will produce **only** the preprocessed source code: $ gcc-E program.c-o program.preprocessed The `program.preprocessed` file will contain the file preprocessed by `gcc` (`Macros` will be expanded and all include files will be resolved). This preprocessed output will contain lines such as the following ones: ~~~ # 131 &quot;/usr/include/bits/types.h&quot; 3 4 # 1 &quot;/usr/include/bits/typesizes.h&quot; 1 3 4 # 132 &quot;/usr/include/bits/types.h&quot; 2 3 4 ~~~ These lines are line markers that show from which include files specific source code was taken. If those lines are not desired run the same command adding the `-P` parameter: $ gcc-E-P program.c-o program.preprocessed\n",
      "\n",
      "Generated Queries:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Make GCC stop after preprocessing',\n",
       " 'GCC: how to stop \"gcc\" after the full compilation process?',\n",
       " 'Retrieving Source code preprocessed by gcc without undergoing full compilation process',\n",
       " \"Why can't I make GCC stop after the full compilation process?\",\n",
       " 'Can the -E parameter for gcc or g++ prevent any preprocessed source code from being loaded?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = 'In certain situations it is needed to have source code preprocessed by `gcc` without undergoing the full compilation process. For example, this might be necessary when embedded SQL is included in C or C++ programs and the preprocesssed file will be passed on to another tool which will convert the SQL in native source code.  - Red Hat Enterprise Linux (RHEL)   --- ***Disclaimer:** Links contained herein to an external website(s) are provided for convenience only. Red Hat has not reviewed the links and is not responsible for the content or its availability. The inclusion of any link to an external website does not imply endorsement by Red Hat of the website or their entities, products or services. You agree that Red Hat is not responsible or liable for any loss or expenses that may result due to your use of (or reliance on) the external site or content.*--- To make GCC stop after the preprocessing stage, use the option `-E`, as explained in [GNU- GCC options](https://gcc.gnu.org/onlinedocs/gcc/Overall-Options.html#Overall-Options). In other words, Using the `-E` parameter with `gcc` or `g++` will produce **only** the preprocessed source code: $ gcc-E program.c-o program.preprocessed The `program.preprocessed` file will contain the file preprocessed by `gcc` (`Macros` will be expanded and all include files will be resolved). This preprocessed output will contain lines such as the following ones: ~~~ # 131 &quot;/usr/include/bits/types.h&quot; 3 4 # 1 &quot;/usr/include/bits/typesizes.h&quot; 1 3 4 # 132 &quot;/usr/include/bits/types.h&quot; 2 3 4 ~~~ These lines are line markers that show from which include files specific source code was taken. If those lines are not desired run the same command adding the `-P` parameter: $ gcc-E-P program.c-o program.preprocessed'\n",
    "generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6 ? * Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6? Fails with the following message ~~~ ftp://X.X.X.X/Bootstrap \\X86PC\\BStrap.0... Permission denied (0x0212603c) ~~~ * Red Hat Enterprise Linux 6 * KVM GPXe boot * Two dhcp servers on different subnet and are accessed via relay agetns First dhcp server assign IP address Second dhcp server provides next-server details  If gpxe is used to pxe-boot a KVM guest, it uses the next-server from the first dhcp offer and if it fails, then does not re-try with the next-server option provided in the second dhcp server offer packet Update gpxe packages to below versions or above.- gpxe-bootimgs-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-qemu-0.9.7-6.10.el6.noarch.rpm This has been fixed by errata http://rhn.redhat.com/errata/RHBA-2013-1628.html\n",
      "\n",
      "Generated Queries:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gpxe boot fails when next-server details are offered by the second dhcp server',\n",
       " 'gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6?',\n",
       " 'gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6',\n",
       " 'gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6?',\n",
       " 'Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = 'Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6 ? * Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6? Fails with the following message ~~~ ftp://X.X.X.X/Bootstrap \\\\X86PC\\\\BStrap.0... Permission denied (0x0212603c) ~~~ * Red Hat Enterprise Linux 6 * KVM GPXe boot * Two dhcp servers on different subnet and are accessed via relay agetns First dhcp server assign IP address Second dhcp server provides next-server details  If gpxe is used to pxe-boot a KVM guest, it uses the next-server from the first dhcp offer and if it fails, then does not re-try with the next-server option provided in the second dhcp server offer packet Update gpxe packages to below versions or above.- gpxe-bootimgs-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-qemu-0.9.7-6.10.el6.noarch.rpm This has been fixed by errata http://rhn.redhat.com/errata/RHBA-2013-1628.html'\n",
    "generate(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I provide the title included in the contents, the generated text replicates the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Unable change permission to NFS share mounted at the client. * The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.\n",
      "\n",
      "Generated Queries:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Unable change permission to NFS share mounted at the client',\n",
       " 'Unable change permission to NFS share mounted at the client',\n",
       " 'NFS client \"Unable change permission to NFS share mounted at the client.\"',\n",
       " 'Unable change permission to NFS share mounted at the client',\n",
       " 'Unable change permission to NFS share mounted at the client']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = 'Unable change permission to NFS share mounted at the client. * The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.'\n",
    "generate(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude the title. Provides much better variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "* The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.\n",
      "\n",
      "Generated Queries:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SSH NFS clients permissions can be changed, even by using root',\n",
       " 'NFS: root permissions not changing',\n",
       " \"Why can't the directory permissions be changed in the NFS share?\",\n",
       " 'NFS share fails to resolve NFS: directory permissions cannot be changed',\n",
       " 'How can I set NFS permissions to be changed with root?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = '* The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.'\n",
    "generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "- qpid process segfaulting with backtrace attached- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - mrg messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::frameset::getcontentsize (this=0x128) at qpid/framing/frameset.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::queuepolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/queuepolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::queue::dequeuecommitted (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached c++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) mrg 2.1 (that is qpid 0.14)\n",
      "\n",
      "Generated Queries:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['qpid process segfaulting with backtrace attached',\n",
       " 'qpid segfaulting with backtrace attached',\n",
       " 'Can anyone help me with my data segfaulting qpid transaction?',\n",
       " 'qpid segfault with backtrace attached- coredump has mrg messaging 2.0',\n",
       " 'qpid process segfaulting with backtrace attached - coredump']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = '- qpid process segfaulting with backtrace attached- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - MRG Messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::FrameSet::getContentSize (this=0x128) at qpid/framing/FrameSet.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::QueuePolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/QueuePolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::Queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::Queue::dequeueCommitted (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ Complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached C++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) MRG 2.1 (that is qpid 0.14)'.lower()\n",
    "generate(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some post-processing required such lower casing in order to generate unique text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude the issue statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - mrg messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::frameset::getcontentsize (this=0x128) at qpid/framing/frameset.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::queuepolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/queuepolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::queue::dequeuecommitted (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached c++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) mrg 2.1 (that is qpid 0.14)\n",
      "\n",
      "Generated Queries:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['coredump for mrg messaging in coredump',\n",
       " 'coredump backtrace problem with messaging 2.0',\n",
       " 'mrg messaging, coredump and coredump',\n",
       " 'mrg messaging 1.1 and coredump: why is the backtrace not used?',\n",
       " 'Why has my backAmplitude (_0) returned?']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = '- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - MRG Messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::FrameSet::getContentSize (this=0x128) at qpid/framing/FrameSet.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::QueuePolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/QueuePolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::Queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::Queue::dequeueCommitted (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ Complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached C++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) MRG 2.1 (that is qpid 0.14)'.lower()\n",
    "\n",
    "generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "We are passing through a migration from EPP with SP 5.1 to EPP with SP 5.2.0 we noticed some strange behaviors. For example, the context menu in &quot;Content Explorer&quot; is all messed. How can we solve this? - Red Hat JBoss Portal Platform (JPP also known as EPP)- 5.2.0- Site Publisher (SP)- 5.2.0  In Site Publisher Groovy templates are also contents from JCR (JCR nodes). When we are passing through a migration we need to keep the JCR shared folder and do not change it. It causes that some Groovy templates from the old installation are not correctly replaced to the new ones, which leads to compatibility issues with EPP+SP 5.2. Upgrade to newest EPP 5.2.x release and [migrate all templates to the new version](https://access.redhat.com/site/solutions/85613).\n",
      "\n",
      "Generated Queries:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Migrate from EPP to SP 5.2 has strange behavior',\n",
       " 'Magento migration for a different version from SharePoint with new environment is messed up',\n",
       " 'Disable JCR contents with custom templates, SP 5.1 to EPP + SP 5.2.0',\n",
       " 'Migrating JCR and context menu messes',\n",
       " 'JCR menu of JCR-Node is corrupted']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = 'We are passing through a migration from EPP with SP 5.1 to EPP with SP 5.2.0 we noticed some strange behaviors. For example, the context menu in &quot;Content Explorer&quot; is all messed. How can we solve this? - Red Hat JBoss Portal Platform (JPP also known as EPP)- 5.2.0- Site Publisher (SP)- 5.2.0  In Site Publisher Groovy templates are also contents from JCR (JCR nodes). When we are passing through a migration we need to keep the JCR shared folder and do not change it. It causes that some Groovy templates from the old installation are not correctly replaced to the new ones, which leads to compatibility issues with EPP+SP 5.2. Upgrade to newest EPP 5.2.x release and [migrate all templates to the new version](https://access.redhat.com/site/solutions/85613).'\n",
    "generate(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post should serve as the initial starting point for the Document Expansion technique and Refer the [Colbert paper](https://www.arxiv-vanity.com/papers/2004.12832/) for the results using these approaches in standard benchmarks such as BEIR. In order to use these technique in your domain, index your documents using tools such as pyserini and compare with BM25 baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
