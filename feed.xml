<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://manisnesan.github.io/chrestotes/feed.xml" rel="self" type="application/atom+xml" /><link href="https://manisnesan.github.io/chrestotes/" rel="alternate" type="text/html" /><updated>2020-09-05T19:08:33-05:00</updated><id>https://manisnesan.github.io/chrestotes/feed.xml</id><title type="html">Chrestotes</title><subtitle>A Commonplace blog is to document my learning on whatever that interests me at the moment. Read more about the name Commonplace here https://en.wikipedia.org/wiki/Commonplace_book</subtitle><entry><title type="html">Tracking Data and Model in Machine Learning projects</title><link href="https://manisnesan.github.io/chrestotes/markdown/2020/06/29/tracking-data-model-using-dvc.html" rel="alternate" type="text/html" title="Tracking Data and Model in Machine Learning projects" /><published>2020-06-29T00:00:00-05:00</published><updated>2020-06-29T00:00:00-05:00</updated><id>https://manisnesan.github.io/chrestotes/markdown/2020/06/29/tracking-data-model-using-dvc</id><content type="html" xml:base="https://manisnesan.github.io/chrestotes/markdown/2020/06/29/tracking-data-model-using-dvc.html">&lt;h1 id=&quot;tracking-data-and-model-in-machine-learning-projects&quot;&gt;Tracking Data and Model in Machine Learning projects&lt;/h1&gt;

&lt;h2 id=&quot;why-should-you-track-everything&quot;&gt;Why should you track everything&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Difference between software engineering and machine learning projects&lt;/li&gt;
  &lt;li&gt;code, config, dependencies vs code, data, model, config, dependencies&lt;/li&gt;
  &lt;li&gt;Myriad of Data files (raw, intermediate)&lt;/li&gt;
  &lt;li&gt;Not in repository&lt;/li&gt;
  &lt;li&gt;Reproducible ML models targeting metrics [Experiments Link]&lt;/li&gt;
  &lt;li&gt;Tuning and Experimentation tracking&lt;/li&gt;
  &lt;li&gt;Model Deployment &amp;amp; Revert&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dvc&quot;&gt;DVC&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;DVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Experiments are tracked by combining the Code + Data files&lt;/li&gt;
  &lt;li&gt;Data files
    &lt;ul&gt;
      &lt;li&gt;Local cache&lt;/li&gt;
      &lt;li&gt;Data remotes in S3, SSH etc&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Metrics per experiment&lt;/li&gt;
  &lt;li&gt;Pipelines&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;install-dvc-on-fedorarhelcentos&quot;&gt;Install DVC on Fedora/RHEL/CentOS&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget https://dvc.org/rpm/dvc.repo &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; /etc/yum.repos.d/dvc.repo
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;yum update
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;yum &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;dvc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add DVC files in the Git (It is not required but it is good to have project and dvc config in the same directory)&lt;/p&gt;

&lt;h2 id=&quot;workflow-for-model-packaging&quot;&gt;Workflow for Model Packaging&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Setup for connecting to model repository during training development process&lt;/li&gt;
  &lt;li&gt;Push the trained model to the repository&lt;/li&gt;
  &lt;li&gt;Pull the model from the repository during the build process&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pushing-model-files&quot;&gt;Pushing model files&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dvc init
dvc remote add &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; gss-rdu-remote ssh://msivanes@gss-rdu-repo.usersys.redhat.com:/var/www/html/repo/config/ulmfit

dvc add cases_small_sbr_08-06-2020.pkl
git commit &lt;span class=&quot;nt&quot;&gt;-am&lt;/span&gt; “Add ulmfit model to project”
dvc push &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;pulling-the-model&quot;&gt;Pulling the model&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone &lt;span class=&quot;nv&quot;&gt;$REPO&lt;/span&gt;
git pull
dvc pull &lt;span class=&quot;c&quot;&gt;# Pulls the data from remote-storage. Equivalent to dvc fetch followed by dvc checkout&lt;/span&gt;
dvc checkout &lt;span class=&quot;c&quot;&gt;#Update model files&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;food-for-thought&quot;&gt;Food for thought&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Rather than feature branches, think in terms of experiment branches targeting metrics.&lt;/li&gt;
  &lt;li&gt;Keep Data and Model files stored outside the repository&lt;/li&gt;
  &lt;li&gt;Model files built as part of build process.&lt;/li&gt;
  &lt;li&gt;Smoke test should validate the constructed model using validation set.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dvc.org/doc&quot;&gt;DVC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.globalsqa.com/dvc-cheat-sheet/&quot;&gt;Cheatsheet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;https://christophergs.github.io/machine%20learning/2019/05/13/first-impressions-of-dvc/&lt;/li&gt;
  &lt;li&gt;https://www.slideshare.net/DmitryPetrov15/pydata-berlin-2018-dvcorg&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Tracking Data and Model in Machine Learning projects</summary></entry><entry><title type="html">Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset</title><link href="https://manisnesan.github.io/chrestotes/markdown/2020/05/25/Paper_Summary_Covidex.html" rel="alternate" type="text/html" title="Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset" /><published>2020-05-25T00:00:00-05:00</published><updated>2020-05-25T00:00:00-05:00</updated><id>https://manisnesan.github.io/chrestotes/markdown/2020/05/25/Paper_Summary_Covidex</id><content type="html" xml:base="https://manisnesan.github.io/chrestotes/markdown/2020/05/25/Paper_Summary_Covidex.html">&lt;h1 id=&quot;paper-summary---rapidly-deploying-a-neural-search-engine-for-the-covid-19-open-research-dataset&quot;&gt;Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset&lt;/h1&gt;

&lt;h2 id=&quot;overview-of-covidexai&quot;&gt;Overview of covidex.ai&lt;/h2&gt;

&lt;p&gt;This is a paper summary of deploying a Neural Search Engine to answer questions from the COVID-19 dataset.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=PlUA_mgGaPq&quot;&gt;Paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://covidex.ai&quot;&gt;covidex.ai&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/lintool/status/1248668833713467392?s=20&quot;&gt;Twitter Announcement by Jimmy Lin&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Neural Covidex applies state-of-the-art neural network models and artificial intelligence (AI) techniques to answer questions using the COVID-19 Open Research Dataset (CORD-19) provided by the Allen Institute for AI (data release of April 3, 2020).
This project is led by Jimmy Lin from the University of Waterloo and Kyunghyun Cho from NYU, with a small team of wonderful students: Edwin Zhang and Nikhil Gupta. Special thanks to Colin Raffel for his help in pretraining T5 models for the biomedical domain.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://forums.fast.ai/uploads/default/original/3X/2/2/22fbefd8abb7063ab24722cdda185c523abba6ec.png&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;The ongoing pandemic crisis poses a huge challenge to get timely information for public health officials, clinicians, researchers, virologists. In order to respond to this challenge, Allen AI publishes a COVID-19  data set (CORD-19) in collaboration with other research groups. The source for this data set is both research articles published about coronovirus and other related research articles.
The aim of this effort is to bring researchers&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;to apply language processing techniques in order to generate insights &amp;amp; make data driven decisions.&lt;/li&gt;
  &lt;li&gt;to provide ways for the front line to consume the recent developments in a digestible form &amp;amp; apply in the field.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;outcomes&quot;&gt;Outcomes&lt;/h2&gt;

&lt;p&gt;Jimmy Lin &amp;amp; his research team responded to this call.  The two strategies adopted were&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Real time users should be able to find answers to any questions associated with COVID&lt;/li&gt;
  &lt;li&gt;Other Researchers should be able reuse the components they build. Providing a modular and reusable components is set as part of the requirements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The team decided to build end to end real time search application called covidex.ai . They developed the components that powers this engine in a short span of time for the information retrieval need .&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;keyword based search interface :&lt;/strong&gt;  This also provides &lt;strong&gt;faceted navigation&lt;/strong&gt; in the form of filters like author, article source, time range and &lt;strong&gt;highlighting&lt;/strong&gt; words from the results that matches with user query.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;neural ranking component&lt;/strong&gt; that sorts the results with the top most results answering user’s question.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background--related-work&quot;&gt;Background &amp;amp; Related Work&lt;/h2&gt;

&lt;p&gt;Traditional search architecture comprises of two phases Content Indexing and Keyword Searching. During indexing, content is transformed into an Indexable form called as Document The search engine convert this document into a fundamental data structure called as Inverted Index. This is similar to what you see in Book Appendix where terms are mapped towards the pages. Similarly Inverted index contains terms mapped towards docIds where the term appears &amp;amp; position in the document.&lt;/p&gt;

&lt;p&gt;Searching phase is further divided into two stages &lt;strong&gt;retrieval stage&lt;/strong&gt; and &lt;strong&gt;ranking stage&lt;/strong&gt;. In the first stage, given a search term(s) you will retrieve the list of matching documents from the inverted index. In the second stage, the matched documents are sorted based on the computed relevance score.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modern&lt;/strong&gt; &lt;strong&gt;Search Architectures&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;More modern multi-stage search architectures from Bing &amp;amp; Alibaba expand the Search Phase with additional reranking stages. Except for the first retrieval stage, the additional subsequent ranking stages will rerank and refine the results further from previous stages .&lt;/p&gt;

&lt;h2 id=&quot;modular-and-reusable-keyword-search&quot;&gt;Modular and Reusable Keyword Search&lt;/h2&gt;

&lt;h3 id=&quot;anserini&quot;&gt;Anserini&lt;/h3&gt;

&lt;p&gt;Anserini is an opensource Information Retrieval toolkit in order to solve the reproducibility problems in research and bridge the gap between research and real world systems. This is a tool that is built on top of Lucene, a popular open source search library &amp;amp; enhanced with features specific for conducting IR research. &lt;strong&gt;pyserini&lt;/strong&gt; is a python interface to Anserini.&lt;/p&gt;

&lt;h3 id=&quot;indexing&quot;&gt;Indexing&lt;/h3&gt;

&lt;p&gt;The first challenge faced by the team is representing the &lt;strong&gt;Indexable Document.&lt;/strong&gt; This is fundamental unit of search engine. Results are basically collection of documents. Eg: Tweets, WebPage, Article&lt;/p&gt;

&lt;p&gt;One of the common challenge wrt Information Retrieval systems, they tend to favor longer documents. In order to give all documents a fairer chances irrespective of its length, normalization is needed.&lt;/p&gt;

&lt;p&gt;The articles are in the following format&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;“Impact&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Masks”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;abstract:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;“Masks&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;protects&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;others&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;you.”&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;body_text&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;“Effectiveness&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/n&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;/n&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;This&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;being&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;studied&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;…..”&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In order to compare the effectiveness, the team decide to index the articles in 3 different ways&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Index only the title and abstract of the article as a document&lt;/li&gt;
  &lt;li&gt;Index the entire text of the article as a document combining title, abstract and body_text&lt;/li&gt;
  &lt;li&gt;Break the body_text of the article into paragraphs and each paragraph as separate documents. In this case, the Indexable Document is title, abstract &amp;amp; the paragraph.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;searching&quot;&gt;Searching&lt;/h3&gt;

&lt;p&gt;Once the team built the lucene indices based on the above scheme for CORD-19, we can able to search for a given term,  retrieve matching documents and ranked them using BM25 scoring function. These indices are available for the researchers to perform further research.&lt;/p&gt;

&lt;p&gt;The full search pipeline for keyword search is demonstrated using &lt;a href=&quot;https://github.com/castorini/anserini-notebooks/blob/master/pyserini_covid19_default.ipynb&quot;&gt;notebooks using Pyserini&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In order to provide the users a live system that can be used to answer questions, the team leveraged their earlier work on &lt;a href=&quot;https://www.semanticscholar.org/paper/Solr-Integration-in-the-Anserini-Information-Clancy-Eskildsen/31fc159be43811170b9906c5809c1583e9778151&quot;&gt;anserini integration with Solr&lt;/a&gt; , open source search platform. The user search interface is built using Blacklight discovery interface with Ruby on Rails for faceting &amp;amp; highlighting.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://d3i71xaburhd42.cloudfront.net/31fc159be43811170b9906c5809c1583e9778151/2-Figure1-1.png&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;highlighting&quot;&gt;Highlighting&lt;/h3&gt;

&lt;p&gt;In addition to that the team also built a highlighting feature on of keyword search. This allows the user to quickly scan the results with the matched keywords in the document.&lt;/p&gt;

&lt;p&gt;It is built using &lt;a href=&quot;https://www.semanticscholar.org/paper/BioBERT%3A-a-pre-trained-biomedical-language-model-Lee-Yoon/1e43c7084bdcb6b3102afaf301cce10faead2702&quot;&gt;BioBERT&lt;/a&gt;. The idea behind it is that a) the candidate matched documents &amp;amp; convert them into sentences. b) Similarly the query is treated as a sentence. The sentences &amp;amp; query are in turn converted into its numerical representations (vectors). Top sentences closer to the query are obtained using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;&gt;cosine similarity&lt;/a&gt;. The top-K words in the context are highlighted in these top sentences.&lt;/p&gt;

&lt;h2 id=&quot;neural-covidex-for-reranking&quot;&gt;Neural Covidex for reranking&lt;/h2&gt;

&lt;p&gt;The research group was  already working on the neural architectures specifically applying transfer learning on retrieval/ranking based problems.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BERT for query based Passage Ranking : Applying transfer learning for passage reranking pre-trained on MS-MARCO dataset&lt;/li&gt;
  &lt;li&gt;BERTserini for retrieval-based question answering: Incorporating Anserini Retriever to retrieve the top K segments of text followed by BERT based pre-trained model to retrieve the answer span.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Typically the task of reranking is turn the problem into a classification task where we take the query, candidate_document &amp;amp; predict the target as (relevant, not-relevant). To avoid the costly operation of performing classification on the entire corpus, this is applied at the reranking stages. The engine gets the top K documents from the previous retrieval stage and rerank them using machine learning model. As part of reranking stage, the team leveraged Sequence to Sequence Model for reranking (Nogueira et al. 2020).&lt;/p&gt;

&lt;p&gt;Stages involved in training the reranking model using Transfer Learning Methodology&lt;/p&gt;

&lt;p&gt;Pre-training →Fine Tuning → Inference&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;[Pre-training]&lt;/strong&gt; Transformer based Language Model trained on MS Marco dataset&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;[Fine Tuning]&lt;/strong&gt; Given a query q, document D, the model is fine tuned to predict the output as either &lt;strong&gt;true&lt;/strong&gt; or &lt;strong&gt;false&lt;/strong&gt; as targets indicating the relevance.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;[Inference]&lt;/strong&gt; In reranking setting, for each candidate documents, predict the prob distribution of (relevant, non-relevant) and sort the scores of relevant doc(&lt;strong&gt;true outputs&lt;/strong&gt;) alone.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Training a language model and the encoder from this fined tuned language model is normally used for the downstream tasks like Classification in transfer learning methodology. But this method of applying Sequence to Sequence model (&lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;&gt;based on T5&lt;/a&gt;) is quite new for document ranking setting (Nogueira et al., 2020).&lt;/p&gt;

&lt;p&gt;The reasoning provided was the predicted target words can capture the relatedness through pre-training. This is based on encoder-decoder architecture &amp;amp; uses a similar masked language modeling objective. Given a query, document the model is fine tuned to produce true or false if the document is relevant or not to the query.&lt;/p&gt;

&lt;h2 id=&quot;challenges-in-results-evaluation&quot;&gt;Challenges in Results Evaluation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The authors rightfully mention that the individual components comprising such a system is evaluated against various test datasets. But as this is specific to an evolving dataset like CORD-19, there is no such existing test collections.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is not always necessary that ranking is the most important for such an end to end system. We have to switch to an &lt;strong&gt;outcome based measure&lt;/strong&gt; rather than a single output based measure like batch retrieval evaluations(MRR, nDCG). Eg: “Did the researchers, practitioners get their questions answered?” How many of them are not finding the answers? So involving human in the loop to qualitatively evaluate the results is essential to know if the system is really contributing towards the efforts fighting the pandemic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What if the exploratory users do not know the right type of keywords to use ? In that case ranking is a wrong goal to pursue.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Current challenge is all the targeted users are working on the front line and hard to provide qualitative feedback about search experience. So the author asks for more hallway usability testing to gather insights from the users.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;author-reflections&quot;&gt;Author Reflections&lt;/h2&gt;

&lt;p&gt;An end to end system like Covidex is not possible without the power of the current Open Source Software ecosystem, Open culture of curating, cleaning &amp;amp; sharing the data with the community (Thanks to CORD-19 by Allen AI) and pre-trained language models like MS-Marco etc.&lt;/p&gt;

&lt;p&gt;Good software engineering practices is the foundation for a team and ensure that the underlying software components can be replicated &amp;amp; reused to provide this system. This is essential to rapidly explore and experiment with new ideas.&lt;/p&gt;

&lt;p&gt;Building a strong research culture to produce the results in the form of open source software artifacts aid the community in reproducing the results and build on top of it.&lt;/p&gt;

&lt;p&gt;Reminder about the mismatch between producing research code for conference and building a system for a real users. For example concerns like latency of search requests, throughput about the number of users, deploying &amp;amp; managing a system in production, user experience does not arise in a research setting.&lt;/p&gt;

&lt;h2 id=&quot;insights--takeaways-from-this-paper&quot;&gt;Insights &amp;amp; Takeaways from this paper&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Lack of proper training data &amp;amp; human annotators is a common challenge. Leveraging pre-trained models on MS-MARCO is critical for ranking tasks in this type of situation.&lt;/li&gt;
  &lt;li&gt;The experimentation mindset need to be adopted and one need to interactive computing tools like pyserini to experiment with search index. This allows the search practitioners to constantly iterate and learn from these experiments.&lt;/li&gt;
  &lt;li&gt;Adoption of Openness in not only the source but science and data &amp;amp; the way we work is truly inspiring.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset</summary></entry></feed>