{
  
    
        "post0": {
            "title": "Title",
            "content": "Image Classification using fastai MidLevel API . In this blog post, we will explore a toy classification example using the Fashion MNIST dataset. We will discuss the limitations of the fastai DataBlock API and how learning about the Mid-level API can help overcome these limitations using Transforms, Pipeline, and Datasets. We will also cover the debugging steps involved in creating the Dataloaders for training image classifier &amp; deploy the app using Gradio &amp; HuggingFace spaces. . See the complete app Code &amp; Space . toc: true | badges: true | comments: true | categories: [image-classification, machine-learning] | image: images/chart-preview.png | . Setup . Context . When working with the fastai library, we may encounter situations where the Data block API is not flexible enough to handle our specific data processing needs. In such cases, we can use fastai&#39;s mid-level API, which provides a more comprehensive set of tools for processing data. The mid-level API includes a range of features such as Transforms, Pipeline, TfmdLists, Datasets, Callbacks, and General Optimizer. By using these, we can overcome the limitations of the Data block API and create more customized data processing pipelines to suit our specific use case. . To read more about Mid-Level API, please refer Chapter 11 - Data Munging with fastai&#39;s Mid-Level API in fastbook. . . In this post, I wanted to share a toy classification example using Fashion MNIST dataset where the DataBlock API is not flexibile and how learning about Mid-level API using Transforms, Pipeline and Datasets helped to create the Dataloaders for training the model. We will use the model to create a image classifier predicting the target class given an black &amp; white image of a fashion apparel using Gradio and HuggingFace spaces. We will also cover the debugging steps involved at the relevant step. . The end to end workflow is as follows . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G Dataset Dataset Dataloaders Dataloaders Dataset&#45;&gt;Dataloaders Train Train Dataloaders&#45;&gt;Train Predict_Item Predict_Item Train&#45;&gt;Predict_Item Deploy Deploy Predict_Item&#45;&gt;Deploy Exploring the Dataset . Download and load dataset with the name fashion_mnist from https://huggingface.co/datasets/fashion_mnist using HuggingFace datasets library. | . Fashion-MNIST is a dataset of Zalando&#39;s article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 apparels such as t-shirt, ankle boot, . Explore the dataset using load_dataset_builder inspecting the dataset info such as description, features, splits etc | . name=&#39;fashion_mnist&#39; ds_builder = load_dataset_builder(name) . print(ds_builder.info.description) . Fashion-MNIST is a dataset of Zalando&#39;s article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. . print(ds_builder.info.features) . {&#39;image&#39;: Image(decode=True, id=None), &#39;label&#39;: ClassLabel(names=[&#39;T - shirt / top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;], id=None)} . print(ds_builder.info.splits) . {&#39;train&#39;: SplitInfo(name=&#39;train&#39;, num_bytes=31296655, num_examples=60000, shard_lengths=None, dataset_name=None), &#39;test&#39;: SplitInfo(name=&#39;test&#39;, num_bytes=5233818, num_examples=10000, shard_lengths=None, dataset_name=None)} . ds_builder.info.features[&#39;label&#39;] . ClassLabel(names=[&#39;T - shirt / top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;], id=None) . ds_builder.info.features[&#39;label&#39;].int2str(9) . &#39;Ankle boot&#39; . Load the dataset from the Hugging Face Hub specifying the name. | . dset = load_dataset(name) . This is a DatasetDict containing a train and test dataset dictionary within this object. . dset . DatasetDict({ train: Dataset({ features: [&#39;image&#39;, &#39;label&#39;], num_rows: 60000 }) test: Dataset({ features: [&#39;image&#39;, &#39;label&#39;], num_rows: 10000 }) }) . We can inspect the individual item within train and test and the different labels. | . dset[&#39;train&#39;][0], dset[&#39;test&#39;][0] . ({&#39;image&#39;: &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;, &#39;label&#39;: 9}, {&#39;image&#39;: &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;, &#39;label&#39;: 9}) . Create fastai DataLoaders . We eventually want to create Datasets object in fastai from HuggingFace Dataset. We will first attempt with the the high level DataBlock API and then transition to fastai Datasets. . DataBlock is a high level API in fastai allowing the user to define the standard steps to prepare the data for deep learning model. . Steps involved to prepare the data . Identify the types of inputs/targets for your data and define them as &quot;Blocks&quot;. | Specify how to fetch and define any transformations that need to be applied to the inputs using the &quot;get_x&quot; function. | Specify how to fetch and define any transformations that need to be applied to the targets using the &quot;get_y&quot; function. | Split the data into training and validation sets using the &quot;splitter&quot; function. | Apply any additional transformations to the items using the &quot;item_tfms&quot; function. | Apply any additional transformations to the batches using the &quot;batch_tfms&quot; function. | . Let&#39;s create training and test sets. . train, test = dset[&#39;train&#39;], dset[&#39;test&#39;] . Create an Image from argument using PILImageBW.create function . im = PILImageBW.create(train[0][&#39;image&#39;]) . Display the image using show method . im.show() . &lt;AxesSubplot:&gt; . Let&#39;s examine the type of the features (ie Image) and label . type(train[0][&#39;image&#39;]) . PIL.PngImagePlugin.PngImageFile . Let&#39;s examine the first 3 training samples . train[:3] . {&#39;image&#39;: [&lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;, &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;, &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;], &#39;label&#39;: [9, 0, 0]} . The type of label is an int but since fastai also performs Categorize transform we can create a separate target which contains the original apparel name. This can be achieved using ClassLabel.int2str method. . classLabel = ClassLabel(names=[&#39;T - shirt / top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;], id=None) def add_target(x:dict): x[&#39;target&#39;] = classLabel.int2str(x[&#39;label&#39;]) return x train = train.map(lambda x: add_target(x)) valid = test.map(lambda x: add_target(x)) . train[:3], valid[:3] . ({&#39;image&#39;: [&lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;, &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;, &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;], &#39;label&#39;: [9, 0, 0], &#39;target&#39;: [&#39;Ankle boot&#39;, &#39;T - shirt / top&#39;, &#39;T - shirt / top&#39;]}, {&#39;image&#39;: [&lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;, &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;, &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;], &#39;label&#39;: [9, 2, 1], &#39;target&#39;: [&#39;Ankle boot&#39;, &#39;Pullover&#39;, &#39;Trouser&#39;]}) . Now we can concatenate training and validation datasets into a single set of items which can be passed to fastai Datasets with index from 60000 to 70000 set aside as validation set. . concat_dsets = concatenate_datasets([train, valid]) . concat_dsets . Dataset({ features: [&#39;image&#39;, &#39;label&#39;, &#39;target&#39;], num_rows: 70000 }) . Attempting to use DataBlock . Let&#39;s first create a DataBlock and then learn how to create Datasets. In order to inform fastai on how to turn the data into DataLoaders object, 4 key pieces of info are needed. . the kind of data used for inputs and the target | Getters for the list of items | Labeling the items | Validation set creation | . Tip: It&#8217;s best to create DataBlock in an iterative manner and running datablock.summary to understand the pieces that fastai adds behind the scenes. . def get_image_attr(x): return x[&#39;image&#39;] def get_target_attr(x): return x[&#39;target&#39;] . Image.fromarray(array(train[0][&#39;image&#39;])) . def image2tensor(img): &quot;Transform image to byte tensor in `c*h*w` dim order.&quot; res = tensor(img) if res.dim()==2: res = res.unsqueeze(-1) return res.permute(2,0,1) . type(image2tensor(train[0][&#39;image&#39;])) . torch.Tensor . We will discuss image2tensor function when we discuss Attempting with Datasets . This shows how to specify the indices that need to be part of validation set. Indices 6, 7, 8 &amp; 9 are in validation and rest are in training set. . IndexSplitter(valid_idx=L(range(6, 10)))(concat_dsets) . ((#69996) [0,1,2,3,4,5,10,11,12,13...], (#4) [6,7,8,9]) . DataBlock definition is as follows . dblock = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_x=Pipeline([get_image_attr, image2tensor]), get_y=get_target_attr, splitter=IndexSplitter(valid_idx=L(range(60000, 70000))), ) . Run the DataBlock.summary to understand how fastai set up the data pipeline and perform the necessary transforms. . #Output cleared #dblock.summary(concat_dsets) . Traceback while running dblock.summary(concat_dsets) . Setting-up type transforms pipelines Collecting items from Dataset({ features: [&#39;image&#39;, &#39;label&#39;, &#39;target&#39;], num_rows: 70000 }) Found 70000 items 2 datasets of sizes 60000,10000 Setting up Pipeline: get_image_attr -&gt; image2tensor -&gt; PILBase.create KeyError Traceback (most recent call last) ~/miniconda3/envs/fastai/lib/python3.9/site-packages/PIL/Image.py in fromarray(obj, mode) 2812 try: -&gt; 2813 mode, rawmode = _fromarray_typemap[typekey] 2814 except KeyError as e: KeyError: ((1, 1, 28), &#39;|u1&#39;) . Essentially the data pipeline is as follows . Get the image attribute from the item, Convert the image to tensor. But fastai also adds the PILBase.create since we specified ImageBlock as our independent variable. This caused an issue KeyError: ((1, 1, 28), &#39;|u1&#39;) due to Image.fromarray function used in PILBase.create. From https://stackoverflow.com/questions/57621092/keyerror-1-1-1280-u1-while-using-pils-image-fromarray-pilPillow&#39;s fromarray function can only do a MxNx3 array (RGB image), or an MxN array (grayscale). . | . Internally fastai calls load_image with the item during the data pipeline creation as part of PILBase.create which fastai adds by default if we specify ImageBlock as part of blocks section in the DataBlock. | . def load_image(fn, mode=None): &quot;Open and load a `PIL.Image` and convert to `mode`&quot; im = Image.open(fn) im.load() im = im._new(im.im) return im.convert(mode) if mode else im . load_image requires a filename but dataset already is in PIL.PngImagePlugin.PngImageFile . Image.open(train[0][&#39;image&#39;]) will lead to the following error from pillow library. . AttributeError Traceback (most recent call last) &lt;ipython-input-18-ed0e9de325b8&gt; in &lt;module&gt; -&gt; 1 Image.open(train[0][&#39;image&#39;]) 1 frames /usr/local/lib/python3.8/dist-packages/PIL/Image.py in __getattr__(self, name) 544 ) 545 return self._category --&gt; 546 raise AttributeError(name) 547 548 @property . Attempt with MidLevel API Datasets . We want to convert the Image of size [28 ,28] into [1, 28, 28] as our end goal and decided to perform the item transforms using Mid Level API instead. - Datasets in MidLevel API provides more flexibility and full control over the individual item transforms performed. . Datasets is part of MidLevel API allowing the user to customize the steps involved in data processing that are not possible with DataBlockAPI . Datasets need the following pieces of information . raw items | the list of transforms that builds our inputs from the raw items | the list of transforms that builds our targets from the raw items | the split for training and validation | . For a deeper dive, refer loading the data with mid level api section on Training Imagenette tutorial and Wayde Gilliam blog post Let&#39;s iterate on the individual pieces of info. . Let&#39;s investigate the item transforms that we need for the Image. PyTorch Model expects the items to of type torch.Tensor. So I used ToTensor but it did not convert to tensors as I expected for the Image of type PIL.PngImagePlugin.PngImageFile. So I created img2tensor instead. . type(train[0][&#39;image&#39;]) . PIL.PngImagePlugin.PngImageFile . type(ToTensor()(train[0][&#39;image&#39;])) . PIL.PngImagePlugin.PngImageFile . Referred what fastai does underneath using the source code. This is the function that takes the image and converts the image to a byte tensor of shape c*h*w ie channel x height x width . def image2tensor(img): &quot;Transform image to byte tensor in `c*h*w` dim order.&quot; res = tensor(img) if res.dim()==2: res = res.unsqueeze(-1) return res.permute(2,0,1) . type(image2tensor(train[0][&#39;image&#39;])) . torch.Tensor . I created my own version of it in img2tensor . def img2tensor(im: Image.Image): return TensorImageBW(array(im)).unsqueeze(0) . img2tensor(train[0][&#39;image&#39;]).shape . torch.Size([1, 28, 28]) . tensor(train[0][&#39;image&#39;]).unsqueeze(-1).permute(2,0,1).shape . torch.Size([1, 28, 28]) . TensorImageBW(array(im)).unsqueeze(0).shape . torch.Size([1, 28, 28]) . test_eq([1, 28, 28], img2tensor(train[0][&#39;image&#39;]).shape) test_eq([1, 28, 28], image2tensor(train[0][&#39;image&#39;]).shape) . As you can see both image2tensor and img2tensor behaves the same way.Now the input item is ready. Let&#39;s look at the target. . concat_dsets . Dataset({ features: [&#39;image&#39;, &#39;label&#39;, &#39;target&#39;], num_rows: 70000 }) . fasti internally converts the label and encodes them into numbers by creating a vocabulary of labels using Categorize transform. . Categorize(vocab=ds_builder.info.features[&#39;label&#39;].names, sort=False) . Categorize -- {&#39;vocab&#39;: [&#39;T - shirt / top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;], &#39;sort&#39;: False, &#39;add_na&#39;: False}: encodes: (Tabular,object) -&gt; encodes (object,object) -&gt; encodes decodes: (Tabular,object) -&gt; decodes (object,object) -&gt; decodes . Create the Datasets definition as follows . splits = IndexSplitter(valid_idx=L(range(60000, 70000)))(concat_dsets) dsets = Datasets(concat_dsets, [[get_image_attr], [get_target_attr, Categorize]], splits=splits) . Define the item transformations and batch transformations . item_tfms = [img2tensor] # convert PILImage to tensors batch_tfms = [IntToFloatTensor] # convert the int tensors from images to floats, and divide every pixel by 255 dls = dsets.dataloaders(after_item=item_tfms, after_batch=batch_tfms, bs=64, num_workers=8) . Visualize the items in a batch . dls.show_batch() . Verify the items and their shapes in a single batch . xb, yb = dls.one_batch() . xb.shape, yb.shape . (torch.Size([64, 1, 28, 28]), torch.Size([64])) . dls.c # 10 classes as targets . 10 . dls.vocab # targets vocabulary . [&#39;Ankle boot&#39;, &#39;Bag&#39;, &#39;Coat&#39;, &#39;Dress&#39;, &#39;Pullover&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;T - shirt / top&#39;, &#39;Trouser&#39;] . Train the Model . For training a image classification model, resnet architecture (a form on convolution neural network) is used as our backbone and fully connected (fc) linear layer as our head. In order for the linear layer to predict the outputs as one of the classes, pass the number of classes in order to configure the final layer. For overview review this resource about transfer learning and fine tuning. . Review the model layers . #gpu required model = resnet34(num_classes=dls.c).cuda() . #gpu required model.avgpool, model.fc . # Uncomment this line to validate if the model accepts single batch as input # model(xb) . We can access the convolutional layers as attributes of the model. . Input to the convolutional layer is set as 3 channel (RGB) image but the images used as inputs are single channel images. Let&#39;s update the input channel to single channel. . #gpu required model.conv1 . Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) . #gpu required model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) . #gpu required model.cuda(); . #gpu required model(xb); . Learner . Learner is a class that combines the ingredients such as data, model and metrics used to train a model. . learn = Learner(dls, model, metrics=accuracy) . pretrained=False type setting is used since the fashnion mnist is not similar dataset as Imagenet so keeping all the layers are trainable. If in case pretrained=True, we may want to freeze the layers except the head and do a bit of fine tuning the head first followed by unfreeze &amp; then train all the layers. . Review the learner summary to know about the input shape, output shape, different layers involved, parameters, trainable, Optimizer used and loss function used. . learn.summary() . ResNet (Input shape: 64 x 1 x 28 x 28) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 64 x 14 x 14 Conv2d 3136 True BatchNorm2d 128 True ReLU ____________________________________________________________________________ 64 x 64 x 7 x 7 MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True ____________________________________________________________________________ 64 x 128 x 4 x 4 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 8192 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ 64 x 256 x 2 x 2 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 32768 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ 64 x 512 x 1 x 1 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 131072 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True AdaptiveAvgPool2d ____________________________________________________________________________ 64 x 10 Linear 5130 True ____________________________________________________________________________ Total params: 21,283,530 Total trainable params: 21,283,530 Total non-trainable params: 0 Optimizer used: &lt;function Adam&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - CastToTensor - Recorder - ProgressCallback . Learning rate, a hyperparameter used for training can be determined using learning rate finder function in fastai . learn.lr_find() . SuggestedLRs(valley=0.0004786300996784121) . Train the deep learning model with 6 epochs and with the learning rate obtained from previous step . learn.fit_one_cycle(6, 5e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.350417 | 0.380862 | 0.860200 | 01:17 | . 1 | 0.350359 | 0.350519 | 0.867500 | 01:18 | . 2 | 0.266588 | 0.297127 | 0.889900 | 01:19 | . 3 | 0.215826 | 0.248561 | 0.909700 | 01:16 | . 4 | 0.173092 | 0.230798 | 0.918000 | 01:18 | . 5 | 0.139866 | 0.226217 | 0.922900 | 01:17 | . Model is trained with an accuracy of 92.3%. . Exporting the trained model . . Tip: Avoid using lambda as Getters during data processing in order to export the model correctly. learn.export(fname=&#39;export.pkl&#39;) will lead to following Pickling error if lambda is used in the Data pipeline . dsets = Datasets(concat_dsets, [[lambda x: x[&#39;image&#39;]], [lambda x: x[&#39;label&#39;], Categorize]], splits=splits) . PicklingError: Can&#39;t pickle &lt;function at 0x7f378c5aeaf0&gt;: attribute lookup on main failed&lt;/p&gt; . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.export(fname=&#39;export.pkl&#39;) . Inference . Now load the learner from the exported model. Predict the item ie image using Learner.predict. This function performs the necessary transforms on the item used as part of training using the learner. . item, expected = valid[0][&#39;image&#39;], valid[0][&#39;target&#39;]; item, expected . (&lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;, &#39;Ankle boot&#39;) . learn.predict(item) . (&#39;9&#39;, tensor(9), tensor([5.0262e-13, 4.2399e-12, 1.5671e-10, 1.6854e-12, 4.1097e-14, 1.6369e-06, 1.2831e-10, 1.0092e-05, 1.8003e-10, 9.9999e-01])) . learn.dls.after_item, learn.dls.after_batch . (Pipeline: img2tensor, Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1}) . Recreating Learner.predict from the source code . We can run learn.predict?? to examine what fastai does and review each line carefully. . def predict(self, item, rm_type_tfms=None, with_input=False): dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0) inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True) i = getattr(self.dls, &#39;n_inp&#39;, -1) inp = (inp,) if i==1 else tuplify(inp) dec = self.dls.decode_batch(inp + tuplify(dec_preds))[0] dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]]) res = dec_targ,dec_preds[0],preds[0] if with_input: res = (dec_inp,) + res return res . Converting the item by applying the transforms and create a dataloader out of it. . dl = learn.dls.test_dl([item]); dl . &lt;fastai.data.core.TfmdDL&gt; . Internally learn.predict calls the get_preds method which accepts data loader and returns the input, predictions, the decoded predictions. This applies the same transforms done during training applied on the input during inference. . inp, preds, _, decoded_preds = learn.get_preds(dl=dl, with_input=True, with_decoded=True) . Examine the input shape and type . inp.shape, type(inp) . (torch.Size([1, 1, 28, 28]), fastai.torch_core.TensorImageBW) . tuplify(decoded_preds) . (tensor([9]),) . type(learn.dls.decode_batch((inp,) + tuplify(decoded_preds))), len(learn.dls.decode_batch((inp,) + tuplify(decoded_preds))) . (fastcore.foundation.L, 1) . image, prediction = learn.dls.decode_batch((inp,) + tuplify(decoded_preds))[0]; prediction . &#39;9&#39; . prediction, decoded_preds . (&#39;9&#39;, tensor([9])) . Let&#39;s examine fastai transforms done after item and after batch. . learn.dls.after_item, learn.dls.after_batch . (Pipeline: img2tensor, Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1}) . type_tfms = Pipeline([get_image_attr]) item_tfms = Pipeline([img2tensor]) batch_tfms = Pipeline([IntToFloatTensor]) . train[0] . {&#39;image&#39;: &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;, &#39;label&#39;: 9, &#39;is_valid&#39;: False} . img = type_tfms(train[0]);img.shape . (28, 28) . item_tfms(img).shape . torch.Size([1, 28, 28]) . batch_tfms(item_tfms(img).cuda()).shape . torch.Size([1, 28, 28]) . From the previous steps, we can uncover fastai magic such as transforms happening behind the scenes. . Gradio app deployment . Let&#39;s deploy the trained model and inference functionality in Gradio app and host the app in HuggingFace Space. . Steps followed . Create a new space in HF Space (Profile -&gt; New Space) | Upload the exported model export.pkl | Move all the necessary functions used as part of the transforms along with the inference provided below. This includes all the getters. | Add all the dependencies to requirements.txt | Create a gradio interface passing the classify function, specifying the inputs(Image) and outputs(Label) | See the complete Code &amp; Space | . from fastai.vision.core import PILImageBW, TensorImageBW from datasets import ClassLabel import gradio as gr from fastai.learner import load_learner def get_image_attr(x): return x[&#39;image&#39;] def get_target_attr(x): return x[&#39;target&#39;] def img2tensor(im: Image.Image): return TensorImageBW(array(im)).unsqueeze(0) classLabel = ClassLabel(names=[&#39;T - shirt / top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;], id=None) def add_target(x:dict): x[&#39;target&#39;] = classLabel.int2str(x[&#39;label&#39;]) return x learn = load_learner(&#39;export.pkl&#39;, cpu=True) def classify(inp): img = PILImageBW.create(inp) item = dict(image=img) pred, _, _ = learn.predict(item) return classLabel.int2str(int(pred)) . fname=&#39;shoes.jpg&#39; classify(fname) . &#39;Ankle boot&#39; . fname1 = &#39;t-shirt.jpg&#39; classify(fname1) . &#39;T - shirt / top&#39; . Note: This is my project write up for WalkWithFastai revisited course as one of my goal for this course is to get comfortable with low level API, debug issues diving into the source, uncovering the fastai magic. Thanks to Zach Mueller for an excellent course. . References . https://store.walkwithfastai.com/walk-with-fastai-revisited | https://walkwithfastai.com/MNIST | https://github.com/fastai/fastbook/blob/master/11_midlevel_data.ipynb | . &lt;/div&gt; .",
            "url": "https://manisnesan.github.io/chrestotes/2023/04/14/fashion-mnist.html",
            "relUrl": "/2023/04/14/fashion-mnist.html",
            "date": " • Apr 14, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Poetry First Impressions",
            "content": "Poetry First Impressions . Recently I inherited a python project which did not have dependency management setup and this post is a summary of my investigation using Poetry. . Issues happening in the project . Different python versions used by developers and in production. | setup.py not maintained | Differentiating the dev dependencies vs production dependencies is hard with a single requirements.txt file | Tracking the transitive dependencies. | Locking the dependencies when we are ready to push the code. | . Key Requirements . Reproducibility and Consistency are the key aspects for maintaining the project. | Managing dependencies in a python project and relying on a single tool (eg: pip, conda) | . Test Drive with Poetry . Poetry | . Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you. . Getting Started on Linux | . $ curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - . To start a new project poetry new &lt;project-name&gt;. If you have an existing project then poetry init - guide you through setting a project specific pyproject.toml config. . Package, version, description, author, compatible versions | Main Dependencies | Development Dependencies | | Format of pyproject.toml config . [tool.poetry] name = &quot;language-detection&quot; version = &quot;0.1.0&quot; description = &quot;Detects the language of the provided text using fasttext.&quot; authors = [&quot;Your Name &lt;you@example.com&gt;&quot;] [tool.poetry.dependencies] python = &quot;^3.6&quot; fasttext = &quot;0.9.1&quot; Flask = &quot;1.1.1&quot; gunicorn = &quot;20.0.4&quot; [tool.poetry.dev-dependencies] codecov = &quot;2.1.7&quot; flake8 = &quot;^3.8.4&quot; black = &quot;^20.8b1&quot; pytest = &quot;^6.2.1&quot; pytest-cov = &quot;^2.10.1&quot; [build-system] requires = [&quot;poetry-core&gt;=1.0.0&quot;] build-backend = &quot;poetry.core.masonry.api&quot; . | Manage virtual environments - Poetry automatically creates a virtual environments for you based on the project when you ran new or init. You can know more info using the following commands &amp; use either poetry shell or source &lt;cache_dir&gt;/pypoetry/virtualenvs/&lt;project_venv&gt;/bin/activate to activate the environment. deactivate if you want to exit the environment. . | . $ poetry env info Virtualenv Python: 3.8.6 Implementation: CPython Path: /home/msivanes/.cache/pypoetry/virtualenvs/language-detection-ddSi-Wir-py3.8 Valid: True System Platform: linux OS: posix Python: /usr $ poetry config --list cache-dir = &quot;/home/msivanes/.cache/pypoetry&quot; experimental.new-installer = true installer.parallel = true virtualenvs.create = true virtualenvs.in-project = null virtualenvs.path = &quot;{cache-dir}/virtualenvs&quot; # /home/msivanes/.cache/pypoetry/virtualenvs . Add dependency through poetry add &lt;package&gt; or poetry add &lt;package&gt;@&lt;version&gt; . $ poetry add Flask . $ poetry add fasttext@0.9.1 . $ poetry add pytest . | Remove a dependency using poetry remove &lt;package&gt; . $ poetry remove pyfasttext . | Once you finish adding the dependencies, run poetry install to install all the dependencies in your project. This will generate the poetry.lock file. This is where all the dependencies are locked and needs to be version controlled. If any other developer run poetry install then poetry uses the exact same versions specified in the dependencies while installing. This ensures all the developers are using a consistent dependencies across the board. . | If you want to upgrade the dependencies and use the latest versions, then use poetry update. This is the equivalent of deleting the poetry.lock and using install . . | Exporting the requirements.txt if you want to containarize your app. . $ poetry export --without-hashes -f requirements.txt &gt; requirements.txt . | For scenarios in libraries where you want to perform an editable install and hence you need a setup.py file. (Github user @albireox provided a script to quickly create setup.py which I found it here . | . #!/usr/bin/env python # -*- coding: utf-8 -*- # # @Author: José Sánchez-Gallego (gallegoj@uw.edu) # @Date: 2019-12-18 # @Filename: create_setup.py # @License: BSD 3-clause (http://www.opensource.org/licenses/BSD-3-Clause) # This is a temporary solution for the fact that pip install . fails with # poetry when there is no setup.py and an extension needs to be compiled. # See https://github.com/python-poetry/poetry/issues/1516. Running this # script creates a setup.py filled out with information generated by # poetry when parsing the pyproject.toml. import os import sys from distutils.version import StrictVersion # If there is a global installation of poetry, prefer that. lib = os.path.expanduser(&#39;~/.poetry/lib&#39;) vendors = os.path.join(lib, &#39;poetry&#39;, &#39;_vendor&#39;) current_vendors = os.path.join( vendors, &#39;py{}&#39;.format(&#39;.&#39;.join(str(v) for v in sys.version_info[:2])) ) sys.path.insert(0, lib) sys.path.insert(0, current_vendors) try: try: from poetry.core.factory import Factory from poetry.core.masonry.builders.sdist import SdistBuilder except (ImportError, ModuleNotFoundError): from poetry.masonry.builders.sdist import SdistBuilder from poetry.factory import Factory from poetry.__version__ import __version__ except (ImportError, ModuleNotFoundError) as ee: raise ImportError(&#39;install poetry by doing pip install poetry to use &#39; f&#39;this script: {ee}&#39;) # Generate a Poetry object that knows about the metadata in pyproject.toml factory = Factory() poetry = factory.create_poetry(os.path.dirname(__file__)) # Use the SdistBuilder to genrate a blob for setup.py if StrictVersion(__version__) &gt;= StrictVersion(&#39;1.1.0b1&#39;): sdist_builder = SdistBuilder(poetry, None) else: sdist_builder = SdistBuilder(poetry, None, None) setuppy_blob = sdist_builder.build_setup() with open(&#39;setup.py&#39;, &#39;wb&#39;) as unit: unit.write(setuppy_blob) unit.write(b&#39; n# This setup.py was autogenerated using poetry. n&#39;) . Challenges . I still have issues with pyenv which I need to figure how to easily switch between python versions | . References . Poetry Documentation | .",
            "url": "https://manisnesan.github.io/chrestotes/python/reproducibility/2021/01/09/poetry-first-impressions.html",
            "relUrl": "/python/reproducibility/2021/01/09/poetry-first-impressions.html",
            "date": " • Jan 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Intent detection",
            "content": "Introduction . Intent Classification is a type of supervised text classification problem. It is used to predict the motivation of the user providing search keywords in a given search engine. Each user visiting the site have a goal in mind and they express in the form of keywords. We have to optimize for satisfying their goal in addition to returning the best documents matching the keywords as search results. In Search for intent, Not inventory[1] Daniel Tunkelang author of Query Understanding publication emphasizes that . &quot;Searchers don’t care what you have in your catalog or how you organize your inventory. They care about what they want. Make it easy for searchers to express their intent, and craft the search experience around their intent.&quot; . Hence as part of Red Hat Customer Portal personalization, we started working on identifying the intents for our site visitor and classify the search queries in order to craft the search experience based on their intent. . Related Work . Classification of searches is domain specific and there is no one size fits all approach. Taxonomy of Web Searches[2] classify the queries into these categories 1. Navigational 2. Informational 3. Transactional . . Taxonomy of Ecommerce Searches[3] classifies them into following categories &amp; each with a distinct search behavior. . Shallow Exploration Queries are short vague queries that a user may use initially in exploring the product space. | Targeted Purchase Queries are queries used by users to purchase items that they are generally familiar with, thus without much decision making. | Major-Item Shopping Queries are used by users shopping a major item relatively expensive &amp; requires some serious exploration, but typically in a limited scope of choices. | Minor-Item Shopping Queries are provided by users to shop for minor items that are generally not very expensive, but still require some exploration of choices. | Hard-Choice Shopping Queries are used by users who want to deeply explore all the candidate products before finalizing the choice often appropriate when multiple products must be carefully compared with each other. | Examples . . . We looked at how others are handling the intent classification problem because of the shorter length of the text and latency requirement. Deep Search Query Intent Understanding from LinkedIn provides two designs for modeling 1. Predicting the intent in typeahead search using character models 2. Predicting the intent for complete search queries to do Seach Results Page(SERP) Blending. Each linkedin usecase has its own latency and accuracy requirement, former having higher latency but with lower accuracy whereas the latter with acceptable latency but with higher accuracy. Though they explored deep learning CNN, LSTM, BERT models in this paper, production baseline model used is the traditional logistic regression model. Unless you can reduce the size of the model, latency will pose a challenge for productionizing these bigger models.&gt; &quot;The production baseline model is a logistic regression model, with bag-of-words features, and user profile/behavioral features, etc. &quot; . The next one is Discovering and Classifying In-app Message Intent at Airbnb where they classify the guest messaging intents such as Parking, Checking In and try to provide timely responses. Here they found CNN performed better and has faster training with a shorter serving time. . . Problem Framing . We leveraged the redhat customer portal top tasks survey in which most of the users indicated the aspect of problem solving as the main reason for the visit in 4/5 responses. Keeping that in mind, we analyzed the search queries for the patterns and came up with these four buckets. . Broad Explorational are single worded vague queries provided by users exploring the product portfolio, a component, vulnerability. | Targeted Queries (Eg: CVE-1243, RHBA-2351, RHSA-3194) such as Bug fixes, Advisories, Enhancements. | Usage Queries are provided by users wanting to know how to use specific component or aspect of a product. | Troubleshoot are provided by users facing an issue and more likely leading to case creation. | Instead of providing the model with poor data, we wanted the model to focus only on classifying the Usage and Troublesoot. We avoided passing the models with vague queries and targeted queries and let the keyword search, onebox and curated searches to handle these types of queries. . INTENT DEFINITION EXAMPLES . Troublesoot | When a user reports an issue/error message or expresses the text in negative forms explicitly that he/she wants to troubleshoot/debug, then such statements are considered to be of TROUBLESHOOT intent | X Window won&#39;t start after updating xorg-x11-server-Xorg package? | . | . Usage | USAGE intent is considered when it is more along the lines of how to use a product/component or perform an action. | How can I block video streaming using squid ? | . Data Collection . In order to iterate and perform rapid prototyping, we leverage ML teaching and annotation tool Prodigy. The recommendation is to perform manual annotation with binary label in order to keep cognitive load low. This allows the annotator to focus only one concept at a time. We started looking at the Portal Search queries and manually annotated them as TROUBLESHOOT or not. Please check this excellent text classification tutorial by Ines Montani. . If you only provide a single label, the annotation decision becomes much simpler:does the label apply or not? In this case, Prodigy will present the question as a binary task using the classification interface. You can then hit ACCEPT or REJECT. Even if you have more than one label, it can sometimes be very efficient to make several passes over the data instead of selecting from a list of options. The annotators can focus on one concept at a time, which can reduce the potential for human error – especially when working with complicated texts and label schemes. $ prodigy textcat.teach troubleshoot en_core_web_sm troubleshoot.jsonl . We collected around 1500 training samples to bootstrap the project with the help of Active learning features provided. Learn more about active learning from here . Example training samples . {&quot;text&quot;:&quot;add new nfs storage&quot;,&quot;label&quot;:&quot;USAGE&quot;,&quot;answer&quot;:&quot;accept&quot;} {&quot;text&quot;:&quot;sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target&quot;,&quot;label&quot;:&quot;TROUBLESHOOT&quot;,&quot;answer&quot;:&quot;accept&quot;} . Imports and Utilty functions . def common_word_removal(df, common_words): &quot;&quot;&quot;This is to remove manually identified common words from the training corpus.&quot;&quot;&quot; # Manual list of common words to be removed based on above data. This will change based on text data input. # We cannot automate this to directly omit the first x words as these words are very relevant to our use case # of inferring intent df[&#39;text&#39;] = df[&#39;text&#39;].apply(lambda x: &quot; &quot;.join(x for x in x.split() if x not in common_words)) return df def drop_nulls(df): &quot;&quot;&quot;This function drops the rows in data with null values. It takes in the raw queries as the input.&quot;&quot;&quot; print(&#39;Number of queries with null values: &#39; + str(df[&#39;text&#39;].isnull().sum())) df.dropna(inplace=True) return df def avg_word(sentence): &quot;&quot;&quot;Used in EDA to calculate average wordlength per query&quot;&quot;&quot; words = sentence.split() return (sum(len(word) for word in words)/len(words)) . Data Preparation . Loading the data in dataframe . This reads the dataset in json lines format into pandas dataframe. . def load(): raw_data - load_raw_data(pd.read_json(f&quot;{DATA_DIR}/portalsearch/portalsearch.jsonl&quot;, lines=True)) # Joining the &#39;label&#39; and &#39;answer&#39; column to become one. # E.g.: Troubleshoot-accept, Troubleshoot-reject, Usage-accept, Usage-reject raw_data[&#39;label&#39;] = raw_data[&#39;label&#39;] + &#39;-&#39; + raw_data[&#39;answer&#39;] # Only selecting the &#39;accept&#39; labels. raw_data = raw_data[raw_data[&#39;label&#39;].isin([&#39;TROUBLESHOOT-accept&#39;,&#39;USAGE-accept&#39;])] raw_data.drop(&#39;answer&#39;,axis=1,inplace=True) return raw_data . Preprocessing . We performed the following preprocessing . Adding standard features like word count, character count and average word length per query to the dataframe. | Remove captalization by lowercasing all words | Removing punctuations and special characters | Remove traditional stopwords like &quot;the&quot;, &quot;a&quot;, &quot;an&quot; and &quot;and&quot; | Remove noisy data that are of low value and commonly occuring terms | Lemmatization | . def eda(df): &quot;&quot;&quot;Classic EDA like word count per query, average word length per query etc.&quot;&quot;&quot; # Word count per query df[&#39;word_count&#39;] = df[&#39;text&#39;].apply(lambda x: len(str(x).split(&quot; &quot;))) # Character count per query df[&#39;character_count&#39;] = df[&#39;text&#39;].str.len() # this also includes spaces # Average word length per query df[&#39;avg_wordlength_per_query&#39;] = df[&#39;text&#39;].apply(lambda x: avg_word(x)) return df . def preprocessing(df): &quot;&quot;&quot;Traditional Pre-processing steps.&quot;&quot;&quot; # Lower case for all words. This helps in removing duplicates later df[&#39;text&#39;] = df[&#39;text&#39;].apply(lambda x: &quot; &quot;.join(x.lower() for x in x.split())) # Removing Punctuations and special characters using regular expression df[&#39;text&#39;] = df[&#39;text&#39;].str.replace(&#39;[^ w s]&#39;,&#39;&#39;) # Removing non-english words df[&#39;text&#39;] = df[&#39;text&#39;].apply(lambda row: row.encode(&#39;ascii&#39;,errors=&#39;ignore&#39;).decode()) # Removing stopwords df[&#39;text&#39;] = df[&#39;text&#39;].apply(lambda x: &quot; &quot;.join(x for x in x.split() if x not in stop)) # Lemmatization lemmatizer = WordNetLemmatizer() df[&#39;text&#39;] = df[&#39;text&#39;].apply(lambda x: &quot; &quot;.join([lemmatizer.lemmatize(word) for word in x.split()])) # Common word removal # Remove commonly occurring words from our text data # as their presence will not of any use in classification of our text data. # This step would require manual scanning through the list. frequent_words = pd.Series(&#39; &#39;.join(df[&#39;text&#39;]).split()).value_counts() print(&#39;Top 20 frequent words are n&#39; + str(frequent_words[:20])) print(&#39; n&#39;) # Rare words removal # Because they’re so rare, the association between them and other words is dominated by noise. # Hence we can remove them and later decide whether or not the results improved based on it. # Printing out rare words occuring less than 50 times. rare_words = frequent_words[frequent_words &lt; 50] print(&#39;Top 10 rare words are: n&#39; + str(rare_words[-10:])) print(&#39; n&#39;) # Dropping queries which are empty after all the pre-processing df[&#39;text&#39;].replace(&#39;&#39;, np.nan, inplace=True) df.dropna(inplace=True) print(&#39;The final number of queries after preprocessing are: &#39; + str(df.shape)) return df . Modeling . Train Test Split . Next we split the data into training and test data with 3:1 split . def train_test_splits(df, test_size): &quot;&quot;&quot;Splitting raw data into training and test data.&quot;&quot;&quot; X = raw_data[&#39;text&#39;] y = raw_data[&#39;label&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42) return X_train, X_test, y_train, y_test . # Splitting raw dataset into testing and training data X_train, X_test, y_train, y_test = train_test_splits(raw_data,0.33) . Vectorization . The text needs to be converted into a format that a model could interpret ie numbers. This process is called vectorization. We started with Term Frequency-Inverse Document Frequency(TF-IDF) method using sklearn TfidfVectorizer . TF-IDF is a way to calculate the ‘importance’ of each word in the dataset. This vectorizer calculates how often a given word appears in the string, and downscales words that appear across different strings. . An example illustrating how tfidf is calculate for two strings . . Code sample . def text_vectorization_tfidf(X_train,X_test):&quot;&quot;&quot;tf-idf vectorization using sklearn&quot;&quot;&quot; vectorizer = TfidfVectorizer() X_train_vec = vectorizer.fit_transform(X_train) X_test_vec = vectorizer.transform(X_test) # pretty printing the vocabulary built by the tf-idf vectorizer # pprint.pprint(vectorizer.vocabulary_) return vectorizer, X_train_vec, X_test_vec # Text Vectorization of training data vectorizer, X_train_vec, X_test_vec = text_vectorization_tfidf(X_train, X_test) . LinearSVC . The algorithm we used is a linear support vector classifier (SVC), a commonly used text classification algorithm that works by finding the line or hyper-plane that best differentiates two groups of data points. It is a Support Vector Machine with a linear kernel.Learn more about SVM from here Here we are performing 3-fold cross validation to improve the generalization and minimize the overfitting on validation set. With the given training set, it is split into 3 folds and one of the fold is used for validation and a score is calculated. Similarity this process repeated with the rest of the folds and the average of all the scores is used as the final score. Please see the excellent scikit guide here for additional details . def build_model(X_train_vec, y_train): &quot;&quot;&quot;Build an SVM model with a linear kernel&quot;&quot;&quot; svm = LinearSVC(class_weight=&quot;balanced&quot;) linear_svc = CalibratedClassifierCV(svm,method=&#39;sigmoid&#39;) # 3-fold Cross-Validation print(cross_val_score(linear_svc, X_train_vec, y_train, cv=3)) # Fitting the model after tuning parameters based on cross-validation results linear_svc.fit(X_train_vec, y_train) return linear_svc # Building, cross validation and fitting of model model = build_model(X_train_vec, y_train) . Evaluation . The model can be interpreted by looking at the confusion matrix of true labels and predicted labels. Confusion matrix(a.k.a error matrix) allows to understand the performance of the model on unseen or test queries. We can see the accuracy of the model to be around 95% . def make_predictions(model,X_test_vec): &quot;&quot;&quot;Makes predictions and spits out confusion matrix.&quot;&quot;&quot; # Predicting results on test data predictions = model.predict(X_test_vec) # Accuracy of model print(&#39;Accuracy of model is &#39; + str(round(accuracy_score(y_test, predictions)*100,2)) + &#39;%&#39;) # Precision, Recall and other metrics print(str(classification_report(y_test, predictions, target_names=[&#39;TROUBLESHOOT-accept&#39;, &#39;USAGE-accept&#39;]))) # Confusion Matrix labels = [&#39;TROUBLESHOOT-accept&#39;,&#39;USAGE-accept&#39;] Confusion_Matrix = confusion_matrix(y_test, predictions, labels) # Plotting the confusion matrix df_cm = pd.DataFrame(Confusion_Matrix, index = labels, columns = labels) ax = plt.figure(figsize = (12,8)) sns.heatmap(df_cm, annot=True,fmt=&#39;g&#39;) # labels, title and ticks ax.suptitle(&#39;Confusion Matrix&#39;) plt.xlabel(&#39;Predicted labels&#39;, fontsize=18) plt.ylabel(&#39;True labels&#39;, fontsize=16) . . Deployment . Web application . Once the model is trained and saved, the next step is to create a simple REST endpoint called introspect where the client applications can send the text blob and get the intent prediction as the response. The application endpoint is built using python, flask web framework, gunicorn web containier with 4 parallel workers. The overview of the application is as follows . Loading the model at the application startup | Once the query is received, send it to the model only if it is not a known lexical pattern like CVE, Errata, numeric id, url. This prevents garbage inputs and invalid predictions. | If the query is one of the known patten return the response with the intent as OTHER. | Transform the query into a vectorized form before model prediction | Predict the query and return the prediction, probablity of the predicted class as confidence score. | . Containerizing the application . Basically containeraization is a modern way to package the application with your code, dependencies, configuration into a format (Eg: Docker Image) suitable to run anywhere whether it&#39;s public cloud like aws or in your own datacenter. . Code Sample for util.py . from flask import request, Response from validators import ValidationFailure from validators.url import url from functools import wraps import logging def is_url(keyword: str) -&gt; bool: &#39;&#39;&#39; Returns True if the keyword contains a valid url else return False &#39;&#39;&#39; try: value = url(keyword) except ValidationFailure: return False return value def has_known_lexical_pattern(keyword: str) -&gt; bool: &#39;&#39;&#39; Return True if the keyword contains known navigational intent like CVE, Errata, id or url else returns False &#39;&#39;&#39; #Navigational intent for CVE, Errata, id or url return keyword.lower().startswith((&#39;cve&#39;, &#39;(cve&#39;, &#39;rhsa&#39;, &#39;(rhsa&#39;, &#39;rhba&#39;, &#39;(rhba&#39;)) or keyword.isdigit() or is_url(keyword) def has_other_intent(query: str) -&gt; bool: return has_known_lexical_pattern(query) def invalid_input(): &quot;&quot;&quot;Sends a 400 Bad Request&quot;&quot;&quot; return Response( &#39;ensure this value has at mininum 3 characters and most 500 characters&#39;, 400, {}) def validate_input (f): @wraps(f) def decorated(*args, **kwargs): try: query = request.args [&#39;query&#39;] if len (query) &lt; 3 or len (query) &gt; 500: raise Exception () return f(*args, **kwargs) except: logging.info(&quot;Input validation failed&quot;) return invalid_input () return decorated . Code Sample for predict.py Intent Prediction . from flask import Flask, jsonify, request, Response from functools import wraps from util import validate_input, has_other_intent import json, pickle, datetime, logging, uuid from typing import Dict application = Flask(__name__) def load_model() -&gt; None: &#39;&#39;&#39; Initialize the global variables to load the model. &#39;&#39;&#39; global vectorizer, model vectorizer = pickle.load(open(&quot;./models/tfidf_vectorizer.pkl&quot;, &quot;rb&quot;)) model = pickle.load(open(&quot;./models/intent_clf.pkl&quot;, &quot;rb&quot;)) def initialize (): load_model() def strip_accept_label(prediction:str) -&gt; str: if &#39;-&#39; in prediction: return prediction.split(&#39;-&#39;)[0] return prediction @application.route(&quot;/introspect&quot;, methods = [&quot;GET&quot;]) @validate_input def introspect() -&gt; Dict: &quot;&quot;&quot; Intent Prediction &quot;&quot;&quot; query = request.args [&#39;query&#39;] # Return intent as OTHER for CVE, Errata (RHBA, RHSA), id or url if has_other_intent(query): response = {&quot;query&quot;: query, &quot;intent&quot;: &#39;OTHER&#39;, &quot;confidence_score&quot;: 1, &quot;req_id&quot;: str(uuid.uuid4())} logging.info(f&quot;Prediction response : {response}&quot;) return response query_transformed = vectorizer.transform([query]) prediction = model.predict([query_transformed.toarray()[0]])[0] prob = model.predict_proba([query_transformed.toarray()[0]])[0] confidence_score = round(max(prob), 2) response = {&quot;query&quot;: query, &quot;intent&quot;: strip_accept_label(prediction), &quot;confidence_score&quot;: confidence_score, &quot;req_id&quot;: str(uuid.uuid4())} return response initialize() if __name__ == &quot;__main__&quot;: initialize() application.run(debug = True, host = &quot;0.0.0.0&quot;, port = &quot;8080&quot;) . Dockerfile sample . FROM registry.access.redhat.com/rhscl/python-36-rhel7 USER root ADD . /opt/customer-portal-search-intent/ WORKDIR /opt/customer-portal-search-intent RUN wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/intent_clf-version1.pkl &amp;&amp; wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/intent_clf.pkl &amp;&amp; wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/tfidf_vectorizer-version1.pkl &amp;&amp; wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/tfidf_vectorizer.pkl &amp;&amp; mv *.pkl models/ &amp;&amp; pip install -r requirements.txt USER 1001 EXPOSE 8080 8443 ENTRYPOINT [&quot;./run_intent_detection_service&quot;] CMD [ ] . run_intent_detection_service . #!/bin/bash # Lay down the cert for this server KEYFILE=cert/rsa.key CERTFILE=cert/ssl.crt BIND=&quot;127.0.0.1:8080&quot; if [ -f $KEYFILE ] &amp;&amp; [ -f $CERTFILE ] ; then OPTS=&quot;--keyfile $KEYFILE --certfile $CERTFILE $OPTS&quot; BIND=&quot;0.0.0.0:8443&quot; fi # num_workers = (2 * cpu) + 1 =&gt; 9 OPTS=&quot;$OPTS -b $BIND --workers 5 --log-level=DEBUG &quot; export REQUESTS_CA_BUNDLE=$(pwd)/root.crt set -x gunicorn main:application $OPTS --access-logfile - --access-logformat &quot;{&#39;remote_ip&#39;:&#39;%(h)s&#39;,&#39;request_id&#39;:&#39;%({X-Request-Id}i)s&#39;,&#39;response_code&#39;:&#39;%(s)s&#39;,&#39;request_method&#39;:&#39;%(m)s&#39;,&#39;request_path&#39;:&#39;%(U)s&#39;,&#39;request_querystring&#39;:&#39;%(q)s&#39;,&#39;request_timetaken&#39;:&#39;%(D)s&#39;,&#39;response_length&#39;:&#39;%(B)s&#39;}&quot; . Building Image and Deployment on OpenShift . In order to make the service available to our users we are going to lean on OpenShift, container platform for building the image and deployment. . Once we have trained our model and have the application packaged in a Dockerfile, all we need to provide is BuildTemplate and DeploymentTemplate. . BuildTemplate provides information such as source repository where the Dockerfile(assumed to be at the root), what buildstrategy to use(in this docker strategy) and finally about the ImageRepository for storing the built images. . DeploymentTemplate contains info about cpu, memory requirements on the deployments, number of pods(instances of the services) for availablility and seamlessly transition between the instances during deployment rollouts without service interruption. . Integration . Let&#39;s see how this model is integrated with the overall product ecosystem.As a first step in using the service, we wanted to avoid any risks and carefully provide the options to the user to choose troubleshoot experience when we detect the same intent from the query. . In the below example user searching for kernel panic occuring in Red Hat Linux systems, the service predicted the TROUBLESHOOT with greater 70% confidence and a banner showing option to user allowing them to choose TROUBLESHOOT experience. . . Conclusion . In this post, we covered how we went from ideation, data collection, module building, deployment and finally integrating with the product. | There are constraints such as shorter text, latency, model size in choosing a modeling technique for intent classification and creating a simpler &amp; traditional model such as LinearSVC can always be a better fit for such scenarios. | . References . Search: Intent, Not Inventory | Broder, Andrei. (2002). A Taxonomy of Web Search. SIGIR Forum. 36. 3-10. 10.1145/792550.792552 | Sondhi, Parikshit &amp; Sharma, Mohit &amp; Kolari, Pranam &amp; Zhai, ChengXiang. (2018). A Taxonomy of Queries for E-commerce Search. 1245-1248. 10.1145/3209978.3210152 | Deep Search Query Intent Understanding | Discovering and Classifying In-app Message Intent at Airbnb | Prodigy - ML teaching and annotation tool | SVM | Active Learning | OpenShift Builds 10.OpenShift Deployments |",
            "url": "https://manisnesan.github.io/chrestotes/intent-detection/text-classification/information-retrieval/query-understanding/machine-learning/2020/09/20/intent-detection.html",
            "relUrl": "/intent-detection/text-classification/information-retrieval/query-understanding/machine-learning/2020/09/20/intent-detection.html",
            "date": " • Sep 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Tracking Data and Model in Machine Learning projects",
            "content": "Why should you track everything . Difference between software engineering and machine learning projects | code, config, dependencies vs code, data, model, config, dependencies | Myriad of Data files (raw, intermediate) | Not in repository | Reproducible ML models targeting metrics [Experiments Link] | Tuning and Experimentation tracking | Model Deployment &amp; Revert | . DVC . DVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code. . | Experiments are tracked by combining the Code + Data files | Data files Local cache | Data remotes in S3, SSH etc | . | Metrics per experiment | Pipelines | . Install DVC on Fedora/RHEL/CentOS . sudo wget https://dvc.org/rpm/dvc.repo -O /etc/yum.repos.d/dvc.repo sudo yum update sudo yum install dvc . Add DVC files in the Git (It is not required but it is good to have project and dvc config in the same directory) . Workflow for Model Packaging . Setup for connecting to model repository during training development process | Push the trained model to the repository | Pull the model from the repository during the build process | . Pushing model files . dvc init dvc remote add -d gss-rdu-remote ssh://msivanes@gss-rdu-repo.usersys.redhat.com:/var/www/html/repo/config/ulmfit dvc add cases_small_sbr_08-06-2020.pkl git commit -am “Add ulmfit model to project” dvc push -v . Pulling the model . git clone $REPO git pull dvc pull # Pulls the data from remote-storage. Equivalent to dvc fetch followed by dvc checkout dvc checkout #Update model files . Food for thought . Rather than feature branches, think in terms of experiment branches targeting metrics. | Keep Data and Model files stored outside the repository | Model files built as part of build process. | Smoke test should validate the constructed model using validation set. | . References . DVC | Cheatsheet | https://christophergs.github.io/machine%20learning/2019/05/13/first-impressions-of-dvc/ | https://www.slideshare.net/DmitryPetrov15/pydata-berlin-2018-dvcorg | .",
            "url": "https://manisnesan.github.io/chrestotes/machine-learning/2020/06/29/tracking-data-model-using-dvc.html",
            "relUrl": "/machine-learning/2020/06/29/tracking-data-model-using-dvc.html",
            "date": " • Jun 29, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
            "content": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset . Overview of covidex.ai . This is a paper summary of deploying a Neural Search Engine to answer questions from the COVID-19 dataset. . Paper | covidex.ai | Twitter Announcement by Jimmy Lin | . Neural Covidex applies state-of-the-art neural network models and artificial intelligence (AI) techniques to answer questions using the COVID-19 Open Research Dataset (CORD-19) provided by the Allen Institute for AI (data release of April 3, 2020). This project is led by Jimmy Lin from the University of Waterloo and Kyunghyun Cho from NYU, with a small team of wonderful students: Edwin Zhang and Nikhil Gupta. Special thanks to Colin Raffel for his help in pretraining T5 models for the biomedical domain.” . . Motivation . The ongoing pandemic crisis poses a huge challenge to get timely information for public health officials, clinicians, researchers, virologists. In order to respond to this challenge, Allen AI publishes a COVID-19 data set (CORD-19) in collaboration with other research groups. The source for this data set is both research articles published about coronovirus and other related research articles. The aim of this effort is to bring researchers . to apply language processing techniques in order to generate insights &amp; make data driven decisions. | to provide ways for the front line to consume the recent developments in a digestible form &amp; apply in the field. | . Outcomes . Jimmy Lin &amp; his research team responded to this call. The two strategies adopted were . Real time users should be able to find answers to any questions associated with COVID | Other Researchers should be able reuse the components they build. Providing a modular and reusable components is set as part of the requirements. | . The team decided to build end to end real time search application called covidex.ai . They developed the components that powers this engine in a short span of time for the information retrieval need . . keyword based search interface : This also provides faceted navigation in the form of filters like author, article source, time range and highlighting words from the results that matches with user query. | neural ranking component that sorts the results with the top most results answering user’s question. | . Background &amp; Related Work . Traditional search architecture comprises of two phases Content Indexing and Keyword Searching. During indexing, content is transformed into an Indexable form called as Document The search engine convert this document into a fundamental data structure called as Inverted Index. This is similar to what you see in Book Appendix where terms are mapped towards the pages. Similarly Inverted index contains terms mapped towards docIds where the term appears &amp; position in the document. . Searching phase is further divided into two stages retrieval stage and ranking stage. In the first stage, given a search term(s) you will retrieve the list of matching documents from the inverted index. In the second stage, the matched documents are sorted based on the computed relevance score. . Modern Search Architectures . More modern multi-stage search architectures from Bing &amp; Alibaba expand the Search Phase with additional reranking stages. Except for the first retrieval stage, the additional subsequent ranking stages will rerank and refine the results further from previous stages . . Modular and Reusable Keyword Search . Anserini . Anserini is an opensource Information Retrieval toolkit in order to solve the reproducibility problems in research and bridge the gap between research and real world systems. This is a tool that is built on top of Lucene, a popular open source search library &amp; enhanced with features specific for conducting IR research. pyserini is a python interface to Anserini. . Indexing . The first challenge faced by the team is representing the Indexable Document. This is fundamental unit of search engine. Results are basically collection of documents. Eg: Tweets, WebPage, Article . One of the common challenge wrt Information Retrieval systems, they tend to favor longer documents. In order to give all documents a fairer chances irrespective of its length, normalization is needed. . The articles are in the following format . { title : “Impact of Masks”, abstract: “Masks protects others from you.” body_text : “Effectiveness of mask /n /n This is being studied …..” } . In order to compare the effectiveness, the team decide to index the articles in 3 different ways . Index only the title and abstract of the article as a document | Index the entire text of the article as a document combining title, abstract and body_text | Break the body_text of the article into paragraphs and each paragraph as separate documents. In this case, the Indexable Document is title, abstract &amp; the paragraph. | . Searching . Once the team built the lucene indices based on the above scheme for CORD-19, we can able to search for a given term, retrieve matching documents and ranked them using BM25 scoring function. These indices are available for the researchers to perform further research. . The full search pipeline for keyword search is demonstrated using notebooks using Pyserini. . In order to provide the users a live system that can be used to answer questions, the team leveraged their earlier work on anserini integration with Solr , open source search platform. The user search interface is built using Blacklight discovery interface with Ruby on Rails for faceting &amp; highlighting. . . Highlighting . In addition to that the team also built a highlighting feature on of keyword search. This allows the user to quickly scan the results with the matched keywords in the document. . It is built using BioBERT. The idea behind it is that a) the candidate matched documents &amp; convert them into sentences. b) Similarly the query is treated as a sentence. The sentences &amp; query are in turn converted into its numerical representations (vectors). Top sentences closer to the query are obtained using the cosine similarity. The top-K words in the context are highlighted in these top sentences. . Neural Covidex for reranking . The research group was already working on the neural architectures specifically applying transfer learning on retrieval/ranking based problems. . BERT for query based Passage Ranking : Applying transfer learning for passage reranking pre-trained on MS-MARCO dataset | BERTserini for retrieval-based question answering: Incorporating Anserini Retriever to retrieve the top K segments of text followed by BERT based pre-trained model to retrieve the answer span. | . Typically the task of reranking is turn the problem into a classification task where we take the query, candidate_document &amp; predict the target as (relevant, not-relevant). To avoid the costly operation of performing classification on the entire corpus, this is applied at the reranking stages. The engine gets the top K documents from the previous retrieval stage and rerank them using machine learning model. As part of reranking stage, the team leveraged Sequence to Sequence Model for reranking (Nogueira et al. 2020). . Stages involved in training the reranking model using Transfer Learning Methodology . Pre-training →Fine Tuning → Inference . [Pre-training] Transformer based Language Model trained on MS Marco dataset | [Fine Tuning] Given a query q, document D, the model is fine tuned to predict the output as either true or false as targets indicating the relevance. | [Inference] In reranking setting, for each candidate documents, predict the prob distribution of (relevant, non-relevant) and sort the scores of relevant doc(true outputs) alone. | . Training a language model and the encoder from this fined tuned language model is normally used for the downstream tasks like Classification in transfer learning methodology. But this method of applying Sequence to Sequence model (based on T5) is quite new for document ranking setting (Nogueira et al., 2020). . The reasoning provided was the predicted target words can capture the relatedness through pre-training. This is based on encoder-decoder architecture &amp; uses a similar masked language modeling objective. Given a query, document the model is fine tuned to produce true or false if the document is relevant or not to the query. . Challenges in Results Evaluation . The authors rightfully mention that the individual components comprising such a system is evaluated against various test datasets. But as this is specific to an evolving dataset like CORD-19, there is no such existing test collections. . | It is not always necessary that ranking is the most important for such an end to end system. We have to switch to an outcome based measure rather than a single output based measure like batch retrieval evaluations(MRR, nDCG). Eg: “Did the researchers, practitioners get their questions answered?” How many of them are not finding the answers? So involving human in the loop to qualitatively evaluate the results is essential to know if the system is really contributing towards the efforts fighting the pandemic. . | What if the exploratory users do not know the right type of keywords to use ? In that case ranking is a wrong goal to pursue. . | Current challenge is all the targeted users are working on the front line and hard to provide qualitative feedback about search experience. So the author asks for more hallway usability testing to gather insights from the users. . | . Author Reflections . An end to end system like Covidex is not possible without the power of the current Open Source Software ecosystem, Open culture of curating, cleaning &amp; sharing the data with the community (Thanks to CORD-19 by Allen AI) and pre-trained language models like MS-Marco etc. . Good software engineering practices is the foundation for a team and ensure that the underlying software components can be replicated &amp; reused to provide this system. This is essential to rapidly explore and experiment with new ideas. . Building a strong research culture to produce the results in the form of open source software artifacts aid the community in reproducing the results and build on top of it. . Reminder about the mismatch between producing research code for conference and building a system for a real users. For example concerns like latency of search requests, throughput about the number of users, deploying &amp; managing a system in production, user experience does not arise in a research setting. . Insights &amp; Takeaways from this paper . Lack of proper training data &amp; human annotators is a common challenge. Leveraging pre-trained models on MS-MARCO is critical for ranking tasks in this type of situation. | The experimentation mindset need to be adopted and one need to interactive computing tools like pyserini to experiment with search index. This allows the search practitioners to constantly iterate and learn from these experiments. | Adoption of Openness in not only the source but science and data &amp; the way we work is truly inspiring. | .",
            "url": "https://manisnesan.github.io/chrestotes/information-retrieval/deep-learning/papers/2020/05/25/Paper_Summary_Covidex.html",
            "relUrl": "/information-retrieval/deep-learning/papers/2020/05/25/Paper_Summary_Covidex.html",
            "date": " • May 25, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://manisnesan.github.io/chrestotes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://manisnesan.github.io/chrestotes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}