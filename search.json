[
  {
    "objectID": "drafts/vaccination_tweets_sp.html",
    "href": "drafts/vaccination_tweets_sp.html",
    "title": "Starter Notebook for Text Classification using fastai",
    "section": "",
    "text": "Dataset from the competition of our choice\n\nUnderstand the problem and the evaluation metric used\n\nSetup fastai, Config to store the constants and imports\nDownload the data\nCleaning\nUnderstand the structure of the data in order to setup the datablock\n\ndata is balanced or imbalanced\nwhere to fetch the data from\ntrain-test split, with stratify or cross-validation(kfold, stratifiedKFold)\ndecide if preprocessing is required\ndecide on the tokenization(or subword)\ndebugging help\n\nPost DataBlock setup\n\ncheck the one-batch\ncheck the summary\n\nLanguage Model\n\nAdd Mixed Precision Training\nAWD_LSTM\nintegrate callbacks for weights and biases\ntrain the language model using discriminative learning rates\nsave the encoder\n\nClassification Model\n\nuse the vocab from language model dataloaders\nuse the encoder on the learner\ntrain the model\ngradual unfreezing\n\nFuture\n\nHow to extend it further(by making it public - add Tendo’s share here)\nadd pratik and amit’s on structuring the project\n\nQuestions/Reference\n\nhow to train a backward language model\nhow to combine forward and backward language model\nHow to incorporate blurr along with fastai\nText Interpretation https://muellerzr.github.io/fastinference/text.inference/\nCaptum for text interpretation\nDiscriminative Learning Rate https://harish3110.github.io/through-tinted-lenses/natural%20language%20processing/sentiment%20analysis/2020/06/27/Introduction-to-NLP-using-Fastai.html#Discriminative-Fine-tuning\n\nhttps://github.com/muellerzr/Practical-Deep-Learning-For-Coders/blob/master/05a_NLP.ipynb\nMixup for AWD_LSTM https://github.com/fmcurti/MixUp-for-AWD_LSTM/blob/master/ULMFiT.ipynb\n\n\n\n%%capture\n!pip install git+https://github.com/fastai/fastai2.git -q\n!pip install git+https://github.com/fastai/fastcore.git -q\n\n!pip install -q nbdev\n!pip install -q azure-cognitiveservices-search-imagesearch sentencepiece\n\n# Upload utils.py from fastai repository\n!wget https://raw.githubusercontent.com/fastai/fastbook/master/utils.py\n\n\n# make your Google Drive accessible\nfrom google.colab import drive\ndrive.mount('/content/gdrive', force_remount=True)\n\nroot_dir = \"/content/gdrive/My Drive\"\ndrive_data = f\"{root_dir}/Colab Notebooks/data/vaccination_tweet\"\n\nGo to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n\nEnter your authorization code:\n··········\nMounted at /content/gdrive\n\n\n\nfrom fastai2.text.all import *\n\n\nc = Config()\nc['data_path'] = f'{drive_data}'\nc['model_path'] = f'{drive_data}/models'\nc['bs'] = 128\nc['seq_len'] = 72\nc.d\n\n{'archive_path': '/root/.fastai/archive',\n 'bs': '128',\n 'data_path': '/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet',\n 'model_path': '/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/models',\n 'seq_len': '72',\n 'storage_path': '/root/.fastai/data',\n 'version': 2}\n\n\n\nvaccination = Path(c.d['data_path']);\ntrain_df = pd.read_csv(vaccination/'Train.csv')\ntest_df = pd.read_csv(vaccination/'Test.csv')\nlen(train_df), len(test_df)\n\n(10001, 5177)\n\n\n\ntrain_df.dtypes\n\ntweet_id      object\nsafe_text     object\nlabel        float64\nagreement    float64\ndtype: object\n\n\n\ntrain_df.dropna(axis='rows', inplace=True);len(train_df)\n\n9999\n\n\n\n#train_df.fillna(value=' ', inplace=True)\ntest_df.fillna(value=' ', inplace=True)"
  },
  {
    "objectID": "drafts/vaccination_tweets_sp.html#dataloaders",
    "href": "drafts/vaccination_tweets_sp.html#dataloaders",
    "title": "Starter Notebook for Text Classification using fastai",
    "section": "DataLoaders",
    "text": "DataLoaders\n\ndoc(SentencePieceTokenizer)\n\nclass SentencePieceTokenizer[source]SentencePieceTokenizer(lang='en', special_toks=None, sp_model=None, vocab_sz=None, max_vocab_sz=30000, model_type='unigram', char_coverage=None, cache_dir='tmp')\n\nSpacy tokenizer for lang\nShow in docs\n\n\n\ntweet_lm = DataBlock(\n            blocks=TextBlock.from_df(text_cols='safe_text', is_lm=True, tok_func=SentencePieceTokenizer, model_type='bpe', max_vocab_sz=10000),\n            get_x=ColReader('text'),\n            splitter=RandomSplitter(valid_pct=0.15, seed=42)\n            )\n\n\ndf = pd.concat([train_df, test_df])\n\n\ndls = tweet_lm.dataloaders(source=df, bs=int(c.d['bs']), seq_len=int(c.d['seq_len']))\n\n\n\n\n\n\n\n\nlen(dls.vocab)\n\n6216"
  },
  {
    "objectID": "drafts/vaccination_tweets_sp.html#language-learner",
    "href": "drafts/vaccination_tweets_sp.html#language-learner",
    "title": "Starter Notebook for Text Classification using fastai",
    "section": "Language Learner",
    "text": "Language Learner\n\ndoc(language_model_learner)\n\nlanguage_model_learner[source]language_model_learner(dls, arch, config=None, drop_mult=1.0, pretrained=True, pretrained_fnames=None, loss_func=None, opt_func='Adam', lr=0.001, splitter='trainable_params', cbs=None, metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95))\n\nCreate a Learner with a language model from dls and arch.\nShow in docs\n\n\n\nlearn_lm = language_model_learner(dls, arch=AWD_LSTM, drop_mult=0.3, pretrained=True, metrics=[accuracy, Perplexity()])\n\n\nlearn_lm.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.2754228591918945, lr_steep=0.013182567432522774)\n\n\n\n\n\n\nlm_lr = 1e-2\n\n\nlearn_lm.fine_tune(8, base_lr=lm_lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      7.731276\n      5.721750\n      0.173679\n      305.439056\n      00:29\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      5.676833\n      5.327257\n      0.207547\n      205.872421\n      00:31\n    \n    \n      1\n      5.365718\n      4.827284\n      0.255697\n      124.871391\n      00:31\n    \n    \n      2\n      5.025544\n      4.518177\n      0.286770\n      91.668335\n      00:31\n    \n    \n      3\n      4.728319\n      4.355146\n      0.300401\n      77.878227\n      00:31\n    \n    \n      4\n      4.496605\n      4.251612\n      0.311808\n      70.218529\n      00:31\n    \n    \n      5\n      4.332778\n      4.201892\n      0.317858\n      66.812614\n      00:31\n    \n    \n      6\n      4.213473\n      4.182868\n      0.320936\n      65.553589\n      00:31\n    \n    \n      7\n      4.144303\n      4.179251\n      0.321221\n      65.316925\n      00:31\n    \n  \n\n\n\n\nlearn_lm.save('8_1e-2_lm_fine_tuned')\n\n\nMODEL_PATH = c.d['model_path']\n\n\nlearn_lm.save_encoder('8_1e-2_lm_fine_tuned_enc')\n\n\n#learn_lm.save_encoder(f'{MODEL_PATH}/8_1e-2_lm_bpe_fine_tuned_enc')"
  },
  {
    "objectID": "drafts/vaccination_tweets_sp.html#regression",
    "href": "drafts/vaccination_tweets_sp.html#regression",
    "title": "Starter Notebook for Text Classification using fastai",
    "section": "Regression",
    "text": "Regression\n\ntweet_cls = DataBlock(blocks=[TextBlock.from_df(text_cols='safe_text', vocab=dls.vocab, tok_func=SentencePieceTokenizer, model_type='bpe', max_vocab_sz=10000),\n                              RegressionBlock],\n                      get_x=ColReader(cols='text'),\n                      get_y=ColReader(cols='label'),\n                      splitter=RandomSplitter(valid_pct=0.2, seed=42))\n\n\ntweet_cls_dls = tweet_cls.dataloaders(source=train_df,verbose=True,\n                                      bs=int(c.d['bs']),\n                                      seq_len=int(c.d['seq_len']))\n\n\n\n\n\n\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: partial\nSetting up after_batch: Pipeline: \n\n\n\ntweet_cls_dls.show_batch(max_n=3)\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      ▁xxbos ▁ xxunk xxunk xxunk xxunk xxunk xxunk の に mmr xxunk xxunk xxunk xxunk の xxunk xxunk xxunk xxunk xxunk た xxunk $ 550 の xxunk xxunk xxunk xxunk た 。 ▁xxrep ▁4 ▁( ▁; xxunk д xxunk ▁xxrep ▁7 ▁ ) ▁ xxunk xxunk xxunk xxunk xxunk xxunk xxunk に xxunk xxunk し xxunk xxunk xxunk xxunk xxunk た 。 xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk 。\n      0.0\n    \n    \n      1\n      ▁xxbos ▁... в к о л о л и ▁ т у т ▁ м н е ▁xxup ▁mmr - п р и в и в к у ▁... ▁( measles - mumps - r ub ella ▁vaccine ) ▁.... ▁ в ▁ xxunk у д у xxunk е е ▁ с м о т р ю ▁ с ▁ о п т и м и з м о м ▁...\n      0.0\n    \n    \n      2\n      ▁xxbos ▁xxmaj ▁the ▁xxup ▁mmr ▁mo ds p ace ▁xxup ▁m x - 5 ▁team ▁on ▁the ▁gr id ▁for ▁the ▁second ▁xxmaj ▁b atter y ▁xxmaj ▁tend er ▁race ▁at ▁xxmaj ▁se br ing ! ▁# ▁xxmaj ▁se br ing ▁# ▁mo ds p ace ▁# ▁ma z da ▁# ▁m x 5 c up ▁# ▁global m x 5 c up ▁# ▁sc ca\n      0.0\n    \n  \n\n\n\n\ndoc(text_classifier_learner)\n\ntext_classifier_learner[source]text_classifier_learner(dls, arch, seq_len=72, config=None, pretrained=True, drop_mult=0.5, n_out=None, lin_ftrs=None, ps=None, max_len=1440, y_range=None, loss_func=None, opt_func='Adam', lr=0.001, splitter='trainable_params', cbs=None, metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95))\n\nCreate a Learner with a text classifier from dls and arch.\nShow in docs\n\n\n\nlearn_cls = text_classifier_learner(dls=tweet_cls_dls,\n                                    arch=AWD_LSTM,\n                                    loss_func=mse,                                    \n                                    metrics=rmse,\n                                    y_range=[-1, 1])\n\n\nlearn_cls.load_encoder(f'8_1e-2_lm_fine_tuned_enc')\n\n<fastai2.text.learner.TextLearner at 0x7fce8db1a2b0>\n\n\n\nlearn_cls.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.012022644281387329, lr_steep=5.754399353463668e-06)\n\n\n\n\n\n\nlearn_cls.fine_tune(epochs=8, base_lr=1e-2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      _rmse\n      time\n    \n  \n  \n    \n      0\n      0.655240\n      0.384711\n      0.620251\n      00:16\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      _rmse\n      time\n    \n  \n  \n    \n      0\n      0.479516\n      0.387409\n      0.622422\n      00:17\n    \n    \n      1\n      0.405052\n      0.344687\n      0.587101\n      00:17\n    \n    \n      2\n      0.378217\n      0.357070\n      0.597553\n      00:17\n    \n    \n      3\n      0.359391\n      0.340642\n      0.583646\n      00:17\n    \n    \n      4\n      0.347887\n      0.340789\n      0.583771\n      00:17\n    \n    \n      5\n      0.335765\n      0.327030\n      0.571865\n      00:17\n    \n    \n      6\n      0.322824\n      0.326958\n      0.571802\n      00:17\n    \n    \n      7\n      0.312481\n      0.327277\n      0.572081\n      00:17\n    \n  \n\n\n\n\nlearn_cls.recorder.plot_loss()"
  },
  {
    "objectID": "drafts/vaccination_tweets_sp.html#inference---not-working",
    "href": "drafts/vaccination_tweets_sp.html#inference---not-working",
    "title": "Starter Notebook for Text Classification using fastai",
    "section": "Inference - NOT WORKING",
    "text": "Inference - NOT WORKING\n\ntest_df.rename(columns={'safe_text': 'text'}, inplace=True)\n\n\nlen(test_df)\n\n5177\n\n\n\ntest_df\n\n\n\n\n\n  \n    \n      \n      tweet_id\n      text\n    \n  \n  \n    \n      0\n      00BHHHP1\n      <user> <user> ... &amp; 4 a vaccine given 2 healthy peeps, FDA think just not worth the AE risk unfortunately.\n    \n    \n      1\n      00UNMD0E\n      Students starting school without whooping cough vaccinations <url> #scpick\n    \n    \n      2\n      01AXPTJF\n      I'm kinda over every ep of <user> being \"ripped from the headlines.\" Measles? Let's get back to crime. #SVU\n    \n    \n      3\n      01HOEQJW\n      How many innocent children die for lack of vaccination each year? Around 1.5 million. Too bad all their parents couldn't be here. #SB277\n    \n    \n      4\n      01JUKMAO\n      CDC eyeing bird flu vaccine for humans, though risk is low: Federal officials said Wednesday they're taking steps… <url>\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      5172\n      ZXVVNC5O\n      jenny mccarthy is on new years rockin eve. what has she done lately besides not vaccinate her kids and give us all goddamn polio??\n    \n    \n      5173\n      ZYIANVI8\n      Measles reported in Clark Co. for 1st time since 2011 <url>\n    \n    \n      5174\n      ZYITEHAH\n      <user> issues alert regarding Measles in TX. Keep your DDx up to date, people! #Emergencymedicine\n    \n    \n      5175\n      ZZ3BMBTG\n      I can't believe people don't vaccinate their kids! I've been vaccinated for everything and then some.\n    \n    \n      5176\n      ZZIYCVNH\n      \"<user>  Alternatives to #Flu Vaccine <url> #natural #health\" A good read with a few new tips &amp; many we #jerf folk know\n    \n  \n\n5177 rows × 2 columns\n\n\n\n\nsent_tfm = Tokenizer.from_df(text_cols='text', \n                             tok_func=SentencePieceTokenizer,\n                             model_type='bpe',\n                             max_vocab_sz=10000)\n\n\ntfms = [attrgetter('text'), sent_tfm, Numericalize()]\n\n\ndoc(Datasets)\n\nclass Datasets[source]Datasets(items=None, tfms=None, tls=None, n_inp=None, dl_type=None, use_list=None, do_setup=True, split_idx=None, train_setup=True, splits=None, types=None, verbose=False) :: FilteredBase\n\nA dataset that creates a tuple from each tfms, passed thru item_tfms\nShow in docs\n\n\n\ntest_dset = Datasets(items=test_df, tfms=[tfms], splits=None, dl_type=)\n\n\n\n\n\n\n\n\ntest_dl = test_dset.dataloaders(bs=int(c.d['bs']), seq_len=int(c.d['seq_len']))\n\n\nlearn_cls.dls.test_dl = test_dl\n\n\npredictions, _, targets = learn_cls.get_preds(dl=test_dl, with_decoded=True)\n\n\n\n\nIndexError: ignored\n\n\n\ntargets\n\ntensor([[-0.6514],\n        [ 0.5469],\n        [ 0.2638],\n        ...,\n        [ 0.2053],\n        [ 0.9290],\n        [ 0.6997]])\n\n\n\nvaccination.ls()\n\n(#4) [Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/SampleSubmission.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/Train.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/Test.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/models')]\n\n\n\nsubmission = pd.read_csv(vaccination/'SampleSubmission.csv')\n\n\nsubmission.head()\n\n\n\n\n\n  \n    \n      \n      tweet_id\n      label\n    \n  \n  \n    \n      0\n      00BHHHP1\n      0\n    \n    \n      1\n      00UNMD0E\n      0\n    \n    \n      2\n      01AXPTJF\n      0\n    \n    \n      3\n      01HOEQJW\n      0\n    \n    \n      4\n      01JUKMAO\n      0\n    \n  \n\n\n\n\n\nlen(submission), len(predictions)\n\n(5177, 5177)\n\n\n\nsubmission['label'] = predictions.flatten()\n\n\nsubmission\n\n\n\n\n\n  \n    \n      \n      tweet_id\n      label\n    \n  \n  \n    \n      0\n      00BHHHP1\n      -0.651409\n    \n    \n      1\n      00UNMD0E\n      0.546905\n    \n    \n      2\n      01AXPTJF\n      0.263799\n    \n    \n      3\n      01HOEQJW\n      0.988286\n    \n    \n      4\n      01JUKMAO\n      0.055132\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      5172\n      ZXVVNC5O\n      0.890525\n    \n    \n      5173\n      ZYIANVI8\n      0.190089\n    \n    \n      5174\n      ZYITEHAH\n      0.205295\n    \n    \n      5175\n      ZZ3BMBTG\n      0.928989\n    \n    \n      5176\n      ZZIYCVNH\n      0.699665\n    \n  \n\n5177 rows × 2 columns\n\n\n\n\nsubmission.to_csv('submission.csv', index=False)"
  },
  {
    "objectID": "drafts/zindi_vaccination.html",
    "href": "drafts/zindi_vaccination.html",
    "title": "Outline",
    "section": "",
    "text": "Problem Understanding * Text Regression problem: The task is given the tweet text, we are expected to predict whether the given tweet is positive, negetive or neutral. The target variable value ranges from (-1, 1)"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#setup",
    "href": "drafts/zindi_vaccination.html#setup",
    "title": "Outline",
    "section": "Setup",
    "text": "Setup\n\n! pip install fastai2 -q\n! pip install nbdev -q\n\n     |████████████████████████████████| 194kB 2.8MB/s \n     |████████████████████████████████| 51kB 1.7MB/s \n\n\n\nimport fastai2\nprint(fastai2.__version__)\n\n0.0.17"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#download-the-dataset",
    "href": "drafts/zindi_vaccination.html#download-the-dataset",
    "title": "Outline",
    "section": "Download the Dataset",
    "text": "Download the Dataset\n\n! wget \"https://fastaistudygroup.slack.com/files/URA50KQ7R/F0140NHLB8Q/zindi_vaccinate.zip\"\n\n--2020-05-17 16:48:37--  https://fastaistudygroup.slack.com/files/URA50KQ7R/F0140NHLB8Q/zindi_vaccinate.zip\nResolving fastaistudygroup.slack.com (fastaistudygroup.slack.com)... 3.229.174.54, 54.209.135.9, 54.227.211.227\nConnecting to fastaistudygroup.slack.com (fastaistudygroup.slack.com)|3.229.174.54|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://fastaistudygroup.slack.com/?redir=%2Ffiles%2FURA50KQ7R%2FF0140NHLB8Q%2Fzindi_vaccinate.zip [following]\n--2020-05-17 16:48:37--  https://fastaistudygroup.slack.com/?redir=%2Ffiles%2FURA50KQ7R%2FF0140NHLB8Q%2Fzindi_vaccinate.zip\nReusing existing connection to fastaistudygroup.slack.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘zindi_vaccinate.zip’\n\nzindi_vaccinate.zip     [ <=>                ]  39.75K  --.-KB/s    in 0.01s   \n\n2020-05-17 16:48:37 (3.13 MB/s) - ‘zindi_vaccinate.zip’ saved [40702]\n\n\n\n\n!pwd\n\n/content\n\n\n\ndata_dir = '/content/drive/My Drive/Colab Notebooks/data'\n\n\nfrom pathlib import Path\n\n\npath = Path(data_dir)\n\n\n# make your Google Drive accessible\nfrom google.colab import drive\ndrive.mount('/content/gdrive', force_remount=True)\n\nroot_dir = \"/content/gdrive/My Drive\"\ndrive_data = f\"{root_dir}/Colab Notebooks/data/vaccination_tweet\"\n\nGo to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n\nEnter your authorization code:\n··········\nMounted at /content/gdrive\n\n\n\nfrom fastai2.text.all import *\n\n\nPath(drive_data).ls()\n\n(#4) [Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/SampleSubmission.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/Train.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/Test.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/models')]\n\n\n\nvaccinate_tweets = Path(drive_data)\n\n\nPath.BASE_PATH = vaccinate_tweets\n\n\nPath(vaccinate_tweets).ls()\n\n(#4) [Path('SampleSubmission.csv'),Path('Train.csv'),Path('Test.csv'),Path('models')]\n\n\n\ntrain_df = pd.read_csv(vaccinate_tweets/'Train.csv')\n\n\ntrain_df.head()\n\n\n\n\n\n  \n    \n      \n      tweet_id\n      safe_text\n      label\n      agreement\n    \n  \n  \n    \n      0\n      CL1KWCMY\n      Me &amp; The Big Homie meanboy3000 #MEANBOY #MB #MBS #MMR #STEGMANLIFE @ Stegman St. <url>\n      0.0\n      1.0\n    \n    \n      1\n      E3303EME\n      I'm 100% thinking of devoting my career to proving autism isn't caused by vaccines due to the IDIOTIC posts I've seen about World Autism Day\n      1.0\n      1.0\n    \n    \n      2\n      M4IVFSMS\n      #whatcausesautism VACCINES, DO NOT VACCINATE YOUR CHILD\n      -1.0\n      1.0\n    \n    \n      3\n      1DR6ROZ4\n      I mean if they immunize my kid with something that won't secretly kill him years down the line then I'm all for it, but I don't trust that\n      -1.0\n      1.0\n    \n    \n      4\n      J77ENIIE\n      Thanks to <user> Catch me performing at La Nuit NYC 1134 1st ave. Show starts at 6! #jennifair #mmr… <url>\n      0.0\n      1.0\n    \n  \n\n\n\n\n\nlen(train_df)\n\n10001\n\n\n\ntrain_df.dtypes\n\ntweet_id      object\nsafe_text     object\nlabel        float64\nagreement    float64\ndtype: object\n\n\n\ntest_df = pd.read_csv(vaccinate_tweets/'Test.csv')\n\n\nlen(test_df)\n\n5177\n\n\n\ntest_df.head()\n\n\n\n\n\n  \n    \n      \n      tweet_id\n      safe_text\n    \n  \n  \n    \n      0\n      00BHHHP1\n      <user> <user> ... &amp; 4 a vaccine given 2 healthy peeps, FDA think just not worth the AE risk unfortunately.\n    \n    \n      1\n      00UNMD0E\n      Students starting school without whooping cough vaccinations <url> #scpick\n    \n    \n      2\n      01AXPTJF\n      I'm kinda over every ep of <user> being \"ripped from the headlines.\" Measles? Let's get back to crime. #SVU\n    \n    \n      3\n      01HOEQJW\n      How many innocent children die for lack of vaccination each year? Around 1.5 million. Too bad all their parents couldn't be here. #SB277\n    \n    \n      4\n      01JUKMAO\n      CDC eyeing bird flu vaccine for humans, though risk is low: Federal officials said Wednesday they're taking steps… <url>"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#cleaning-the-data",
    "href": "drafts/zindi_vaccination.html#cleaning-the-data",
    "title": "Outline",
    "section": "Cleaning the data",
    "text": "Cleaning the data\n\n# Find the missing values\ntrain_df.isna().sum()\n\ntweet_id     0\nsafe_text    0\nlabel        1\nagreement    2\ndtype: int64\n\n\n\ntest_df.isna().sum()\n\ntweet_id     0\nsafe_text    1\ndtype: int64\n\n\nlabel has 1 missing value and agreement has 2 missing values in train_df and safe_text has 1 missing value in safe_text"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#filter-outfill-the-missing-values-in-train_df-and-test_df",
    "href": "drafts/zindi_vaccination.html#filter-outfill-the-missing-values-in-train_df-and-test_df",
    "title": "Outline",
    "section": "Filter out/Fill the missing values in train_df and test_df",
    "text": "Filter out/Fill the missing values in train_df and test_df\n\n# In case of train dataframe, it's okay to drop the missing value\ntrain_df.dropna(axis=0, inplace=True) # axis=0 \n\n\nlen(train_df)\n\n9999\n\n\nWe have removed the two missing rows due to missing values in label & agreement\nSince we have to submit our predictions on our Test.csv, we cannot remove the missing value. So we are going to fill the missing value with space\n\ntest_df.fillna(value=\" \", axis=0, inplace=True)\n\n\nlen(test_df)\n\n5177"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#create-dataloader-using-datablock-api",
    "href": "drafts/zindi_vaccination.html#create-dataloader-using-datablock-api",
    "title": "Outline",
    "section": "Create DataLoader using DataBlock API",
    "text": "Create DataLoader using DataBlock API\n\ndoc(DataBlock)\n\nclass DataBlock[source]DataBlock(blocks=None, dl_type=None, getters=None, n_inp=None, item_tfms=None, batch_tfms=None, get_items=None, splitter=None, get_y=None, get_x=None)\n\nGeneric container to quickly build Datasets and DataLoaders\nShow in docs\n\n\nimdb = DataBlock(blocks=(TextBlock.from_folder(path), CategoryBlock), get_items=get_text_files, get_y=parent_label, splitter=GrandparentSplitter(valid_name=‘test’))\n\ndoc(TextBlock.from_df)\n\nTextBlock.from_df[source]TextBlock.from_df(text_cols, vocab=None, is_lm=False, seq_len=72, min_freq=3, max_vocab=60000, tok_func='SpacyTokenizer', rules=None, sep=' ', n_workers=2, mark_fields=None, res_col_name='text', **kwargs)\n\nBuild a TextBlock from a dataframe using text_cols\nShow in docs\n\n\n\ndoc(RegressionBlock)\n\nRegressionBlock[source]RegressionBlock(n_out=None)\n\nTransformBlock for float targets\nShow in docs\n\n\n\ndoc(SentencePieceTokenizer)\n\nclass SentencePieceTokenizer[source]SentencePieceTokenizer(lang='en', special_toks=None, sp_model=None, vocab_sz=None, max_vocab_sz=30000, model_type='unigram', char_coverage=None, cache_dir='tmp')\n\nSpacy tokenizer for lang\nShow in docs\n\n\n\ntxt_blk_lm = TextBlock.from_df(text_cols='safe_text', is_lm=True, tok_func=SpacyTokenizer, res_col_name='text' ) #Next : SentencePiece https://forums.fast.ai/t/fastai-v2-text/53529/293\n\n\ndoc(ColReader)\n\nclass ColReader[source]ColReader(cols, pref='', suff='', label_delim=None)\n\nRead cols in row with potential pref and suff\nShow in docs\n\n\nThe task of Languagemodel is to predict the words based on the given text\n\nvaccinate_lm = DataBlock(blocks=(txt_blk_lm), # There is no dependent variable \n                      get_x=ColReader(cols='text'), # Independent variable\n                      splitter=RandomSplitter(valid_pct=0.15,seed=42)\n                      )\n\n\n#vaccinate_lm.summary(source=train_df)\n\n\ndoc(DataBlock.dataloaders)\n\nDataBlock.dataloaders[source]DataBlock.dataloaders(source, path='.', verbose=False, lens=None, cache=2, bs=64, seq_len=72, num_workers=0, shuffle=False, do_setup=True, pin_memory=False, timeout=0, batch_size=None, drop_last=False, indexed=None, n=None, device=None, wif=None, before_iter=None, after_item=None, before_batch=None, after_batch=None, after_iter=None, create_batches=None, create_item=None, create_batch=None, retain=None, get_idxs=None, sample=None, shuffle_fn=None, do_batch=None)\n\nCreate a DataLoaders object from source\nShow in docs\n\n\n\nvaccinate_lm\n\n<fastai2.data.block.DataBlock at 0x7fee3255b6a0>\n\n\n\ndls_lm = vaccinate_lm.dataloaders(source=train_df, bs=64, seq_len=72, verbose=True) \n\n\n\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: \n\n\n\ndls_lm.one_batch()\n\n(LMTensorText([[   2,    8,  375,  ...,   62,   15,    0],\n         [  48,   36,    2,  ...,   15,  818,  625],\n         [ 488,   22,    8,  ...,  113,   42, 2207],\n         ...,\n         [  79,   11,   88,  ...,   86,   18,  110],\n         [   9,    8,   49,  ..., 1314,   93,   19],\n         [  13,    9,    8,  ..., 4422,  197,   22]], device='cuda:0'),\n tensor([[   8,  375,   32,  ...,   15,    0,    0],\n         [  36,    2,    8,  ...,  818,  625,    7],\n         [  22,    8,  104,  ...,   42, 2207,   58],\n         ...,\n         [  11,   88,   10,  ...,   18,  110,    0],\n         [   8,   49,  200,  ...,   93,   19,  594],\n         [   9,    8,  108,  ...,  197,   22,   10]], device='cuda:0'))\n\n\n\nx, y = dls_lm.one_batch()\n\n\nx.shape, y.shape\n\n(torch.Size([64, 72]), torch.Size([64, 72]))\n\n\n\ndls_lm.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos xxmaj eradicating polio . xxmaj using xxup gis to fight this disease . # esriuc vaccinations and improved xxunk conditions . < url > xxbos xxmaj third xxup mmr xxmaj dose well - tolerated xxmaj during xxmaj mumps xxmaj outbreak xxunk < url > # xxunk # newyork # xxup ny xxbos “ < user > xxmaj how about parents xxmaj guide to xxmaj dealing xxmaj with xxmaj people xxmaj who\n      xxmaj eradicating polio . xxmaj using xxup gis to fight this disease . # esriuc vaccinations and improved xxunk conditions . < url > xxbos xxmaj third xxup mmr xxmaj dose well - tolerated xxmaj during xxmaj mumps xxmaj outbreak xxunk < url > # xxunk # newyork # xxup ny xxbos “ < user > xxmaj how about parents xxmaj guide to xxmaj dealing xxmaj with xxmaj people xxmaj who have\n    \n    \n      1\n      > < user > < user > parents would no longer be able to xxunk on the herd immunity of protected children . xxbos xxmaj all i wanted was an opportunity , a bad nigga , & & some immunity ; to keep a xxunk ' safe from the snakes .. xxbos < user > xxmaj news you might 've missed , from < user > xxmaj no . 5 : 400\n      < user > < user > parents would no longer be able to xxunk on the herd immunity of protected children . xxbos xxmaj all i wanted was an opportunity , a bad nigga , & & some immunity ; to keep a xxunk ' safe from the snakes .. xxbos < user > xxmaj news you might 've missed , from < user > xxmaj no . 5 : 400 million\n    \n    \n      2\n      url > xxbos “ < user > xxmaj the ethical xxunk of parents who refuse to vaccinate their children < url > # vaccineswork # vaccinations xxbos xxmaj how well - vaccinated is your child ’s kindergarten ? : xxmaj the xxmaj california xxmaj health and xxmaj safety xxmaj code requires elementary school … < url > xxbos < user > \" i never got my kids vaccinated and they are fine\n      > xxbos “ < user > xxmaj the ethical xxunk of parents who refuse to vaccinate their children < url > # vaccineswork # vaccinations xxbos xxmaj how well - vaccinated is your child ’s kindergarten ? : xxmaj the xxmaj california xxmaj health and xxmaj safety xxmaj code requires elementary school … < url > xxbos < user > \" i never got my kids vaccinated and they are fine \"\n    \n    \n      3\n      xxmaj vaccination xxmaj records for xxmaj kids < url > xxbos < user > my mmr shows that because xxmaj i 've only xxunk with my xxup xxunk friend for like two weeks . xxmaj and alright xxmaj i 'll believe it when i see it lmao xxbos xxmaj german xxmaj xxunk xxmaj who xxmaj denied xxmaj measles xxmaj exists xxmaj xxunk xxmaj to xxmaj pay xxmaj more xxmaj than $ 100\n      vaccination xxmaj records for xxmaj kids < url > xxbos < user > my mmr shows that because xxmaj i 've only xxunk with my xxup xxunk friend for like two weeks . xxmaj and alright xxmaj i 'll believe it when i see it lmao xxbos xxmaj german xxmaj xxunk xxmaj who xxmaj denied xxmaj measles xxmaj exists xxmaj xxunk xxmaj to xxmaj pay xxmaj more xxmaj than $ 100 ,\n    \n    \n      4\n      worried about , it 's not worse than the measles . xxmaj shut up . \\n xxmaj xxunk , \\n xxmaj everyone xxbos xxmaj how can we convince parents to vaccinate ? xxmaj acknowledge their fears . < url > via < user > # autism # antivaxxer xxbos xxmaj is this year 's flu vaccine effective ? xxmaj the experts xxunk in : xxmaj the xxmaj centers for xxmaj disease xxmaj\n      about , it 's not worse than the measles . xxmaj shut up . \\n xxmaj xxunk , \\n xxmaj everyone xxbos xxmaj how can we convince parents to vaccinate ? xxmaj acknowledge their fears . < url > via < user > # autism # antivaxxer xxbos xxmaj is this year 's flu vaccine effective ? xxmaj the experts xxunk in : xxmaj the xxmaj centers for xxmaj disease xxmaj control\n    \n    \n      5\n      < user > forms it for sure asked about vaccines . i went to private school but i seem to remember it was required . xxmaj maybe public is xxunk ? xxbos xxmaj neurosurgeon xxmaj exposes xxmaj vaccines , xxup cdc , xxup fda and xxmaj xxunk … : < url > xxbos xxmaj xxunk about running up to xxmaj jenny mccarthy and xxunk \" vaccinate xxmaj this ! \" - xxmaj\n      user > forms it for sure asked about vaccines . i went to private school but i seem to remember it was required . xxmaj maybe public is xxunk ? xxbos xxmaj neurosurgeon xxmaj exposes xxmaj vaccines , xxup cdc , xxup fda and xxmaj xxunk … : < url > xxbos xxmaj xxunk about running up to xxmaj jenny mccarthy and xxunk \" vaccinate xxmaj this ! \" - xxmaj than\n    \n    \n      6\n      school . xxmaj you must send your kids to some type of school . xxmaj you need vaccines to work most places . xxmaj force xxbos xxmaj state vaccination exemptions up xxunk over 30 years : xxmaj religious and medical exemptions claimed by xxmaj bay xxmaj state … < url > xxbos xxmaj vaccines … xxunk xxmaj childrens xxmaj clinic ) < url > xxbos y' all better be xxunk your measles\n      . xxmaj you must send your kids to some type of school . xxmaj you need vaccines to work most places . xxmaj force xxbos xxmaj state vaccination exemptions up xxunk over 30 years : xxmaj religious and medical exemptions claimed by xxmaj bay xxmaj state … < url > xxbos xxmaj vaccines … xxunk xxmaj childrens xxmaj clinic ) < url > xxbos y' all better be xxunk your measles up\n    \n    \n      7\n      immigrants bringing diseases into the xxup us ? a xxmaj child from xxmaj mexico got measles at xxmaj disney from an # xxunk xxbos < user > i choose not to vaccinate my kids and my xxunk & & xxunk make me feel good about it . xxbos i xxunk this : xxmaj why xxmaj did xxmaj vaccinated xxmaj people xxmaj get xxmaj measles at xxmaj disneyland ? | xxup xxunk <\n      bringing diseases into the xxup us ? a xxmaj child from xxmaj mexico got measles at xxmaj disney from an # xxunk xxbos < user > i choose not to vaccinate my kids and my xxunk & & xxunk make me feel good about it . xxbos i xxunk this : xxmaj why xxmaj did xxmaj vaccinated xxmaj people xxmaj get xxmaj measles at xxmaj disneyland ? | xxup xxunk < url\n    \n    \n      8\n      xxmaj measles outbreak in xxmaj xxunk xxmaj county spreads to four < url > via < url > xxbos xxmaj good . xxmaj exempt from vaccines , exempt from school . xxmaj period . \" california xxmaj moves to xxmaj ban xxmaj all xxmaj vaccination xxmaj exemptions \" < url > xxbos xxmaj lol i ca n't with people who do n't vaccinate their children . xxmaj god is n't going to\n      measles outbreak in xxmaj xxunk xxmaj county spreads to four < url > via < url > xxbos xxmaj good . xxmaj exempt from vaccines , exempt from school . xxmaj period . \" california xxmaj moves to xxmaj ban xxmaj all xxmaj vaccination xxmaj exemptions \" < url > xxbos xxmaj lol i ca n't with people who do n't vaccinate their children . xxmaj god is n't going to save\n    \n  \n\n\n\n\n# Vocab\n\n\nlen(dls_lm.train.vocab), dls_lm.train.vocab[:30]\n\n(4472,\n ['xxunk',\n  'xxpad',\n  'xxbos',\n  'xxeos',\n  'xxfld',\n  'xxrep',\n  'xxwrep',\n  'xxup',\n  'xxmaj',\n  '>',\n  '<',\n  '.',\n  '#',\n  'user',\n  'url',\n  'the',\n  'to',\n  ',',\n  'measles',\n  'a',\n  'of',\n  'i',\n  ':',\n  'in',\n  'and',\n  '…',\n  '!',\n  'is',\n  'vaccine',\n  'for'])"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#language-model",
    "href": "drafts/zindi_vaccination.html#language-model",
    "title": "Outline",
    "section": "Language Model",
    "text": "Language Model\n\ndoc(language_model_learner)\n\nlanguage_model_learner[source]language_model_learner(dls, arch, config=None, drop_mult=1.0, pretrained=True, pretrained_fnames=None, loss_func=None, opt_func='Adam', lr=0.001, splitter='trainable_params', cbs=None, metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95))\n\nCreate a Learner with a language model from dls and arch.\nShow in docs\n\n\n\nlearn_lm = language_model_learner(dls=dls_lm, arch=AWD_LSTM, pretrained=True, metrics=[accuracy, Perplexity()]) # Next : [accuracy, Perplexity()], AWD_QRNN\n\n\n\n\n\nlearn_lm.model\n\nSequentialRNN(\n  (0): AWD_LSTM(\n    (encoder): Embedding(4472, 400, padding_idx=1)\n    (encoder_dp): EmbeddingDropout(\n      (emb): Embedding(4472, 400, padding_idx=1)\n    )\n    (rnns): ModuleList(\n      (0): WeightDropout(\n        (module): LSTM(400, 1152, batch_first=True)\n      )\n      (1): WeightDropout(\n        (module): LSTM(1152, 1152, batch_first=True)\n      )\n      (2): WeightDropout(\n        (module): LSTM(1152, 400, batch_first=True)\n      )\n    )\n    (input_dp): RNNDropout()\n    (hidden_dps): ModuleList(\n      (0): RNNDropout()\n      (1): RNNDropout()\n      (2): RNNDropout()\n    )\n  )\n  (1): LinearDecoder(\n    (decoder): Linear(in_features=400, out_features=4472, bias=True)\n    (output_dp): RNNDropout()\n  )\n)\n\n\n\nlearn_lm.summary()\n\nSequentialRNN (Input shape: ['64 x 72'])\n================================================================\nLayer (type)         Output Shape         Param #    Trainable \n================================================================\nRNNDropout           64 x 72 x 400        0          False     \n________________________________________________________________\nRNNDropout           64 x 72 x 1152       0          False     \n________________________________________________________________\nRNNDropout           64 x 72 x 1152       0          False     \n________________________________________________________________\nLinear               64 x 72 x 4472       1,793,272  True      \n________________________________________________________________\nRNNDropout           64 x 72 x 400        0          False     \n________________________________________________________________\n\nTotal params: 1,793,272\nTotal trainable params: 1,793,272\nTotal non-trainable params: 0\n\nOptimizer used: <function Adam at 0x7fee89ac9d90>\nLoss function: FlattenedLoss of CrossEntropyLoss()\n\nModel frozen up to parameter group number 3\n\nCallbacks:\n  - TrainEvalCallback\n  - Recorder\n  - ProgressCallback\n  - ModelReseter\n  - RNNRegularizer\n\n\nFind the learning rate\n\ndoc(Learner.lr_find)\n\nLearner.lr_find[source]Learner.lr_find(start_lr=1e-07, end_lr=10, num_it=100, stop_div=True, show_plot=True, suggestions=True)\n\nLaunch a mock training to find a good learning rate, return lr_min, lr_steep if suggestions is True\nShow in docs\n\n\n\nlearn_lm.lr_find(suggestions=True)\n\n\n\n\nSuggestedLRs(lr_min=0.06309573650360108, lr_steep=0.0691830962896347)\n\n\n\n\n\n\ndoc(Learner.fine_tune)\n\nLearner.fine_tune[source]Learner.fine_tune(epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False)\n\nFine tune with freeze for freeze_epochs then with unfreeze from epochs using discriminative LR\nShow in docs\n\n\n\nlr=1e-2\n\n\nlearn_lm.fine_tune(epochs=3, base_lr=lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      5.276912\n      3.852003\n      0.315016\n      47.087299\n      00:14\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      4.046282\n      3.538154\n      0.352984\n      34.403339\n      00:15\n    \n    \n      1\n      3.818501\n      3.393587\n      0.369528\n      29.772551\n      00:15\n    \n    \n      2\n      3.671724\n      3.367341\n      0.373283\n      29.001310\n      00:15\n    \n  \n\n\n\n\nlearn_lm.save_encoder(file='fine_tuned_enc')"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#datablock-for-textregression",
    "href": "drafts/zindi_vaccination.html#datablock-for-textregression",
    "title": "Outline",
    "section": "DataBlock for TextRegression",
    "text": "DataBlock for TextRegression\n\ndoc(TextBlock.from_df)\n\nTextBlock.from_df[source]TextBlock.from_df(text_cols, vocab=None, is_lm=False, seq_len=72, min_freq=3, max_vocab=60000, tok_func='SpacyTokenizer', rules=None, sep=' ', n_workers=2, mark_fields=None, res_col_name='text', **kwargs)\n\nBuild a TextBlock from a dataframe using text_cols\nShow in docs\n\n\n\ndoc(SpacyTokenizer)\n\nclass SpacyTokenizer[source]SpacyTokenizer(lang='en', special_toks=None, buf_sz=5000)\n\nSpacy tokenizer for lang\nShow in docs\n\n\n\ntxt_blk_reg = TextBlock.from_df(text_cols='safe_text', vocab=dls_lm.vocab, is_lm=False, tok_func=SpacyTokenizer)\n\n\nvaccinate_reg = DataBlock(blocks=(txt_blk_reg, RegressionBlock),\n                          get_x=ColReader(cols='text'),\n                          get_y=ColReader(cols='label'),\n                          splitter=RandomSplitter(valid_pct=0.2, seed=42))\n\n\nvaccinate_reg.summary(source=train_df)\n\nSetting-up type transforms pipelines\nCollecting items from        tweet_id  ... agreement\n0      CL1KWCMY  ...  1.000000\n1      E3303EME  ...  1.000000\n2      M4IVFSMS  ...  1.000000\n3      1DR6ROZ4  ...  1.000000\n4      J77ENIIE  ...  1.000000\n...         ...  ...       ...\n9996   IU0TIJDI  ...  1.000000\n9997   WKKPCJY6  ...  0.666667\n9998   ST3A265H  ...  1.000000\n9999   6Z27IJGD  ...  1.000000\n10000  P6190L3Q  ...  0.666667\n\n[9999 rows x 4 columns]\nFound 9999 items\n2 datasets of sizes 8000,1999\nSetting up Pipeline: ColReader -> Tokenizer -> Numericalize\n\n\n\n\n\nSetting up Pipeline: ColReader -> RegressionSetup\n\nBuilding one sample\n  Pipeline: ColReader -> Tokenizer -> Numericalize\n    starting from\n      tweet_id                                                                                                                                                                                                                    HF5RFML5\nlabel                                                                                                                                                                                                                              0\nagreement                                                                                                                                                                                                                   0.666667\ntext           [xxbos, i, find, it, hard, to, believe, that, no, one, saw, a, problem, with, \", diplomatic, immunity, ., \", xxmaj, it, 's, like, a, pass, to, go, on, a, killing, spree, ,, says, xxmaj, law, &, &, xxmaj, order, .]\ntext_length                                                                                                                                                                                                                       40\nName: 3327, dtype: object\n    applying ColReader gives\n      (#40) ['xxbos','i','find','it','hard','to','believe','that','no','one'...]\n    applying Tokenizer gives\n      (#40) ['xxbos','i','find','it','hard','to','believe','that','no','one'...]\n    applying Numericalize gives\n      TensorText of size 40\n  Pipeline: ColReader -> RegressionSetup\n    starting from\n      tweet_id                                                                                                                                                                                                                    HF5RFML5\nlabel                                                                                                                                                                                                                              0\nagreement                                                                                                                                                                                                                   0.666667\ntext           [xxbos, i, find, it, hard, to, believe, that, no, one, saw, a, problem, with, \", diplomatic, immunity, ., \", xxmaj, it, 's, like, a, pass, to, go, on, a, killing, spree, ,, says, xxmaj, law, &, &, xxmaj, order, .]\ntext_length                                                                                                                                                                                                                       40\nName: 3327, dtype: object\n    applying ColReader gives\n      0.0\n    applying RegressionSetup gives\n      tensor(0.)\n\nFinal sample: (TensorText([   2,   21,  455,   40,  574,   16,  228,   41,   68,  123,  792,   19,\n         622,   56,   31, 1667,   65,   11,   31,    8,   40,   38,   93,   19,\n         790,   16,  155,   49,   19,  340,    0,   17,  199,    8,  353,   39,\n          39,    8, 1697,   11]), tensor(0.))\n\n\n\n\n\n\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: partial\nSetting up after_batch: Pipeline: \n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ToTensor\n    starting from\n      (TensorText of size 40, tensor(0.))\n    applying ToTensor gives\n      (TensorText of size 40, tensor(0.))\n\nAdding the next 3 samples\n\nApplying before_batch to the list of samples\n  Pipeline: partial\n    starting from\n      [(TensorText of size 40, tensor(0.)), (TensorText of size 28, tensor(0.)), (TensorText of size 27, tensor(1.)), (TensorText of size 33, tensor(1.))]\n    applying partial gives\n      [(TensorText of size 40, tensor(0.)), (TensorText of size 40, tensor(0.)), (TensorText of size 40, tensor(1.)), (TensorText of size 40, tensor(1.))]\n\nCollating items in a batch\n\nNo batch_tfms to apply\n\n\n\ndls_reg = vaccinate_reg.dataloaders(source=train_df, verbose=True, bs=64, seq_len=72)\n\n\n\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: partial\nSetting up after_batch: Pipeline: \n\n\n\nx, y = dls_reg.one_batch()\n\n\nx.shape, y.shape # One would expect seq_len as 72 but we get 53 Why???\n\n(torch.Size([64, 53]), torch.Size([64]))\n\n\n\ndls_reg.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos < user > xxmaj how xxmaj could xxmaj you xxmaj put xxmaj ur xxmaj child xxmaj in xxmaj so xxmaj much xxmaj danger ? xxmaj most xxmaj of xxmaj the xxmaj diseases xxmaj that xxmaj they xxmaj have xxmaj vaccines 4 , i xxmaj had xxmaj xxunk . xxmaj vaccinate . xxunk\n      1.0\n    \n    \n      1\n      xxbos am - news : xxmaj week xxmaj ahead : xxup xxunk in the xxup er , xxup hiv xxmaj news , xxmaj docs on xxmaj measles xxmaj vaccine : xxmaj week xxmaj ahead : xxup xxunk in the xxup er , xxup hiv xxmaj news , xxmaj docs … < url >\n      0.0\n    \n    \n      2\n      xxbos xxup like xxup why xxup the xxup fuck xxup are xxup people xxup not xxup vaccinating xxup their xxup kids ? ! ? ! ? ! i xxup do nt xxup get xxup it & & xxup it xxup xxunk xxup me xxup off xxrep 3 !\n      1.0\n    \n    \n      3\n      xxbos i xxmaj know xxmaj i 'm xxmaj late xxup af xxmaj but xxmaj happy # xxunk xxmaj to xxmaj my xxmaj xxunk < user > xxmaj turn xxmaj up xxmaj lil xxmaj bro ! 💯 # xxup mmr @ xxup xxunk xxmaj south … < url >\n      0.0\n    \n    \n      4\n      xxbos < user > < user > xxmaj ppl xxmaj need 2 xxmaj immunize xxmaj their xxmaj kids . xxmaj do xxmaj research . xxmaj most xxmaj diseases xxup worse xxup than xxup inoculation . xxup stop xxmaj this xxmaj misguided xxmaj insanity , xxmaj shots !\n      1.0\n    \n    \n      5\n      xxbos xxmaj let 's get this straight , first it was xxup isis , then it was xxmaj ebola , then back to xxup isis , and now … xxmaj measles ? xxmaj fake xxup isis threats ? xxmaj yep . xxmaj fear . xxmaj fear .\n      0.0\n    \n    \n      6\n      xxbos < user > < user > < user > xxup smdh # xxmaj flu # xxmaj vaccines , # xxmaj pharma # xxmaj fraud , # xxmaj quack # xxmaj science , the # xxup cdc & & # xxup who # health < url >\n      -1.0\n    \n    \n      7\n      xxbos xxup not inspiring to hear xxmaj another xxmaj xxunk in the xxmaj xxunk this xxup am . xxmaj my gr 8 class voted to sing it at grad ( xxunk ) & & then i heard it on xxup mmr , not xxup xxunk .\n      0.0\n    \n    \n      8\n      xxbos [ to roommate ] \\n \" we should go get flu vaccines . \" \\n xxmaj why ? \\n \" well , the flu is n't a great a great disease . \" \\n xxmaj the flu is fine . \\n \" ok . \"\n      0.0"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#regression-learner",
    "href": "drafts/zindi_vaccination.html#regression-learner",
    "title": "Outline",
    "section": "Regression Learner",
    "text": "Regression Learner\n\ndoc(text_classifier_learner)\n\ntext_classifier_learner[source]text_classifier_learner(dls, arch, seq_len=72, config=None, pretrained=True, drop_mult=0.5, n_out=None, lin_ftrs=None, ps=None, max_len=1440, y_range=None, loss_func=None, opt_func='Adam', lr=0.001, splitter='trainable_params', cbs=None, metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95))\n\nCreate a Learner with a text classifier from dls and arch.\nShow in docs\n\n\n\nlearn_reg = text_classifier_learner(dls=dls_reg, \n                                    arch=AWD_LSTM, \n                                    pretrained=True,\n                                    loss_func=MSELossFlat(),\n                                    metrics=[rmse],\n                                    y_range=(-1.2, 1.2)\n                                    ) # Loss func ?? Metrics ??? y_range??? encoder ???\n\n\nlearn_reg.load_encoder('fine_tuned_enc')\n\n<fastai2.text.learner.TextLearner at 0x7fedb215c860>\n\n\n\nlearn_reg.summary()\n\nSequentialRNN (Input shape: ['64 x 53'])\n================================================================\nLayer (type)         Output Shape         Param #    Trainable \n================================================================\nRNNDropout           64 x 53 x 400        0          False     \n________________________________________________________________\nRNNDropout           64 x 53 x 1152       0          False     \n________________________________________________________________\nRNNDropout           64 x 53 x 1152       0          False     \n________________________________________________________________\nBatchNorm1d          64 x 1200            2,400      True      \n________________________________________________________________\nDropout              64 x 1200            0          False     \n________________________________________________________________\nLinear               64 x 50              60,000     True      \n________________________________________________________________\nReLU                 64 x 50              0          False     \n________________________________________________________________\nBatchNorm1d          64 x 50              100        True      \n________________________________________________________________\nDropout              64 x 50              0          False     \n________________________________________________________________\nLinear               64 x 1               50         True      \n________________________________________________________________\nSigmoidRange         64 x 1               0          False     \n________________________________________________________________\n\nTotal params: 62,550\nTotal trainable params: 62,550\nTotal non-trainable params: 0\n\nOptimizer used: <function Adam at 0x7fee89ac9d90>\nLoss function: FlattenedLoss of MSELoss()\n\nModel frozen up to parameter group number 4\n\nCallbacks:\n  - TrainEvalCallback\n  - Recorder\n  - ProgressCallback\n  - ModelReseter\n  - RNNRegularizer\n\n\n\nlearn_reg.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.017378008365631102, lr_steep=9.12010818865383e-07)\n\n\n\n\n\n\nlr=1e-2\n\n\nlearn_reg.fine_tune(epochs=3, base_lr=lr)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      _rmse\n      time\n    \n  \n  \n    \n      0\n      0.517643\n      0.393474\n      0.627276\n      00:11\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      _rmse\n      time\n    \n  \n  \n    \n      0\n      0.407758\n      0.361201\n      0.601000\n      00:13\n    \n    \n      1\n      0.345667\n      0.342141\n      0.584928\n      00:13\n    \n    \n      2\n      0.292315\n      0.336554\n      0.580133\n      00:13"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#predictions",
    "href": "drafts/zindi_vaccination.html#predictions",
    "title": "Outline",
    "section": "Predictions",
    "text": "Predictions\n\ntest_df\n\n\n\n\n\n  \n    \n      \n      tweet_id\n      safe_text\n    \n  \n  \n    \n      0\n      00BHHHP1\n      <user> <user> ... &amp; 4 a vaccine given 2 healthy peeps, FDA think just not worth the AE risk unfortunately.\n    \n    \n      1\n      00UNMD0E\n      Students starting school without whooping cough vaccinations <url> #scpick\n    \n    \n      2\n      01AXPTJF\n      I'm kinda over every ep of <user> being \"ripped from the headlines.\" Measles? Let's get back to crime. #SVU\n    \n    \n      3\n      01HOEQJW\n      How many innocent children die for lack of vaccination each year? Around 1.5 million. Too bad all their parents couldn't be here. #SB277\n    \n    \n      4\n      01JUKMAO\n      CDC eyeing bird flu vaccine for humans, though risk is low: Federal officials said Wednesday they're taking steps… <url>\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      5172\n      ZXVVNC5O\n      jenny mccarthy is on new years rockin eve. what has she done lately besides not vaccinate her kids and give us all goddamn polio??\n    \n    \n      5173\n      ZYIANVI8\n      Measles reported in Clark Co. for 1st time since 2011 <url>\n    \n    \n      5174\n      ZYITEHAH\n      <user> issues alert regarding Measles in TX. Keep your DDx up to date, people! #Emergencymedicine\n    \n    \n      5175\n      ZZ3BMBTG\n      I can't believe people don't vaccinate their kids! I've been vaccinated for everything and then some.\n    \n    \n      5176\n      ZZIYCVNH\n      \"<user>  Alternatives to #Flu Vaccine <url> #natural #health\" A good read with a few new tips &amp; many we #jerf folk know\n    \n  \n\n5177 rows × 2 columns\n\n\n\n\ndoc(learn_reg.dls.test_dl)\n\nDataLoaders.test_dl[source]DataLoaders.test_dl(test_items, rm_type_tfms=None, with_labels=False, **kwargs)\n\nCreate a test dataloader from test_items using validation transforms of dls\nShow in docs\n\n\n\ntest_dl = learn_reg.dls.test_dl(test_items=test_df['safe_text'], verbose=True)\n\n\ndoc(learn_reg.get_preds)\n\nLearner.get_preds[source]Learner.get_preds(ds_idx=1, dl=None, with_input=False, with_decoded=False, with_loss=False, act=None, inner=False, reorder=True, save_preds=None, save_targs=None, concat_dim=0)\n\nGet the predictions and targets on the ds_idx-th dbunchset or dl, optionally with_input and with_loss\nShow in docs\n\n\n\nlearn_reg.get_preds(dl=test_dl, with_decoded=True)\n\n\n\n\n(tensor([[-0.0063],\n         [ 0.8084],\n         [ 0.2124],\n         ...,\n         [ 0.2862],\n         [ 0.8839],\n         [ 0.4553]]), None, tensor([[-0.0063],\n         [ 0.8084],\n         [ 0.2124],\n         ...,\n         [ 0.2862],\n         [ 0.8839],\n         [ 0.4553]]))\n\n\n\npreds, _ , preds_raw = learn_reg.get_preds(dl=test_dl, with_decoded=True)"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#submissions",
    "href": "drafts/zindi_vaccination.html#submissions",
    "title": "Outline",
    "section": "Submissions",
    "text": "Submissions\n\nvaccinate_tweets.ls()\n\n(#4) [Path('SampleSubmission.csv'),Path('Train.csv'),Path('Test.csv'),Path('models')]\n\n\n\nsubmission = pd.read_csv(vaccinate_tweets/'SampleSubmission.csv')\n\n\nsubmission.head()\n\n\n\n\n\n  \n    \n      \n      tweet_id\n      label\n    \n  \n  \n    \n      0\n      00BHHHP1\n      0\n    \n    \n      1\n      00UNMD0E\n      0\n    \n    \n      2\n      01AXPTJF\n      0\n    \n    \n      3\n      01HOEQJW\n      0\n    \n    \n      4\n      01JUKMAO\n      0\n    \n  \n\n\n\n\n\nsubmission['label'] = preds.flatten()\n\n\nsubmission.head()\n\n\n\n\n\n  \n    \n      \n      tweet_id\n      label\n    \n  \n  \n    \n      0\n      00BHHHP1\n      -0.006287\n    \n    \n      1\n      00UNMD0E\n      0.808357\n    \n    \n      2\n      01AXPTJF\n      0.212385\n    \n    \n      3\n      01HOEQJW\n      0.994107\n    \n    \n      4\n      01JUKMAO\n      0.177752\n    \n  \n\n\n\n\nYaaaaaaaaaaaaaaaaaaayyyyyyy!!!!!\n\nsubmission.to_csv('first_baseline.csv', index=False)"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#future-ideas-to-try",
    "href": "drafts/zindi_vaccination.html#future-ideas-to-try",
    "title": "Outline",
    "section": "Future Ideas to try",
    "text": "Future Ideas to try\n\n[Data] Preprocessing\n\nSentencePiece (subword tokenization)\n\n[Data] Language Model\n\nIncorporate additional data during language model (eg: test set, additional social media datasets)\n\n[Model] Ensembling using K fold cross validation [TODO : Link] : average of all predictions.\n[Model] Transformer variants, AWD_QRNN\n[Model] Blending : Models with higher predictions will get higher weights where as models with lower predictions will get lower weights [TODO: Link]\n[Model] Mixed Precision Training\n[Model] Use fit_one_cycle and customize the learning rate.\n[Optimizer] Currently we use ADAM, try with RADAM, Ranger optimizer variants\n[HyperParameter Tuning] Play with hyperparameters\n[Experiment Tracking] Integrating callbacks for Weights and Biases [TODO: Link]"
  },
  {
    "objectID": "drafts/welcome/index.html",
    "href": "drafts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "drafts/post-with-code/index.html",
    "href": "drafts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2020-05-25-Paper_Summary_Covidex.html",
    "href": "posts/2020-05-25-Paper_Summary_Covidex.html",
    "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
    "section": "",
    "text": "This is a paper summary of deploying a Neural Search Engine to answer questions from the COVID-19 dataset.\n\nPaper\ncovidex.ai\nTwitter Announcement by Jimmy Lin\n\n\nNeural Covidex applies state-of-the-art neural network models and artificial intelligence (AI) techniques to answer questions using the COVID-19 Open Research Dataset (CORD-19) provided by the Allen Institute for AI (data release of April 3, 2020). This project is led by Jimmy Lin from the University of Waterloo and Kyunghyun Cho from NYU, with a small team of wonderful students: Edwin Zhang and Nikhil Gupta. Special thanks to Colin Raffel for his help in pretraining T5 models for the biomedical domain.”\n\n\n\n\nalt text\n\n\n\n\n\nThe ongoing pandemic crisis poses a huge challenge to get timely information for public health officials, clinicians, researchers, virologists. In order to respond to this challenge, Allen AI publishes a COVID-19 data set (CORD-19) in collaboration with other research groups. The source for this data set is both research articles published about coronovirus and other related research articles. The aim of this effort is to bring researchers\n\nto apply language processing techniques in order to generate insights & make data driven decisions.\nto provide ways for the front line to consume the recent developments in a digestible form & apply in the field.\n\n\n\n\nJimmy Lin & his research team responded to this call. The two strategies adopted were\n\nReal time users should be able to find answers to any questions associated with COVID\nOther Researchers should be able reuse the components they build. Providing a modular and reusable components is set as part of the requirements.\n\nThe team decided to build end to end real time search application called covidex.ai . They developed the components that powers this engine in a short span of time for the information retrieval need .\n\nkeyword based search interface : This also provides faceted navigation in the form of filters like author, article source, time range and highlighting words from the results that matches with user query.\nneural ranking component that sorts the results with the top most results answering user’s question.\n\n\n\n\nTraditional search architecture comprises of two phases Content Indexing and Keyword Searching. During indexing, content is transformed into an Indexable form called as Document The search engine convert this document into a fundamental data structure called as Inverted Index. This is similar to what you see in Book Appendix where terms are mapped towards the pages. Similarly Inverted index contains terms mapped towards docIds where the term appears & position in the document.\nSearching phase is further divided into two stages retrieval stage and ranking stage. In the first stage, given a search term(s) you will retrieve the list of matching documents from the inverted index. In the second stage, the matched documents are sorted based on the computed relevance score.\nModern Search Architectures\nMore modern multi-stage search architectures from Bing & Alibaba expand the Search Phase with additional reranking stages. Except for the first retrieval stage, the additional subsequent ranking stages will rerank and refine the results further from previous stages .\n\n\n\n\n\nAnserini is an opensource Information Retrieval toolkit in order to solve the reproducibility problems in research and bridge the gap between research and real world systems. This is a tool that is built on top of Lucene, a popular open source search library & enhanced with features specific for conducting IR research. pyserini is a python interface to Anserini.\n\n\n\nThe first challenge faced by the team is representing the Indexable Document. This is fundamental unit of search engine. Results are basically collection of documents. Eg: Tweets, WebPage, Article\nOne of the common challenge wrt Information Retrieval systems, they tend to favor longer documents. In order to give all documents a fairer chances irrespective of its length, normalization is needed.\nThe articles are in the following format\n{\n  title : “Impact of  Masks”,\n  abstract: “Masks protects others from you.”\n  body_text : “Effectiveness of mask /n /n This is being studied …..”\n}\nIn order to compare the effectiveness, the team decide to index the articles in 3 different ways\n\nIndex only the title and abstract of the article as a document\nIndex the entire text of the article as a document combining title, abstract and body_text\nBreak the body_text of the article into paragraphs and each paragraph as separate documents. In this case, the Indexable Document is title, abstract & the paragraph.\n\n\n\n\nOnce the team built the lucene indices based on the above scheme for CORD-19, we can able to search for a given term, retrieve matching documents and ranked them using BM25 scoring function. These indices are available for the researchers to perform further research.\nThe full search pipeline for keyword search is demonstrated using notebooks using Pyserini.\nIn order to provide the users a live system that can be used to answer questions, the team leveraged their earlier work on anserini integration with Solr , open source search platform. The user search interface is built using Blacklight discovery interface with Ruby on Rails for faceting & highlighting.\n\n\n\nalt text\n\n\n\n\n\nIn addition to that the team also built a highlighting feature on of keyword search. This allows the user to quickly scan the results with the matched keywords in the document.\nIt is built using BioBERT. The idea behind it is that a) the candidate matched documents & convert them into sentences. b) Similarly the query is treated as a sentence. The sentences & query are in turn converted into its numerical representations (vectors). Top sentences closer to the query are obtained using the cosine similarity. The top-K words in the context are highlighted in these top sentences.\n\n\n\n\nThe research group was already working on the neural architectures specifically applying transfer learning on retrieval/ranking based problems.\n\nBERT for query based Passage Ranking : Applying transfer learning for passage reranking pre-trained on MS-MARCO dataset\nBERTserini for retrieval-based question answering: Incorporating Anserini Retriever to retrieve the top K segments of text followed by BERT based pre-trained model to retrieve the answer span.\n\nTypically the task of reranking is turn the problem into a classification task where we take the query, candidate_document & predict the target as (relevant, not-relevant). To avoid the costly operation of performing classification on the entire corpus, this is applied at the reranking stages. The engine gets the top K documents from the previous retrieval stage and rerank them using machine learning model. As part of reranking stage, the team leveraged Sequence to Sequence Model for reranking (Nogueira et al. 2020).\nStages involved in training the reranking model using Transfer Learning Methodology\nPre-training →Fine Tuning → Inference\n\n[Pre-training] Transformer based Language Model trained on MS Marco dataset\n[Fine Tuning] Given a query q, document D, the model is fine tuned to predict the output as either true or false as targets indicating the relevance.\n[Inference] In reranking setting, for each candidate documents, predict the prob distribution of (relevant, non-relevant) and sort the scores of relevant doc(true outputs) alone.\n\nTraining a language model and the encoder from this fined tuned language model is normally used for the downstream tasks like Classification in transfer learning methodology. But this method of applying Sequence to Sequence model (based on T5) is quite new for document ranking setting (Nogueira et al., 2020).\nThe reasoning provided was the predicted target words can capture the relatedness through pre-training. This is based on encoder-decoder architecture & uses a similar masked language modeling objective. Given a query, document the model is fine tuned to produce true or false if the document is relevant or not to the query.\n\n\n\n\nThe authors rightfully mention that the individual components comprising such a system is evaluated against various test datasets. But as this is specific to an evolving dataset like CORD-19, there is no such existing test collections.\nIt is not always necessary that ranking is the most important for such an end to end system. We have to switch to an outcome based measure rather than a single output based measure like batch retrieval evaluations(MRR, nDCG). Eg: “Did the researchers, practitioners get their questions answered?” How many of them are not finding the answers? So involving human in the loop to qualitatively evaluate the results is essential to know if the system is really contributing towards the efforts fighting the pandemic.\nWhat if the exploratory users do not know the right type of keywords to use ? In that case ranking is a wrong goal to pursue.\nCurrent challenge is all the targeted users are working on the front line and hard to provide qualitative feedback about search experience. So the author asks for more hallway usability testing to gather insights from the users.\n\n\n\n\nAn end to end system like Covidex is not possible without the power of the current Open Source Software ecosystem, Open culture of curating, cleaning & sharing the data with the community (Thanks to CORD-19 by Allen AI) and pre-trained language models like MS-Marco etc.\nGood software engineering practices is the foundation for a team and ensure that the underlying software components can be replicated & reused to provide this system. This is essential to rapidly explore and experiment with new ideas.\nBuilding a strong research culture to produce the results in the form of open source software artifacts aid the community in reproducing the results and build on top of it.\nReminder about the mismatch between producing research code for conference and building a system for a real users. For example concerns like latency of search requests, throughput about the number of users, deploying & managing a system in production, user experience does not arise in a research setting.\n\n\n\n\nLack of proper training data & human annotators is a common challenge. Leveraging pre-trained models on MS-MARCO is critical for ranking tasks in this type of situation.\nThe experimentation mindset need to be adopted and one need to interactive computing tools like pyserini to experiment with search index. This allows the search practitioners to constantly iterate and learn from these experiments.\nAdoption of Openness in not only the source but science and data & the way we work is truly inspiring."
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "",
    "text": "Enhancing Search Engine Performance: Addressing Vocabulary Mismatch with Machine Learning"
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html#doc2query",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html#doc2query",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "Doc2Query",
    "text": "Doc2Query\nMost of the below are excerpts from this paper for my own reference.\nReference: Paper\nAbstract: - One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents’ content. - From the perspective of a question answering system, this might comprise questions the document can potentially answer. - Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. - By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. - In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.\n\n\n\ndoc-expansion-by-query-pred\n\n\nAdvantage\n\nprimary advantage of this approach is that expensive neural inference is pushed to indexing time,\n“bag of words” queries are against an inverted index built on the augmented document collection.\n\nTwo important observations - Model tends to copy some words from the input document (e.g., Washington DC, River, chromosome), meaning that it can effectively perform term re-weighting (i.e., increasing the importance of key terms). - Nevertheless, the model also produces words not present in the input document (e.g., weather, relationship), which can be characterized as expansion by synonyms and other related terms.\nQuote about T5 for doc expansion and the performance on BEIR dataset\n\nIn contrast, document expansion based docT5query is able to add new relevant keywords to a document and performs strong on the BEIR datasets. It outperforms BM25 on 11/18 datasets while providing a competitive performance on the remaining datasets."
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html#related",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html#related",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "Related",
    "text": "Related\n\nBEIR - Heterogeneous Benchmark for Information Retrieval.\nImprove text ranking with few shot promptin by Jo Kristen Bergum\nhttps://huggingface.co/blog/how-to-generate"
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html#code",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html#code",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "Code",
    "text": "Code\n\n%pip install transformers sentencepiece -qqq > /dev/null\n\nHere we are using pretrained T5 model from HuggingFace for document expansion.\nAs quoted in the model page\n\nDocument expansion: You generate for your paragraphs 20-40 queries and index the paragraphs and the generates queries in a standard BM25 index like Elasticsearch, OpenSearch, or Lucene. The generated queries help to close the lexical gap of lexical search, as the generate queries contain synonyms. Further, it re-weights words giving important words a higher weight even if they appear seldomn in a paragraph. In our BEIR paper we showed that BM25+docT5query is a powerful search engine. In the BEIR repository we have an example how to use docT5query with Pyserini.\nDomain Specific Training Data Generation: It can be used to generate training data to learn an embedding model. On SBERT.net we have an example how to use the model to generate (query, text) pairs for a given collection of unlabeled texts. These pairs can then be used to train powerful dense embedding models.\n\n\n# https://huggingface.co/doc2query/stackexchange-title-body-t5-small-v1\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\nmodel_name = 'doc2query/stackexchange-title-body-t5-small-v1'\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\ntext = \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n\n\ninput_ids = tokenizer.encode(text, max_length=384, truncation=True, return_tensors='pt')\noutputs = model.generate(\n    input_ids=input_ids,\n    max_length=64,\n    do_sample=True,\n    top_p=0.95,\n    num_return_sequences=5)\n\nprint(\"Text:\")\nprint(text)\n\nprint(\"\\nGenerated Queries:\")\nfor i in range(len(outputs)):\n    query = tokenizer.decode(outputs[i], skip_special_tokens=True)\n    print(f'{i + 1}: {query}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText:\nPython is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n\nGenerated Queries:\n1: What is Python's design philosophy?\n2: What are the characteristics of Python?\n3: What does \"python's design philosophy\" mean?\n4: What is the difference between Python and Java?\n5: What is the logic behind Python's programming languages?\n\n\n\ndef generate(text: str):\n    input_ids = tokenizer.encode(text, max_length=384, truncation=True, return_tensors='pt')\n    outputs = model.generate(\n      input_ids=input_ids,\n      max_length=64,\n      do_sample=True,\n      top_p=0.95,\n      num_return_sequences=5)\n\n    print(\"Text:\")\n    print(text)\n\n    gen_texts = []\n    print(\"\\nGenerated Queries:\")\n    for i in range(len(outputs)):\n        query = tokenizer.decode(outputs[i], skip_special_tokens=True)\n        #print(f'{i + 1}: {query}')\n        gen_texts.append(query)\n    return gen_texts\n\n\ntext = 'In certain situations it is needed to have source code preprocessed by `gcc` without undergoing the full compilation process. For example, this might be necessary when embedded SQL is included in C or C++ programs and the preprocesssed file will be passed on to another tool which will convert the SQL in native source code.  - Red Hat Enterprise Linux (RHEL)   --- ***Disclaimer:** Links contained herein to an external website(s) are provided for convenience only. Red Hat has not reviewed the links and is not responsible for the content or its availability. The inclusion of any link to an external website does not imply endorsement by Red Hat of the website or their entities, products or services. You agree that Red Hat is not responsible or liable for any loss or expenses that may result due to your use of (or reliance on) the external site or content.*--- To make GCC stop after the preprocessing stage, use the option `-E`, as explained in [GNU- GCC options](https://gcc.gnu.org/onlinedocs/gcc/Overall-Options.html#Overall-Options). In other words, Using the `-E` parameter with `gcc` or `g++` will produce **only** the preprocessed source code: $ gcc-E program.c-o program.preprocessed The `program.preprocessed` file will contain the file preprocessed by `gcc` (`Macros` will be expanded and all include files will be resolved). This preprocessed output will contain lines such as the following ones: ~~~ # 131 &quot;/usr/include/bits/types.h&quot; 3 4 # 1 &quot;/usr/include/bits/typesizes.h&quot; 1 3 4 # 132 &quot;/usr/include/bits/types.h&quot; 2 3 4 ~~~ These lines are line markers that show from which include files specific source code was taken. If those lines are not desired run the same command adding the `-P` parameter: $ gcc-E-P program.c-o program.preprocessed'\ngenerate(text)\n\nText:\nIn certain situations it is needed to have source code preprocessed by `gcc` without undergoing the full compilation process. For example, this might be necessary when embedded SQL is included in C or C++ programs and the preprocesssed file will be passed on to another tool which will convert the SQL in native source code.  - Red Hat Enterprise Linux (RHEL)   --- ***Disclaimer:** Links contained herein to an external website(s) are provided for convenience only. Red Hat has not reviewed the links and is not responsible for the content or its availability. The inclusion of any link to an external website does not imply endorsement by Red Hat of the website or their entities, products or services. You agree that Red Hat is not responsible or liable for any loss or expenses that may result due to your use of (or reliance on) the external site or content.*--- To make GCC stop after the preprocessing stage, use the option `-E`, as explained in [GNU- GCC options](https://gcc.gnu.org/onlinedocs/gcc/Overall-Options.html#Overall-Options). In other words, Using the `-E` parameter with `gcc` or `g++` will produce **only** the preprocessed source code: $ gcc-E program.c-o program.preprocessed The `program.preprocessed` file will contain the file preprocessed by `gcc` (`Macros` will be expanded and all include files will be resolved). This preprocessed output will contain lines such as the following ones: ~~~ # 131 &quot;/usr/include/bits/types.h&quot; 3 4 # 1 &quot;/usr/include/bits/typesizes.h&quot; 1 3 4 # 132 &quot;/usr/include/bits/types.h&quot; 2 3 4 ~~~ These lines are line markers that show from which include files specific source code was taken. If those lines are not desired run the same command adding the `-P` parameter: $ gcc-E-P program.c-o program.preprocessed\n\nGenerated Queries:\n\n\n['Make GCC stop after preprocessing',\n 'GCC: how to stop \"gcc\" after the full compilation process?',\n 'Retrieving Source code preprocessed by gcc without undergoing full compilation process',\n \"Why can't I make GCC stop after the full compilation process?\",\n 'Can the -E parameter for gcc or g++ prevent any preprocessed source code from being loaded?']\n\n\n\ntext = 'Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6 ? * Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6? Fails with the following message ~~~ ftp://X.X.X.X/Bootstrap \\\\X86PC\\\\BStrap.0... Permission denied (0x0212603c) ~~~ * Red Hat Enterprise Linux 6 * KVM GPXe boot * Two dhcp servers on different subnet and are accessed via relay agetns First dhcp server assign IP address Second dhcp server provides next-server details  If gpxe is used to pxe-boot a KVM guest, it uses the next-server from the first dhcp offer and if it fails, then does not re-try with the next-server option provided in the second dhcp server offer packet Update gpxe packages to below versions or above.- gpxe-bootimgs-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-qemu-0.9.7-6.10.el6.noarch.rpm This has been fixed by errata http://rhn.redhat.com/errata/RHBA-2013-1628.html'\ngenerate(text)\n\nText:\nWhy gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6 ? * Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6? Fails with the following message ~~~ ftp://X.X.X.X/Bootstrap \\X86PC\\BStrap.0... Permission denied (0x0212603c) ~~~ * Red Hat Enterprise Linux 6 * KVM GPXe boot * Two dhcp servers on different subnet and are accessed via relay agetns First dhcp server assign IP address Second dhcp server provides next-server details  If gpxe is used to pxe-boot a KVM guest, it uses the next-server from the first dhcp offer and if it fails, then does not re-try with the next-server option provided in the second dhcp server offer packet Update gpxe packages to below versions or above.- gpxe-bootimgs-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-qemu-0.9.7-6.10.el6.noarch.rpm This has been fixed by errata http://rhn.redhat.com/errata/RHBA-2013-1628.html\n\nGenerated Queries:\n\n\n['gpxe boot fails when next-server details are offered by the second dhcp server',\n 'gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6?',\n 'gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6',\n 'gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6?',\n 'Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6?']\n\n\nIf I provide the title included in the contents, the generated text replicates the title.\n\ntext = 'Unable change permission to NFS share mounted at the client. * The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.'\ngenerate(text)\n\nText:\nUnable change permission to NFS share mounted at the client. * The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.\n\nGenerated Queries:\n\n\n['Unable change permission to NFS share mounted at the client',\n 'Unable change permission to NFS share mounted at the client',\n 'NFS client \"Unable change permission to NFS share mounted at the client.\"',\n 'Unable change permission to NFS share mounted at the client',\n 'Unable change permission to NFS share mounted at the client']\n\n\nExclude the title. Provides much better variations.\n\ntext = '* The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.'\ngenerate(text)\n\nText:\n* The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.\n\nGenerated Queries:\n\n\n['SSH NFS clients permissions can be changed, even by using root',\n 'NFS: root permissions not changing',\n \"Why can't the directory permissions be changed in the NFS share?\",\n 'NFS share fails to resolve NFS: directory permissions cannot be changed',\n 'How can I set NFS permissions to be changed with root?']\n\n\n\ntext = '- qpid process segfaulting with backtrace attached- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - MRG Messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::FrameSet::getContentSize (this=0x128) at qpid/framing/FrameSet.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::QueuePolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/QueuePolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::Queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::Queue::dequeueCommitted (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ Complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached C++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) MRG 2.1 (that is qpid 0.14)'.lower()\ngenerate(text)\n\nText:\n- qpid process segfaulting with backtrace attached- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - mrg messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::frameset::getcontentsize (this=0x128) at qpid/framing/frameset.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::queuepolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/queuepolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::queue::dequeuecommitted (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached c++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) mrg 2.1 (that is qpid 0.14)\n\nGenerated Queries:\n\n\n['qpid process segfaulting with backtrace attached',\n 'qpid segfaulting with backtrace attached',\n 'Can anyone help me with my data segfaulting qpid transaction?',\n 'qpid segfault with backtrace attached- coredump has mrg messaging 2.0',\n 'qpid process segfaulting with backtrace attached - coredump']\n\n\nSome post-processing required such lower casing in order to generate unique text.\nExclude the issue statements\n\ntext = '- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - MRG Messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::FrameSet::getContentSize (this=0x128) at qpid/framing/FrameSet.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::QueuePolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/QueuePolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::Queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::Queue::dequeueCommitted (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ Complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached C++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) MRG 2.1 (that is qpid 0.14)'.lower()\n\ngenerate(text)\n\nText:\n- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - mrg messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::frameset::getcontentsize (this=0x128) at qpid/framing/frameset.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::queuepolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/queuepolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::queue::dequeuecommitted (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached c++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) mrg 2.1 (that is qpid 0.14)\n\nGenerated Queries:\n\n\n['coredump for mrg messaging in coredump',\n 'coredump backtrace problem with messaging 2.0',\n 'mrg messaging, coredump and coredump',\n 'mrg messaging 1.1 and coredump: why is the backtrace not used?',\n 'Why has my backAmplitude (_0) returned?']\n\n\n\ntext = 'We are passing through a migration from EPP with SP 5.1 to EPP with SP 5.2.0 we noticed some strange behaviors. For example, the context menu in &quot;Content Explorer&quot; is all messed. How can we solve this? - Red Hat JBoss Portal Platform (JPP also known as EPP)- 5.2.0- Site Publisher (SP)- 5.2.0  In Site Publisher Groovy templates are also contents from JCR (JCR nodes). When we are passing through a migration we need to keep the JCR shared folder and do not change it. It causes that some Groovy templates from the old installation are not correctly replaced to the new ones, which leads to compatibility issues with EPP+SP 5.2. Upgrade to newest EPP 5.2.x release and [migrate all templates to the new version](https://access.redhat.com/site/solutions/85613).'\ngenerate(text)\n\nText:\nWe are passing through a migration from EPP with SP 5.1 to EPP with SP 5.2.0 we noticed some strange behaviors. For example, the context menu in &quot;Content Explorer&quot; is all messed. How can we solve this? - Red Hat JBoss Portal Platform (JPP also known as EPP)- 5.2.0- Site Publisher (SP)- 5.2.0  In Site Publisher Groovy templates are also contents from JCR (JCR nodes). When we are passing through a migration we need to keep the JCR shared folder and do not change it. It causes that some Groovy templates from the old installation are not correctly replaced to the new ones, which leads to compatibility issues with EPP+SP 5.2. Upgrade to newest EPP 5.2.x release and [migrate all templates to the new version](https://access.redhat.com/site/solutions/85613).\n\nGenerated Queries:\n\n\n['Migrate from EPP to SP 5.2 has strange behavior',\n 'Magento migration for a different version from SharePoint with new environment is messed up',\n 'Disable JCR contents with custom templates, SP 5.1 to EPP + SP 5.2.0',\n 'Migrating JCR and context menu messes',\n 'JCR menu of JCR-Node is corrupted']"
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html#refactor",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html#refactor",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "Refactor",
    "text": "Refactor\nRefactor the code for reusability and also instead use Auto classes from transformers to load the Sequence 2 Sequence model.\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nfrom typing import List\n\nclass QueryGenerator:\n    def __init__(self, model_path:str, use_fast:bool=False):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=use_fast)\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n        \n    def generate(self, text: str, max_length:int=384, output_length=64, num_return_sequences:int=5, top_p:float=0.95)->List[str]:\n        input_ids = self.tokenizer.encode(text, max_length=max_length, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            outputs = self.model.generate(\n                input_ids=input_ids,\n                max_length=output_length,\n                do_sample=True,\n                top_p=top_p,\n                num_return_sequences=num_return_sequences)\n        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n\ntext = 'In certain situations it is needed to have source code preprocessed by `gcc` without undergoing the full compilation process. For example, this might be necessary when embedded SQL is included in C or C++ programs and the preprocesssed file will be passed on to another tool which will convert the SQL in native source code.  - Red Hat Enterprise Linux (RHEL)   --- ***Disclaimer:** Links contained herein to an external website(s) are provided for convenience only. Red Hat has not reviewed the links and is not responsible for the content or its availability. The inclusion of any link to an external website does not imply endorsement by Red Hat of the website or their entities, products or services. You agree that Red Hat is not responsible or liable for any loss or expenses that may result due to your use of (or reliance on) the external site or content.*--- To make GCC stop after the preprocessing stage, use the option `-E`, as explained in [GNU- GCC options](https://gcc.gnu.org/onlinedocs/gcc/Overall-Options.html#Overall-Options). In other words, Using the `-E` parameter with `gcc` or `g++` will produce **only** the preprocessed source code: $ gcc-E program.c-o program.preprocessed The `program.preprocessed` file will contain the file preprocessed by `gcc` (`Macros` will be expanded and all include files will be resolved). This preprocessed output will contain lines such as the following ones: ~~~ # 131 &quot;/usr/include/bits/types.h&quot; 3 4 # 1 &quot;/usr/include/bits/typesizes.h&quot; 1 3 4 # 132 &quot;/usr/include/bits/types.h&quot; 2 3 4 ~~~ These lines are line markers that show from which include files specific source code was taken. If those lines are not desired run the same command adding the `-P` parameter: $ gcc-E-P program.c-o program.preprocessed'\nprint(\"Text:\")\nprint(text)\n\n\nprint(\"\\nGenerated Queries:\")\nmodel_name = 'doc2query/stackexchange-title-body-t5-small-v1'\nqGen = QueryGenerator(model_path=model_name)\nqGen.generate(text, num_return_sequences=5)"
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html#conclusion",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html#conclusion",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "Conclusion",
    "text": "Conclusion\nThis post should serve as the initial starting point for the Document Expansion technique and Refer the Colbert paper for the results using these approaches in standard benchmarks such as BEIR. In order to use these technique in your domain, index your documents using tools such as pyserini and compare with BM25 baseline."
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html",
    "href": "posts/2020-09-20-intent-detection.html",
    "title": "Intent detection",
    "section": "",
    "text": "Intent Classification is a type of supervised text classification problem. It is used to predict the motivation of the user providing search keywords in a given search engine. Each user visiting the site have a goal in mind and they express in the form of keywords. We have to optimize for satisfying their goal in addition to returning the best documents matching the keywords as search results. In Search for intent, Not inventory[1] Daniel Tunkelang author of Query Understanding publication emphasizes that\n\n“Searchers don’t care what you have in your catalog or how you organize your inventory. They care about what they want. Make it easy for searchers to express their intent, and craft the search experience around their intent.”\n\nHence as part of Red Hat Customer Portal personalization, we started working on identifying the intents for our site visitor and classify the search queries in order to craft the search experience based on their intent."
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#related-work",
    "href": "posts/2020-09-20-intent-detection.html#related-work",
    "title": "Intent detection",
    "section": "Related Work",
    "text": "Related Work\nClassification of searches is domain specific and there is no one size fits all approach. Taxonomy of Web Searches[2] classify the queries into these categories 1. Navigational 2. Informational 3. Transactional\n\nTaxonomy of Ecommerce Searches[3] classifies them into following categories & each with a distinct search behavior.\n\n\nShallow Exploration Queries are short vague queries that a user may use initially in exploring the product space.\nTargeted Purchase Queries are queries used by users to purchase items that they are generally familiar with, thus without much decision making.\nMajor-Item Shopping Queries are used by users shopping a major item relatively expensive & requires some serious exploration, but typically in a limited scope of choices.\nMinor-Item Shopping Queries are provided by users to shop for minor items that are generally not very expensive, but still require some exploration of choices.\nHard-Choice Shopping Queries are used by users who want to deeply explore all the candidate products before finalizing the choice often appropriate when multiple products must be carefully compared with each other.\n\n\nExamples\n\n\nWe looked at how others are handling the intent classification problem because of the shorter length of the text and latency requirement. Deep Search Query Intent Understanding from LinkedIn provides two designs for modeling 1. Predicting the intent in typeahead search using character models 2. Predicting the intent for complete search queries to do Seach Results Page(SERP) Blending. Each linkedin usecase has its own latency and accuracy requirement, former having higher latency but with lower accuracy whereas the latter with acceptable latency but with higher accuracy. Though they explored deep learning CNN, LSTM, BERT models in this paper, production baseline model used is the traditional logistic regression model. Unless you can reduce the size of the model, latency will pose a challenge for productionizing these bigger models.\n\n“The production baseline model is a logistic regression model, with bag-of-words features, and user profile/behavioral features, etc.”\n\n\nThe next one is Discovering and Classifying In-app Message Intent at Airbnb where they classify the guest messaging intents such as Parking, Checking In and try to provide timely responses. Here they found CNN performed better and has faster training with a shorter serving time."
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#problem-framing",
    "href": "posts/2020-09-20-intent-detection.html#problem-framing",
    "title": "Intent detection",
    "section": "Problem Framing",
    "text": "Problem Framing\nWe leveraged the redhat customer portal top tasks survey in which most of the users indicated the aspect of problem solving as the main reason for the visit in 4/5 responses. Keeping that in mind, we analyzed the search queries for the patterns and came up with these four buckets.\n\nBroad Explorational are single worded vague queries provided by users exploring the product portfolio, a component, vulnerability.\nTargeted Queries (Eg: CVE-1243, RHBA-2351, RHSA-3194) such as Bug fixes, Advisories, Enhancements.\nUsage Queries are provided by users wanting to know how to use specific component or aspect of a product.\nTroubleshoot are provided by users facing an issue and more likely leading to case creation.\n\nInstead of providing the model with poor data, we wanted the model to focus only on classifying the Usage and Troublesoot. We avoided passing the models with vague queries and targeted queries and let the keyword search, onebox and curated searches to handle these types of queries.\n\n\n\n\n\n\n\n\nINTENT\nDEFINITION\nEXAMPLES\n\n\n\n\nTroublesoot\nWhen a user reports an issue/error message or expresses the text in negative forms explicitly that he/she wants to troubleshoot/debug, then such statements are considered to be of TROUBLESHOOT intent\nX Window won’t start after updating xorg-x11-server-Xorg package?\n\n\n\n\n\n\n\nUsage\nUSAGE intent is considered when it is more along the lines of how to use a product/component or perform an action.\nHow can I block video streaming using squid ?"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#data-collection",
    "href": "posts/2020-09-20-intent-detection.html#data-collection",
    "title": "Intent detection",
    "section": "Data Collection",
    "text": "Data Collection\nIn order to iterate and perform rapid prototyping, we leverage ML teaching and annotation tool Prodigy. The recommendation is to perform manual annotation with binary label in order to keep cognitive load low. This allows the annotator to focus only one concept at a time. We started looking at the Portal Search queries and manually annotated them as TROUBLESHOOT or not. Please check this excellent text classification tutorial by Ines Montani.\n\nIf you only provide a single label, the annotation decision becomes much simpler: does the label apply or not? In this case, Prodigy will present the question as a binary task using the classification interface. You can then hit ACCEPT or REJECT. Even if you have more than one label, it can sometimes be very efficient to make several passes over the data instead of selecting from a list of options. The annotators can focus on one concept at a time, which can reduce the potential for human error – especially when working with complicated texts and label schemes.\n\n$ prodigy textcat.teach troubleshoot en_core_web_sm troubleshoot.jsonl\nWe collected around 1500 training samples to bootstrap the project with the help of Active learning features provided. Learn more about active learning from here\nExample training samples\n{\"text\":\"add new nfs storage\",\"label\":\"USAGE\",\"answer\":\"accept\"}\n{\"text\":\"sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\",\"label\":\"TROUBLESHOOT\",\"answer\":\"accept\"}"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#imports-and-utilty-functions",
    "href": "posts/2020-09-20-intent-detection.html#imports-and-utilty-functions",
    "title": "Intent detection",
    "section": "Imports and Utilty functions",
    "text": "Imports and Utilty functions\ndef common_word_removal(df, common_words):\n    \"\"\"This is to remove manually identified common words from the training corpus.\"\"\"\n    \n    # Manual list of common words to be removed based on above data. This will change based on text data input.\n    # We cannot automate this to directly omit the first x words as these words are very relevant to our use case\n    # of inferring intent\n    \n    df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in common_words))\n    return df\n\ndef drop_nulls(df):\n    \"\"\"This function drops the rows in data with null values. \n    It takes in the raw queries as the input.\"\"\"\n    \n    print('Number of queries with null values: ' + str(df['text'].isnull().sum()))\n    df.dropna(inplace=True)\n    return df\n\n\ndef avg_word(sentence):\n    \"\"\"Used in EDA to calculate average wordlength per query\"\"\"\n    words = sentence.split()\n    return (sum(len(word) for word in words)/len(words))"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#data-preparation",
    "href": "posts/2020-09-20-intent-detection.html#data-preparation",
    "title": "Intent detection",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nLoading the data in dataframe\nThis reads the dataset in json lines format into pandas dataframe.\ndef load():\n    raw_data - load_raw_data(pd.read_json(f\"{DATA_DIR}/portalsearch/portalsearch.jsonl\", lines=True))\n\n    # Joining the 'label' and 'answer' column to become one. \n    # E.g.: Troubleshoot-accept, Troubleshoot-reject, Usage-accept, Usage-reject\n    raw_data['label'] = raw_data['label'] + '-' + raw_data['answer']\n\n    # Only selecting the 'accept' labels.\n    raw_data = raw_data[raw_data['label'].isin(['TROUBLESHOOT-accept','USAGE-accept'])]\n\n    raw_data.drop('answer',axis=1,inplace=True)\n    return raw_data\n\n\nPreprocessing\nWe performed the following preprocessing\n\nAdding standard features like word count, character count and average word length per query to the dataframe.\nRemove captalization by lowercasing all words\nRemoving punctuations and special characters\nRemove traditional stopwords like “the”, “a”, “an” and “and”\nRemove noisy data that are of low value and commonly occuring terms\nLemmatization\n\ndef eda(df):\n    \"\"\"Classic EDA like word count per query, average word length per query etc.\"\"\"\n    \n    # Word count per query\n    df['word_count'] = df['text'].apply(lambda x: len(str(x).split(\" \")))\n    \n    # Character count per query\n    df['character_count'] = df['text'].str.len() # this also includes spaces\n\n    # Average word length per query\n    df['avg_wordlength_per_query'] = df['text'].apply(lambda x: avg_word(x))\n\n    return df\ndef preprocessing(df):\n    \"\"\"Traditional Pre-processing steps.\"\"\"\n    \n    # Lower case for all words. This helps in removing duplicates later\n    df['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    \n    # Removing Punctuations and special characters using regular expression\n    df['text'] = df['text'].str.replace('[^\\w\\s]','')\n    \n    # Removing non-english words\n    df['text'] = df['text'].apply(lambda row: row.encode('ascii',errors='ignore').decode())\n    \n    # Removing stopwords\n    df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n    \n    # Lemmatization\n    lemmatizer = WordNetLemmatizer()\n    df['text'] = df['text'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n    \n    # Common word removal\n    # Remove commonly occurring words from our text data \n    # as their presence will not of any use in classification of our text data.\n    # This step would require manual scanning through the list.\n    frequent_words = pd.Series(' '.join(df['text']).split()).value_counts()\n    print('Top 20 frequent words are \\n' + str(frequent_words[:20]))\n    print('\\n')\n    \n    \n    # Rare words removal\n    # Because they’re so rare, the association between them and other words is dominated by noise. \n    # Hence we can remove them and later decide whether or not the results improved based on it.\n    \n    # Printing out rare words occuring less than 50 times.\n    rare_words = frequent_words[frequent_words < 50]\n    print('Top 10 rare words are: \\n' + str(rare_words[-10:]))\n    print('\\n')\n    \n    # Dropping queries which are empty after all the pre-processing\n    df['text'].replace('', np.nan, inplace=True)\n    df.dropna(inplace=True)\n    print('The final number of queries after preprocessing are: ' + str(df.shape))\n    \n    return df"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#modeling",
    "href": "posts/2020-09-20-intent-detection.html#modeling",
    "title": "Intent detection",
    "section": "Modeling",
    "text": "Modeling\n\nTrain Test Split\nNext we split the data into training and test data with 3:1 split\ndef train_test_splits(df, test_size):\n    \"\"\"Splitting raw data into training and test data.\"\"\"\n    \n    X = raw_data['text']\n    y = raw_data['label']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    \n    return X_train, X_test, y_train, y_test\n# Splitting raw dataset into testing and training data\nX_train, X_test, y_train, y_test = train_test_splits(raw_data,0.33)\n\n\nVectorization\nThe text needs to be converted into a format that a model could interpret ie numbers. This process is called vectorization. We started with Term Frequency-Inverse Document Frequency(TF-IDF) method using sklearn TfidfVectorizer\n\nTF-IDF is a way to calculate the ‘importance’ of each word in the dataset. This vectorizer calculates how often a given word appears in the string, and downscales words that appear across different strings.\n\nAn example illustrating how tfidf is calculate for two strings\n\nCode sample\ndef text_vectorization_tfidf(X_train,X_test):\n    \"\"\"tf-idf vectorization using sklearn\"\"\"\n    \n    vectorizer = TfidfVectorizer()\n    X_train_vec = vectorizer.fit_transform(X_train)\n    X_test_vec = vectorizer.transform(X_test)\n    \n    # pretty printing the vocabulary built by the tf-idf vectorizer\n    # pprint.pprint(vectorizer.vocabulary_)\n    \n    return vectorizer, X_train_vec, X_test_vec\n\n# Text Vectorization of training data\nvectorizer, X_train_vec, X_test_vec = text_vectorization_tfidf(X_train, X_test)\n\n\nLinearSVC\n\nThe algorithm we used is a linear support vector classifier (SVC), a commonly used text classification algorithm that works by finding the line or hyper-plane that best differentiates two groups of data points. It is a Support Vector Machine with a linear kernel.Learn more about SVM from here\n\nHere we are performing 3-fold cross validation to improve the generalization and minimize the overfitting on validation set. With the given training set, it is split into 3 folds and one of the fold is used for validation and a score is calculated. Similarity this process repeated with the rest of the folds and the average of all the scores is used as the final score. Please see the excellent scikit guide here for additional details\ndef build_model(X_train_vec, y_train):\n    \"\"\"Build an SVM model with a linear kernel\"\"\"\n    \n    svm = LinearSVC(class_weight=\"balanced\")\n    linear_svc = CalibratedClassifierCV(svm,method='sigmoid') \n     \n    # 3-fold Cross-Validation\n    print(cross_val_score(linear_svc, X_train_vec, y_train, cv=3))\n    \n    # Fitting the model after tuning parameters based on cross-validation results\n    linear_svc.fit(X_train_vec, y_train) \n    \n    return linear_svc\n\n# Building, cross validation and fitting of model\nmodel = build_model(X_train_vec, y_train)"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#evaluation",
    "href": "posts/2020-09-20-intent-detection.html#evaluation",
    "title": "Intent detection",
    "section": "Evaluation",
    "text": "Evaluation\nThe model can be interpreted by looking at the confusion matrix of true labels and predicted labels. Confusion matrix(a.k.a error matrix) allows to understand the performance of the model on unseen or test queries. We can see the accuracy of the model to be around 95%\ndef make_predictions(model,X_test_vec):\n    \"\"\"Makes predictions and spits out confusion matrix.\"\"\"\n    \n    # Predicting results on test data\n    predictions = model.predict(X_test_vec)\n    \n    # Accuracy of model\n    print('Accuracy of model is ' + str(round(accuracy_score(y_test, predictions)*100,2)) + '%')\n    \n    # Precision, Recall and other metrics\n    print(str(classification_report(y_test, predictions, target_names=['TROUBLESHOOT-accept', 'USAGE-accept'])))\n\n    # Confusion Matrix\n    labels = ['TROUBLESHOOT-accept','USAGE-accept']\n    Confusion_Matrix = confusion_matrix(y_test, predictions, labels)\n\n    # Plotting the confusion matrix\n    df_cm = pd.DataFrame(Confusion_Matrix, index = labels,\n                      columns = labels)\n\n    ax = plt.figure(figsize = (12,8))\n    sns.heatmap(df_cm, annot=True,fmt='g')\n\n    # labels, title and ticks\n    ax.suptitle('Confusion Matrix')\n    plt.xlabel('Predicted labels', fontsize=18)\n    plt.ylabel('True labels', fontsize=16)"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#deployment",
    "href": "posts/2020-09-20-intent-detection.html#deployment",
    "title": "Intent detection",
    "section": "Deployment",
    "text": "Deployment\n\nWeb application\nOnce the model is trained and saved, the next step is to create a simple REST endpoint called introspect where the client applications can send the text blob and get the intent prediction as the response. The application endpoint is built using python, flask web framework, gunicorn web containier with 4 parallel workers. The overview of the application is as follows\n\nLoading the model at the application startup\nOnce the query is received, send it to the model only if it is not a known lexical pattern like CVE, Errata, numeric id, url. This prevents garbage inputs and invalid predictions.\nIf the query is one of the known patten return the response with the intent as OTHER.\nTransform the query into a vectorized form before model prediction\nPredict the query and return the prediction, probablity of the predicted class as confidence score.\n\n\n\nContainerizing the application\nBasically containeraization is a modern way to package the application with your code, dependencies, configuration into a format (Eg: Docker Image) suitable to run anywhere whether it’s public cloud like aws or in your own datacenter.\nCode Sample for util.py\nfrom flask import request, Response\nfrom validators import ValidationFailure\nfrom validators.url import url\nfrom functools import wraps\nimport logging\n\ndef is_url(keyword: str) -> bool:\n    '''\n    Returns True if the keyword contains a valid url else return False\n    '''\n    try:\n        value = url(keyword)\n    except ValidationFailure:\n        return False\n    return value\n\ndef has_known_lexical_pattern(keyword: str) -> bool:\n    '''\n    Return True if the keyword contains known navigational intent like CVE, Errata, id or url else\n    returns False\n    '''\n    #Navigational intent for CVE, Errata, id or url\n    return keyword.lower().startswith(('cve', '(cve', 'rhsa', '(rhsa', 'rhba', '(rhba')) or keyword.isdigit() or is_url(keyword)\n\ndef has_other_intent(query: str) -> bool:\n    return has_known_lexical_pattern(query)\n\ndef invalid_input():\n    \"\"\"Sends a 400 Bad Request\"\"\"\n    return Response(\n    'ensure this value has at mininum 3 characters and most 500 characters', 400,\n    {})\n\ndef validate_input (f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        try:\n            query = request.args ['query']\n            if len (query) < 3 or len (query) > 500:\n                raise Exception ()\n            return f(*args, **kwargs)\n        except:\n            logging.info(\"Input validation failed\")\n            return invalid_input ()\n    return decorated\nCode Sample for predict.py Intent Prediction\nfrom flask import Flask, jsonify, request, Response\nfrom functools import wraps\nfrom util import validate_input, has_other_intent\nimport json, pickle, datetime, logging, uuid\nfrom typing import Dict\n\napplication = Flask(__name__)\n\ndef load_model() -> None:\n    '''\n    Initialize the global variables to load the model.\n    '''\n    global vectorizer, model\n    vectorizer = pickle.load(open(\"./models/tfidf_vectorizer.pkl\", \"rb\")) \n    model = pickle.load(open(\"./models/intent_clf.pkl\", \"rb\"))     \n\ndef initialize ():\n    load_model()\n\ndef strip_accept_label(prediction:str) -> str:\n    if '-' in prediction:\n        return prediction.split('-')[0]\n    return prediction\n\n@application.route(\"/introspect\", methods = [\"GET\"])\n@validate_input\ndef introspect() -> Dict:\n    \"\"\"\n    Intent Prediction\n    \"\"\"\n    query = request.args ['query']\n    \n    # Return intent as OTHER for CVE, Errata (RHBA, RHSA), id or url\n    if has_other_intent(query):\n        response = {\"query\": query, \"intent\": 'OTHER', \"confidence_score\": 1, \n            \"req_id\": str(uuid.uuid4())}\n        logging.info(f\"Prediction response : {response}\")\n        return response\n \n    query_transformed = vectorizer.transform([query])\n    prediction = model.predict([query_transformed.toarray()[0]])[0]\n\n    prob = model.predict_proba([query_transformed.toarray()[0]])[0]\n    confidence_score = round(max(prob), 2)\n\n    response = {\"query\": query, \"intent\": strip_accept_label(prediction), \"confidence_score\": confidence_score, \n            \"req_id\": str(uuid.uuid4())}\n    \n    return response\n\ninitialize()\n\nif __name__ == \"__main__\":\n    initialize()\n    application.run(debug = True, host = \"0.0.0.0\", port = \"8080\")\nDockerfile sample\nFROM registry.access.redhat.com/rhscl/python-36-rhel7\n\nUSER root\n\nADD . /opt/customer-portal-search-intent/\n\nWORKDIR /opt/customer-portal-search-intent\n\nRUN wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/intent_clf-version1.pkl \\\n    && wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/intent_clf.pkl \\ \n    && wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/tfidf_vectorizer-version1.pkl \\ \n    && wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/tfidf_vectorizer.pkl \\ \n    && mv *.pkl models/ \\\n    && pip install -r requirements.txt\n\nUSER 1001\n\nEXPOSE 8080 8443\n\nENTRYPOINT [\"./run_intent_detection_service\"]\n\nCMD [ ]\n\nrun_intent_detection_service\n#!/bin/bash\n\n# Lay down the cert for this server\nKEYFILE=cert/rsa.key\nCERTFILE=cert/ssl.crt\nBIND=\"127.0.0.1:8080\"\n\nif [ -f $KEYFILE ] && [ -f $CERTFILE ] ; then\n   OPTS=\"--keyfile $KEYFILE --certfile $CERTFILE $OPTS\"\n   BIND=\"0.0.0.0:8443\"\nfi\n\n# num_workers = (2 * cpu) + 1 => 9 \nOPTS=\"$OPTS -b $BIND --workers 5 --log-level=DEBUG \"\n\nexport REQUESTS_CA_BUNDLE=$(pwd)/root.crt\n\nset -x\ngunicorn main:application $OPTS --access-logfile - --access-logformat  \"{'remote_ip':'%(h)s','request_id':'%({X-Request-Id}i)s','response_code':'%(s)s','request_method':'%(m)s','request_path':'%(U)s','request_querystring':'%(q)s','request_timetaken':'%(D)s','response_length':'%(B)s'}\"\n\n\nBuilding Image and Deployment on OpenShift\nIn order to make the service available to our users we are going to lean on OpenShift, container platform for building the image and deployment.\nOnce we have trained our model and have the application packaged in a Dockerfile, all we need to provide is BuildTemplate and DeploymentTemplate.\nBuildTemplate provides information such as source repository where the Dockerfile(assumed to be at the root), what buildstrategy to use(in this docker strategy) and finally about the ImageRepository for storing the built images.\nDeploymentTemplate contains info about cpu, memory requirements on the deployments, number of pods(instances of the services) for availablility and seamlessly transition between the instances during deployment rollouts without service interruption."
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#integration",
    "href": "posts/2020-09-20-intent-detection.html#integration",
    "title": "Intent detection",
    "section": "Integration",
    "text": "Integration\nLet’s see how this model is integrated with the overall product ecosystem.As a first step in using the service, we wanted to avoid any risks and carefully provide the options to the user to choose troubleshoot experience when we detect the same intent from the query.\nIn the below example user searching for kernel panic occuring in Red Hat Linux systems, the service predicted the TROUBLESHOOT with greater 70% confidence and a banner showing option to user allowing them to choose TROUBLESHOOT experience.\n\n\n\nRed Hat Customer Portal Search Integration"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#conclusion",
    "href": "posts/2020-09-20-intent-detection.html#conclusion",
    "title": "Intent detection",
    "section": "Conclusion",
    "text": "Conclusion\n\nIn this post, we covered how we went from ideation, data collection, module building, deployment and finally integrating with the product.\nThere are constraints such as shorter text, latency, model size in choosing a modeling technique for intent classification and creating a simpler & traditional model such as LinearSVC can always be a better fit for such scenarios."
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#references",
    "href": "posts/2020-09-20-intent-detection.html#references",
    "title": "Intent detection",
    "section": "References",
    "text": "References\n\nSearch: Intent, Not Inventory\nBroder, Andrei. (2002). A Taxonomy of Web Search. SIGIR Forum. 36. 3-10. 10.1145/792550.792552\nSondhi, Parikshit & Sharma, Mohit & Kolari, Pranam & Zhai, ChengXiang. (2018). A Taxonomy of Queries for E-commerce Search. 1245-1248. 10.1145/3209978.3210152\nDeep Search Query Intent Understanding\nDiscovering and Classifying In-app Message Intent at Airbnb\nProdigy - ML teaching and annotation tool\nSVM\nActive Learning\nOpenShift Builds 10.OpenShift Deployments"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html",
    "href": "posts/2023-04-14-fashion-mnist.html",
    "title": "Image Classification with fastai’s MidLevel API",
    "section": "",
    "text": "In this blog post, we will explore a toy classification example using the Fashion MNIST dataset. We will discuss the limitations of the fastai DataBlock API and how learning about the Mid-level API can help overcome these limitations using Transforms, Pipeline, and Datasets. We will also cover the debugging steps involved in creating the Dataloaders for training image classifier & deploy the app using Gradio & HuggingFace spaces.\nSee the complete app Code & Space"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#context",
    "href": "posts/2023-04-14-fashion-mnist.html#context",
    "title": "Image Classification with fastai’s MidLevel API",
    "section": "Context",
    "text": "Context\nWhen working with the fastai library, we may encounter situations where the Data block API is not flexible enough to handle our specific data processing needs. In such cases, we can use fastai’s mid-level API, which provides a more comprehensive set of tools for processing data. The mid-level API includes a range of features such as Transforms, Pipeline, TfmdLists, Datasets, Callbacks, and General Optimizer. By using these, we can overcome the limitations of the Data block API and create more customized data processing pipelines to suit our specific use case.\nTo read more about Mid-Level API, please refer Chapter 11 - Data Munging with fastai’s Mid-Level API in fastbook.\n\n\n\nfastai - a Layered API\n\n\nIn this post, I wanted to share a toy classification example using Fashion MNIST dataset where the DataBlock API is not flexibile and how learning about Mid-level API using Transforms, Pipeline and Datasets helped to create the Dataloaders for training the model. We will use the model to create a image classifier predicting the target class given an black & white image of a fashion apparel using Gradio and HuggingFace spaces. We will also cover the debugging steps involved at the relevant step.\nThe end to end workflow is as follows"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#exploring-the-dataset",
    "href": "posts/2023-04-14-fashion-mnist.html#exploring-the-dataset",
    "title": "Image Classification with fastai’s MidLevel API",
    "section": "Exploring the Dataset",
    "text": "Exploring the Dataset\n\nDownload and load dataset with the name fashion_mnist from https://huggingface.co/datasets/fashion_mnist using HuggingFace datasets library.\n\n\nFashion-MNIST is a dataset of Zalando’s article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 apparels such as t-shirt, ankle boot,\n\n\nExplore the dataset using load_dataset_builder inspecting the dataset info such as description, features, splits etc\n\n\nname='fashion_mnist'\nds_builder = load_dataset_builder(name)\n\n\nprint(ds_builder.info.description)\n\nFashion-MNIST is a dataset of Zalando's article images—consisting of a training set of\n60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\nassociated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\nreplacement for the original MNIST dataset for benchmarking machine learning algorithms.\nIt shares the same image size and structure of training and testing splits.\n\n\n\n\nprint(ds_builder.info.features)\n\n{'image': Image(decode=True, id=None), 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\n\nprint(ds_builder.info.splits)\n\n{'train': SplitInfo(name='train', num_bytes=31296655, num_examples=60000, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=5233818, num_examples=10000, shard_lengths=None, dataset_name=None)}\n\n\n\nds_builder.info.features['label']\n\nClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)\n\n\n\nds_builder.info.features['label'].int2str(9)\n\n'Ankle boot'\n\n\n\nLoad the dataset from the Hugging Face Hub specifying the name.\n\n\ndset = load_dataset(name)\n\nThis is a DatasetDict containing a train and test dataset dictionary within this object.\n\ndset\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\n\nWe can inspect the individual item within train and test and the different labels.\n\n\ndset['train'][0], dset['test'][0]\n\n({'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n  'label': 9},\n {'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n  'label': 9})"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#create-fastai-dataloaders",
    "href": "posts/2023-04-14-fashion-mnist.html#create-fastai-dataloaders",
    "title": "Image Classification with fastai’s MidLevel API",
    "section": "Create fastai DataLoaders",
    "text": "Create fastai DataLoaders\nWe eventually want to create Datasets object in fastai from HuggingFace Dataset. We will first attempt with the the high level DataBlock API and then transition to fastai Datasets.\n\nDataBlock is a high level API in fastai allowing the user to define the standard steps to prepare the data for deep learning model.\n\nSteps involved to prepare the data - Identify the types of inputs/targets for your data and define them as “Blocks”. - Specify how to fetch and define any transformations that need to be applied to the inputs using the “get_x” function. - Specify how to fetch and define any transformations that need to be applied to the targets using the “get_y” function. - Split the data into training and validation sets using the “splitter” function. - Apply any additional transformations to the items using the “item_tfms” function. - Apply any additional transformations to the batches using the “batch_tfms” function.\nLet’s create training and test sets.\n\ntrain, test = dset['train'], dset['test']\n\nCreate an Image from argument using PILImageBW.create function\n\nim = PILImageBW.create(train[0]['image'])\n\nDisplay the image using show method\n\nim.show()\n\n<AxesSubplot:>\n\n\n\n\n\nLet’s examine the type of the features (ie Image) and label\n\ntype(train[0]['image'])\n\nPIL.PngImagePlugin.PngImageFile\n\n\nLet’s examine the first 3 training samples\n\ntrain[:3]\n\n{'image': [<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>],\n 'label': [9, 0, 0]}\n\n\nThe type of label is an int but since fastai also performs Categorize transform we can create a separate target which contains the original apparel name. This can be achieved using ClassLabel.int2str method.\n\nclassLabel = ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)\n\ndef add_target(x:dict):\n    x['target'] = classLabel.int2str(x['label'])\n    return x\n\ntrain = train.map(lambda x: add_target(x))\nvalid = test.map(lambda x: add_target(x))\n\n\n\n\n\n\n\n\ntrain[:3], valid[:3]\n\n({'image': [<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n   <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n   <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>],\n  'label': [9, 0, 0],\n  'target': ['Ankle boot', 'T - shirt / top', 'T - shirt / top']},\n {'image': [<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n   <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n   <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>],\n  'label': [9, 2, 1],\n  'target': ['Ankle boot', 'Pullover', 'Trouser']})\n\n\nNow we can concatenate training and validation datasets into a single set of items which can be passed to fastai Datasets with index from 60000 to 70000 set aside as validation set.\n\nconcat_dsets = concatenate_datasets([train, valid])\n\n\nconcat_dsets\n\nDataset({\n    features: ['image', 'label', 'target'],\n    num_rows: 70000\n})\n\n\n\nAttempting to use DataBlock\nLet’s first create a DataBlock and then learn how to create Datasets. In order to inform fastai on how to turn the data into DataLoaders object, 4 key pieces of info are needed. 1. the kind of data used for inputs and the target 2. Getters for the list of items 3. Labeling the items 4. Validation set creation\n\n\n\n\n\n\nTip\n\n\n\nIt’s best to create DataBlock in an iterative manner and running datablock.summary to understand the pieces that fastai adds behind the scenes.\n\n\n\ndef get_image_attr(x): return x['image']\ndef get_target_attr(x): return x['target']\n\n\nImage.fromarray(array(train[0]['image']))\n\n\n\n\n\ndef image2tensor(img):\n    \"Transform image to byte tensor in `c*h*w` dim order.\"\n    res = tensor(img)\n    if res.dim()==2: res = res.unsqueeze(-1)\n    return res.permute(2,0,1)\n\n\ntype(image2tensor(train[0]['image']))\n\ntorch.Tensor\n\n\nWe will discuss image2tensor function when we discuss Attempting with Datasets\nThis shows how to specify the indices that need to be part of validation set. Indices 6, 7, 8 & 9 are in validation and rest are in training set.\n\nIndexSplitter(valid_idx=L(range(6, 10)))(concat_dsets)\n\n((#69996) [0,1,2,3,4,5,10,11,12,13...], (#4) [6,7,8,9])\n\n\nDataBlock definition is as follows\n\ndblock = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),\n                   get_x=Pipeline([get_image_attr, image2tensor]),\n                   get_y=get_target_attr,\n                   splitter=IndexSplitter(valid_idx=L(range(60000, 70000))),\n                   )\n\nRun the DataBlock.summary to understand how fastai set up the data pipeline and perform the necessary transforms.\n\n#Output cleared\n#dblock.summary(concat_dsets)\n\nTraceback while running dblock.summary(concat_dsets)\nSetting-up type transforms pipelines\nCollecting items from Dataset({\n    features: ['image', 'label', 'target'],\n    num_rows: 70000\n})\nFound 70000 items\n2 datasets of sizes 60000,10000\nSetting up Pipeline: get_image_attr -> image2tensor -> PILBase.create\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n~/miniconda3/envs/fastai/lib/python3.9/site-packages/PIL/Image.py in fromarray(obj, mode)\n   2812         try:\n-> 2813             mode, rawmode = _fromarray_typemap[typekey]\n   2814         except KeyError as e:\n\nKeyError: ((1, 1, 28), '|u1')\n\nEssentially the data pipeline is as follows - Get the image attribute from the item, Convert the image to tensor. But fastai also adds the PILBase.create since we specified ImageBlock as our independent variable. This caused an issue KeyError: ((1, 1, 28), '|u1') due to Image.fromarray function used in PILBase.create. From https://stackoverflow.com/questions/57621092/keyerror-1-1-1280-u1-while-using-pils-image-fromarray-pil > Pillow’s fromarray function can only do a MxNx3 array (RGB image), or an MxN array (grayscale).\n\nInternally fastai calls load_image with the item during the data pipeline creation as part of PILBase.create which fastai adds by default if we specify ImageBlock as part of blocks section in the DataBlock.\n\n\ndef load_image(fn, mode=None):\n    \"Open and load a `PIL.Image` and convert to `mode`\"\n    im = Image.open(fn)\n    im.load()\n    im = im._new(im.im)\n    return im.convert(mode) if mode else im\n\nload_image requires a filename but dataset already is in PIL.PngImagePlugin.PngImageFile\nImage.open(train[0]['image']) will lead to the following error from pillow library.\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-18-ed0e9de325b8> in <module>\n----> 1 Image.open(train[0]['image'])\n\n1 frames\n/usr/local/lib/python3.8/dist-packages/PIL/Image.py in __getattr__(self, name)\n    544             )\n    545             return self._category\n--> 546         raise AttributeError(name)\n    547 \n    548     @property\n\n\n\nAttempt with MidLevel API Datasets\nWe want to convert the Image of size [28 ,28] into [1, 28, 28] as our end goal and decided to perform the item transforms using Mid Level API instead. - Datasets in MidLevel API provides more flexibility and full control over the individual item transforms performed.\n\nDatasets is part of MidLevel API allowing the user to customize the steps involved in data processing that are not possible with DataBlockAPI\n\nDatasets need the following pieces of information - raw items - the list of transforms that builds our inputs from the raw items - the list of transforms that builds our targets from the raw items - the split for training and validation\nFor a deeper dive, refer loading the data with mid level api section on Training Imagenette tutorial and Wayde Gilliam blog post\nLet’s iterate on the individual pieces of info.\nLet’s investigate the item transforms that we need for the Image. PyTorch Model expects the items to of type torch.Tensor. So I used ToTensor but it did not convert to tensors as I expected for the Image of type PIL.PngImagePlugin.PngImageFile. So I created img2tensor instead.\n\ntype(train[0]['image'])\n\nPIL.PngImagePlugin.PngImageFile\n\n\n\ntype(ToTensor()(train[0]['image']))\n\nPIL.PngImagePlugin.PngImageFile\n\n\nReferred what fastai does underneath using the source code. This is the function that takes the image and converts the image to a byte tensor of shape c*h*w ie channel x height x width\n\ndef image2tensor(img):\n    \"Transform image to byte tensor in `c*h*w` dim order.\"\n    res = tensor(img)\n    if res.dim()==2: res = res.unsqueeze(-1)\n    return res.permute(2,0,1)\n\n\ntype(image2tensor(train[0]['image']))\n\ntorch.Tensor\n\n\nI created my own version of it in img2tensor\n\ndef img2tensor(im: Image.Image):\n    return TensorImageBW(array(im)).unsqueeze(0)\n\n\nimg2tensor(train[0]['image']).shape\n\ntorch.Size([1, 28, 28])\n\n\n\ntensor(train[0]['image']).unsqueeze(-1).permute(2,0,1).shape\n\ntorch.Size([1, 28, 28])\n\n\n\nTensorImageBW(array(im)).unsqueeze(0).shape\n\ntorch.Size([1, 28, 28])\n\n\n\ntest_eq([1, 28, 28], img2tensor(train[0]['image']).shape)\ntest_eq([1, 28, 28], image2tensor(train[0]['image']).shape)\n\nAs you can see both image2tensor and img2tensor behaves the same way.Now the input item is ready. Let’s look at the target.\n\nconcat_dsets\n\nDataset({\n    features: ['image', 'label', 'target'],\n    num_rows: 70000\n})\n\n\nfasti internally converts the label and encodes them into numbers by creating a vocabulary of labels using Categorize transform.\n\nCategorize(vocab=ds_builder.info.features['label'].names, sort=False)\n\nCategorize -- {'vocab': ['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], 'sort': False, 'add_na': False}:\nencodes: (Tabular,object) -> encodes\n(object,object) -> encodes\ndecodes: (Tabular,object) -> decodes\n(object,object) -> decodes\n\n\nCreate the Datasets definition as follows\n\nsplits = IndexSplitter(valid_idx=L(range(60000, 70000)))(concat_dsets)\ndsets = Datasets(concat_dsets, \n                 [[get_image_attr], \n                  [get_target_attr, Categorize]],\n                 splits=splits)\n\nDefine the item transformations and batch transformations\n\nitem_tfms = [img2tensor] # convert PILImage to tensors\nbatch_tfms = [IntToFloatTensor] # convert the int tensors from images to floats, and divide every pixel by 255\ndls = dsets.dataloaders(after_item=item_tfms, after_batch=batch_tfms, bs=64, num_workers=8)\n\nVisualize the items in a batch\n\ndls.show_batch()\n\n\n\n\nVerify the items and their shapes in a single batch\n\nxb, yb = dls.one_batch()\n\n\nxb.shape, yb.shape\n\n(torch.Size([64, 1, 28, 28]), torch.Size([64]))\n\n\n\ndls.c # 10 classes as targets\n\n10\n\n\n\ndls.vocab # targets vocabulary\n\n['Ankle boot', 'Bag', 'Coat', 'Dress', 'Pullover', 'Sandal', 'Shirt', 'Sneaker', 'T - shirt / top', 'Trouser']"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#train-the-model",
    "href": "posts/2023-04-14-fashion-mnist.html#train-the-model",
    "title": "Image Classification with fastai’s MidLevel API",
    "section": "Train the Model",
    "text": "Train the Model\nFor training a image classification model, resnet architecture (a form on convolution neural network) is used as our backbone and fully connected (fc) linear layer as our head. In order for the linear layer to predict the outputs as one of the classes, pass the number of classes in order to configure the final layer. For overview review this resource about transfer learning and fine tuning.\nReview the model layers\n\n#gpu required\nmodel = resnet34(num_classes=dls.c).cuda()\n\n\n#gpu required\nmodel.avgpool, model.fc\n\n\n# Uncomment this line to validate if the model accepts single batch as input\n# model(xb)\n\nWe can access the convolutional layers as attributes of the model.\nInput to the convolutional layer is set as 3 channel (RGB) image but the images used as inputs are single channel images. Let’s update the input channel to single channel.\n\n#gpu required\nmodel.conv1\n\nConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n\n#gpu required\nmodel.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n#gpu required\nmodel.cuda();\n\n\n#gpu required\nmodel(xb);\n\n\nLearner\nLearner is a class that combines the ingredients such as data, model and metrics used to train a model.\n\nlearn = Learner(dls, model, metrics=accuracy)\n\npretrained=False type setting is used since the fashnion mnist is not similar dataset as Imagenet so keeping all the layers are trainable. If in case pretrained=True, we may want to freeze the layers except the head and do a bit of fine tuning the head first followed by unfreeze & then train all the layers.\nReview the learner summary to know about the input shape, output shape, different layers involved, parameters, trainable, Optimizer used and loss function used.\n\nlearn.summary()\n\n\n\n\n\n\n\n\nResNet (Input shape: 64 x 1 x 28 x 28)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     64 x 64 x 14 x 14   \nConv2d                                    3136       True      \nBatchNorm2d                               128        True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 64 x 7 x 7     \nMaxPool2d                                                      \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \n____________________________________________________________________________\n                     64 x 128 x 4 x 4    \nConv2d                                    73728      True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nConv2d                                    8192       True      \nBatchNorm2d                               256        True      \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \n____________________________________________________________________________\n                     64 x 256 x 2 x 2    \nConv2d                                    294912     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    32768      True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \n____________________________________________________________________________\n                     64 x 512 x 1 x 1    \nConv2d                                    1179648    True      \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nConv2d                                    131072     True      \nBatchNorm2d                               1024       True      \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nAdaptiveAvgPool2d                                              \n____________________________________________________________________________\n                     64 x 10             \nLinear                                    5130       True      \n____________________________________________________________________________\n\nTotal params: 21,283,530\nTotal trainable params: 21,283,530\nTotal non-trainable params: 0\n\nOptimizer used: <function Adam>\nLoss function: FlattenedLoss of CrossEntropyLoss()\n\nCallbacks:\n  - TrainEvalCallback\n  - CastToTensor\n  - Recorder\n  - ProgressCallback\n\n\nLearning rate, a hyperparameter used for training can be determined using learning rate finder function in fastai\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0004786300996784121)\n\n\n\n\n\nTrain the deep learning model with 6 epochs and with the learning rate obtained from previous step\n\nlearn.fit_one_cycle(6, 5e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.350417\n      0.380862\n      0.860200\n      01:17\n    \n    \n      1\n      0.350359\n      0.350519\n      0.867500\n      01:18\n    \n    \n      2\n      0.266588\n      0.297127\n      0.889900\n      01:19\n    \n    \n      3\n      0.215826\n      0.248561\n      0.909700\n      01:16\n    \n    \n      4\n      0.173092\n      0.230798\n      0.918000\n      01:18\n    \n    \n      5\n      0.139866\n      0.226217\n      0.922900\n      01:17\n    \n  \n\n\n\nModel is trained with an accuracy of 92.3%.\n\n\nExporting the trained model\n\n\n\n\n\n\nTip\n\n\n\nAvoid using lambda as Getters during data processing in order to export the model correctly.\n\n\nlearn.export(fname=‘export.pkl’) will lead to following Pickling error if lambda is used in the Data pipeline\ndsets = Datasets(concat_dsets, [[lambda x: x['image']], [lambda x: x['label'], Categorize]], splits=splits)\nPicklingError: Can’t pickle <function  at 0x7f378c5aeaf0>: attribute lookup  on main failed\n\n::: {.cell vscode='{\"languageId\":\"python\"}'}\n``` {.python .cell-code}\nlearn.export(fname='export.pkl')\n:::"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#inference",
    "href": "posts/2023-04-14-fashion-mnist.html#inference",
    "title": "Image Classification with fastai’s MidLevel API",
    "section": "Inference",
    "text": "Inference\nNow load the learner from the exported model. Predict the item ie image using Learner.predict. This function performs the necessary transforms on the item used as part of training using the learner.\n\nitem, expected = valid[0]['image'], valid[0]['target']; item, expected\n\n(<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>, 'Ankle boot')\n\n\n\nlearn.predict(item)\n\n\n\n\n\n\n\n\n('9',\n tensor(9),\n tensor([5.0262e-13, 4.2399e-12, 1.5671e-10, 1.6854e-12, 4.1097e-14, 1.6369e-06,\n         1.2831e-10, 1.0092e-05, 1.8003e-10, 9.9999e-01]))\n\n\n\nlearn.dls.after_item, learn.dls.after_batch\n\n(Pipeline: img2tensor,\n Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1})\n\n\n\nRecreating Learner.predict from the source code\nWe can run learn.predict?? to examine what fastai does and review each line carefully.\ndef predict(self, item, rm_type_tfms=None, with_input=False):\n        dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)\n        inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n        i = getattr(self.dls, 'n_inp', -1)\n        inp = (inp,) if i==1 else tuplify(inp)\n        dec = self.dls.decode_batch(inp + tuplify(dec_preds))[0]\n        dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])\n        res = dec_targ,dec_preds[0],preds[0]\n        if with_input: res = (dec_inp,) + res\n        return res\nConverting the item by applying the transforms and create a dataloader out of it.\n\ndl = learn.dls.test_dl([item]); dl\n\n<fastai.data.core.TfmdDL>\n\n\nInternally learn.predict calls the get_preds method which accepts data loader and returns the input, predictions, the decoded predictions. This applies the same transforms done during training applied on the input during inference.\n\ninp, preds, _, decoded_preds = learn.get_preds(dl=dl, with_input=True, with_decoded=True)\n\n\n\n\n\n\n\n\nExamine the input shape and type\n\ninp.shape, type(inp)\n\n(torch.Size([1, 1, 28, 28]), fastai.torch_core.TensorImageBW)\n\n\n\ntuplify(decoded_preds)\n\n(tensor([9]),)\n\n\n\ntype(learn.dls.decode_batch((inp,) + tuplify(decoded_preds))), len(learn.dls.decode_batch((inp,) + tuplify(decoded_preds)))\n\n(fastcore.foundation.L, 1)\n\n\n\nimage, prediction = learn.dls.decode_batch((inp,) + tuplify(decoded_preds))[0]; prediction\n\n'9'\n\n\n\nprediction, decoded_preds\n\n('9', tensor([9]))\n\n\nLet’s examine fastai transforms done after item and after batch.\n\nlearn.dls.after_item, learn.dls.after_batch\n\n(Pipeline: img2tensor,\n Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1})\n\n\n\ntype_tfms = Pipeline([get_image_attr])\nitem_tfms = Pipeline([img2tensor])\nbatch_tfms = Pipeline([IntToFloatTensor])\n\n\ntrain[0]\n\n{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n 'label': 9,\n 'is_valid': False}\n\n\n\nimg = type_tfms(train[0]);img.shape\n\n(28, 28)\n\n\n\nitem_tfms(img).shape\n\ntorch.Size([1, 28, 28])\n\n\n\nbatch_tfms(item_tfms(img).cuda()).shape\n\ntorch.Size([1, 28, 28])\n\n\nFrom the previous steps, we can uncover fastai magic such as transforms happening behind the scenes."
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#gradio-app-deployment",
    "href": "posts/2023-04-14-fashion-mnist.html#gradio-app-deployment",
    "title": "Image Classification with fastai’s MidLevel API",
    "section": "Gradio app deployment",
    "text": "Gradio app deployment\nLet’s deploy the trained model and inference functionality in Gradio app and host the app in HuggingFace Space.\nSteps followed - Create a new space in HF Space (Profile -> New Space) - Upload the exported model export.pkl - Move all the necessary functions used as part of the transforms along with the inference provided below. This includes all the getters. - Add all the dependencies to requirements.txt - Create a gradio interface passing the classify function, specifying the inputs(Image) and outputs(Label) - See the complete Code & Space\n\nfrom fastai.vision.core import PILImageBW, TensorImageBW\nfrom datasets import ClassLabel\nimport gradio as gr\nfrom fastai.learner import load_learner\n\ndef get_image_attr(x): return x['image']\ndef get_target_attr(x): return x['target']\n\ndef img2tensor(im: Image.Image):\n    return TensorImageBW(array(im)).unsqueeze(0)\n\nclassLabel = ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)\n\ndef add_target(x:dict):\n    x['target'] = classLabel.int2str(x['label'])\n    return x\n\nlearn = load_learner('export.pkl', cpu=True)\n\ndef classify(inp):\n    img = PILImageBW.create(inp)\n    item = dict(image=img)\n    pred, _, _ = learn.predict(item)\n    return classLabel.int2str(int(pred))\n\n\nfname='shoes.jpg'\nclassify(fname)\n\n\n\n\n\n\n\n\n'Ankle boot'\n\n\n\nfname1 = 't-shirt.jpg'\nclassify(fname1)\n\n\n\n\n\n\n\n\n'T - shirt / top'\n\n\nNote: This is my project write up for WalkWithFastai revisited course as one of my goal for this course is to get comfortable with low level API, debug issues diving into the source, uncovering the fastai magic. Thanks to Zach Mueller for an excellent course."
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#references",
    "href": "posts/2023-04-14-fashion-mnist.html#references",
    "title": "Image Classification with fastai’s MidLevel API",
    "section": "References",
    "text": "References\n\nhttps://store.walkwithfastai.com/walk-with-fastai-revisited\nhttps://walkwithfastai.com/MNIST\nhttps://github.com/fastai/fastbook/blob/master/11_midlevel_data.ipynb"
  },
  {
    "objectID": "posts/2021-01-09-poetry-first-impressions.html",
    "href": "posts/2021-01-09-poetry-first-impressions.html",
    "title": "Poetry First Impressions",
    "section": "",
    "text": "Recently I inherited a python project which did not have dependency management setup and this post is a summary of my investigation using Poetry.\n\n\n\nDifferent python versions used by developers and in production.\nsetup.py not maintained\nDifferentiating the dev dependencies vs production dependencies is hard with a single requirements.txt file\nTracking the transitive dependencies.\nLocking the dependencies when we are ready to push the code.\n\n\n\n\n\nReproducibility and Consistency are the key aspects for maintaining the project.\nManaging dependencies in a python project and relying on a single tool (eg: pip, conda)\n\n\n\n\nPoetry\n\n\nPoetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you.\n\n\nGetting Started on Linux\n\n$ curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - - To start a new project poetry new <project-name>. If you have an existing project then poetry init - guide you through setting a project specific pyproject.toml config.\n\nPackage, version, description, author, compatible versions\nMain Dependencies\nDevelopment Dependencies\n\n\nFormat of pyproject.toml config\n[tool.poetry]\nname = \"language-detection\"\nversion = \"0.1.0\"\ndescription = \"Detects the language of the provided text using fasttext.\"\nauthors = [\"Your Name <you@example.com>\"]\n[tool.poetry.dependencies]\npython = \"^3.6\"\nfasttext = \"0.9.1\"\nFlask = \"1.1.1\"\ngunicorn = \"20.0.4\"\n[tool.poetry.dev-dependencies]\ncodecov = \"2.1.7\"\nflake8 = \"^3.8.4\"\nblack = \"^20.8b1\"\npytest = \"^6.2.1\"\npytest-cov = \"^2.10.1\"\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\nManage virtual environments - Poetry automatically creates a virtual environments for you based on the project when you ran new or init. You can know more info using the following commands & use either poetry shell or source <cache_dir>/pypoetry/virtualenvs/<project_venv>/bin/activate to activate the environment. deactivate if you want to exit the environment.\n\n$  poetry env info\n\nVirtualenv\nPython:         3.8.6\nImplementation: CPython\nPath:           /home/msivanes/.cache/pypoetry/virtualenvs/language-detection-ddSi-Wir-py3.8\nValid:          True\n\nSystem\nPlatform: linux\nOS:       posix\nPython:   /usr\n\n$ poetry config --list\ncache-dir = \"/home/msivanes/.cache/pypoetry\"\nexperimental.new-installer = true\ninstaller.parallel = true\nvirtualenvs.create = true\nvirtualenvs.in-project = null\nvirtualenvs.path = \"{cache-dir}/virtualenvs\"  # /home/msivanes/.cache/pypoetry/virtualenvs\n\n\nAdd dependency through poetry add <package> or poetry add <package>@<version>\n$ poetry add Flask\n$ poetry add fasttext@0.9.1\n$ poetry add pytest\nRemove a dependency using poetry remove <package>\n$ poetry remove pyfasttext\nOnce you finish adding the dependencies, run poetry install to install all the dependencies in your project. This will generate the poetry.lock file. This is where all the dependencies are locked and needs to be version controlled. If any other developer run poetry install then poetry uses the exact same versions specified in the dependencies while installing. This ensures all the developers are using a consistent dependencies across the board.\nIf you want to upgrade the dependencies and use the latest versions, then use poetry update. This is the equivalent of deleting the poetry.lock and using install .\nExporting the requirements.txt if you want to containarize your app.\n$ poetry export --without-hashes -f requirements.txt > requirements.txt\nFor scenarios in libraries where you want to perform an editable install and hence you need a setup.py file. (Github user @albireox provided a script to quickly create setup.py which I found it here\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# @Author: José Sánchez-Gallego (gallegoj@uw.edu)\n# @Date: 2019-12-18\n# @Filename: create_setup.py\n# @License: BSD 3-clause (http://www.opensource.org/licenses/BSD-3-Clause)\n\n# This is a temporary solution for the fact that pip install . fails with\n# poetry when there is no setup.py and an extension needs to be compiled.\n# See https://github.com/python-poetry/poetry/issues/1516. Running this\n# script creates a setup.py filled out with information generated by\n# poetry when parsing the pyproject.toml.\n\nimport os\nimport sys\nfrom distutils.version import StrictVersion\n\n\n# If there is a global installation of poetry, prefer that.\nlib = os.path.expanduser('~/.poetry/lib')\nvendors = os.path.join(lib, 'poetry', '_vendor')\ncurrent_vendors = os.path.join(\n    vendors, 'py{}'.format('.'.join(str(v) for v in sys.version_info[:2]))\n)\n\nsys.path.insert(0, lib)\nsys.path.insert(0, current_vendors)\n\ntry:\n    try:\n        from poetry.core.factory import Factory\n        from poetry.core.masonry.builders.sdist import SdistBuilder\n    except (ImportError, ModuleNotFoundError):\n        from poetry.masonry.builders.sdist import SdistBuilder\n        from poetry.factory import Factory\n    from poetry.__version__ import __version__\nexcept (ImportError, ModuleNotFoundError) as ee:\n    raise ImportError('install poetry by doing pip install poetry to use '\n                      f'this script: {ee}')\n\n\n# Generate a Poetry object that knows about the metadata in pyproject.toml\nfactory = Factory()\npoetry = factory.create_poetry(os.path.dirname(__file__))\n\n# Use the SdistBuilder to genrate a blob for setup.py\nif StrictVersion(__version__) >= StrictVersion('1.1.0b1'):\n    sdist_builder = SdistBuilder(poetry, None)\nelse:\n    sdist_builder = SdistBuilder(poetry, None, None)\n\nsetuppy_blob = sdist_builder.build_setup()\n\nwith open('setup.py', 'wb') as unit:\n    unit.write(setuppy_blob)\n    unit.write(b'\\n# This setup.py was autogenerated using poetry.\\n')\n\n\n\n\n\nI still have issues with pyenv which I need to figure how to easily switch between python versions\n\n\n\n\n\nPoetry Documentation"
  },
  {
    "objectID": "posts/2020-06-29-tracking-data-model-using-dvc.html",
    "href": "posts/2020-06-29-tracking-data-model-using-dvc.html",
    "title": "Tracking Data and Model in Machine Learning projects",
    "section": "",
    "text": "Difference between software engineering and machine learning projects\ncode, config, dependencies vs code, data, model, config, dependencies\nMyriad of Data files (raw, intermediate)\nNot in repository\nReproducible ML models targeting metrics [Experiments Link]\nTuning and Experimentation tracking\nModel Deployment & Revert"
  },
  {
    "objectID": "posts/2020-06-29-tracking-data-model-using-dvc.html#dvc",
    "href": "posts/2020-06-29-tracking-data-model-using-dvc.html#dvc",
    "title": "Tracking Data and Model in Machine Learning projects",
    "section": "DVC",
    "text": "DVC\n\nDVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.\nExperiments are tracked by combining the Code + Data files\nData files\n\nLocal cache\nData remotes in S3, SSH etc\n\nMetrics per experiment\nPipelines\n\n\nInstall DVC on Fedora/RHEL/CentOS\nsudo wget https://dvc.org/rpm/dvc.repo -O /etc/yum.repos.d/dvc.repo\nsudo yum update\nsudo yum install dvc\nAdd DVC files in the Git (It is not required but it is good to have project and dvc config in the same directory)"
  },
  {
    "objectID": "posts/2020-06-29-tracking-data-model-using-dvc.html#workflow-for-model-packaging",
    "href": "posts/2020-06-29-tracking-data-model-using-dvc.html#workflow-for-model-packaging",
    "title": "Tracking Data and Model in Machine Learning projects",
    "section": "Workflow for Model Packaging",
    "text": "Workflow for Model Packaging\n\nSetup for connecting to model repository during training development process\nPush the trained model to the repository\nPull the model from the repository during the build process\n\n\nPushing model files\ndvc init\ndvc remote add -d gss-rdu-remote ssh://msivanes@gss-rdu-repo.usersys.redhat.com:/var/www/html/repo/config/ulmfit\n\ndvc add cases_small_sbr_08-06-2020.pkl\ngit commit -am “Add ulmfit model to project”\ndvc push -v\n\n\nPulling the model\ngit clone $REPO\ngit pull\ndvc pull # Pulls the data from remote-storage. Equivalent to dvc fetch followed by dvc checkout\ndvc checkout #Update model files"
  },
  {
    "objectID": "posts/2020-06-29-tracking-data-model-using-dvc.html#food-for-thought",
    "href": "posts/2020-06-29-tracking-data-model-using-dvc.html#food-for-thought",
    "title": "Tracking Data and Model in Machine Learning projects",
    "section": "Food for thought",
    "text": "Food for thought\n\nRather than feature branches, think in terms of experiment branches targeting metrics.\nKeep Data and Model files stored outside the repository\nModel files built as part of build process.\nSmoke test should validate the constructed model using validation set."
  },
  {
    "objectID": "posts/2020-06-29-tracking-data-model-using-dvc.html#references",
    "href": "posts/2020-06-29-tracking-data-model-using-dvc.html#references",
    "title": "Tracking Data and Model in Machine Learning projects",
    "section": "References",
    "text": "References\n\nDVC\nCheatsheet\nhttps://christophergs.github.io/machine%20learning/2019/05/13/first-impressions-of-dvc/\nhttps://www.slideshare.net/DmitryPetrov15/pydata-berlin-2018-dvcorg"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chrestotes",
    "section": "",
    "text": "information-retrieval\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nimage-classification\n\n\nfastai\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nreproducibility\n\n\n\n\nGetting started with poetry for dependency management & packaging.\n\n\n\n\n\n\nJan 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nintent-detection\n\n\ntext-classification\n\n\ninformation-retrieval\n\n\nquery-understanding\n\n\nmachine-learning\n\n\n\n\nBlog post about end to end text classification application for detecting intent in Red Hat Customer Portal search queries using machine learning and integrating the intent detection as part of user journey.\n\n\n\n\n\n\nSep 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmachine-learning\n\n\n\n\nDVC as tool for model and data versioning and create reproducible ML project.\n\n\n\n\n\n\nJun 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninformation-retrieval\n\n\ndeep-learning\n\n\npapers\n\n\n\n\nPaper Summary of covidex.ai\n\n\n\n\n\n\nMay 25, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CHANGES.html",
    "href": "CHANGES.html",
    "title": "chrestotes",
    "section": "",
    "text": "posts is the active directory where content is placed. _ _posts, _notebook are deprecated and leftover from fastpages."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is a medium to teach the past version of myself and serve as a documentation to my future self about things that I’m curious about at a given point in time."
  }
]