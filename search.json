[
  {
    "objectID": "CHANGES.html",
    "href": "CHANGES.html",
    "title": "chrestotes",
    "section": "",
    "text": "Migration from fastpages to quarto\n\nposts is the active directory where content is placed. _ _posts, _notebook are deprecated and leftover from fastpages."
  },
  {
    "objectID": "drafts/zindi_vaccination.html",
    "href": "drafts/zindi_vaccination.html",
    "title": "Outline",
    "section": "",
    "text": "Problem Understanding * Text Regression problem: The task is given the tweet text, we are expected to predict whether the given tweet is positive, negetive or neutral. The target variable value ranges from (-1, 1)"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#setup",
    "href": "drafts/zindi_vaccination.html#setup",
    "title": "Outline",
    "section": "Setup",
    "text": "Setup\n\n! pip install fastai2 -q\n! pip install nbdev -q\n\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 194kB 2.8MB/s \n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 1.7MB/s \n\n\n\nimport fastai2\nprint(fastai2.__version__)\n\n0.0.17"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#download-the-dataset",
    "href": "drafts/zindi_vaccination.html#download-the-dataset",
    "title": "Outline",
    "section": "Download the Dataset",
    "text": "Download the Dataset\n\n! wget \"https://fastaistudygroup.slack.com/files/URA50KQ7R/F0140NHLB8Q/zindi_vaccinate.zip\"\n\n--2020-05-17 16:48:37--  https://fastaistudygroup.slack.com/files/URA50KQ7R/F0140NHLB8Q/zindi_vaccinate.zip\nResolving fastaistudygroup.slack.com (fastaistudygroup.slack.com)... 3.229.174.54, 54.209.135.9, 54.227.211.227\nConnecting to fastaistudygroup.slack.com (fastaistudygroup.slack.com)|3.229.174.54|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://fastaistudygroup.slack.com/?redir=%2Ffiles%2FURA50KQ7R%2FF0140NHLB8Q%2Fzindi_vaccinate.zip [following]\n--2020-05-17 16:48:37--  https://fastaistudygroup.slack.com/?redir=%2Ffiles%2FURA50KQ7R%2FF0140NHLB8Q%2Fzindi_vaccinate.zip\nReusing existing connection to fastaistudygroup.slack.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‚Äòzindi_vaccinate.zip‚Äô\n\nzindi_vaccinate.zip     [ &lt;=&gt;                ]  39.75K  --.-KB/s    in 0.01s   \n\n2020-05-17 16:48:37 (3.13 MB/s) - ‚Äòzindi_vaccinate.zip‚Äô saved [40702]\n\n\n\n\n!pwd\n\n/content\n\n\n\ndata_dir = '/content/drive/My Drive/Colab Notebooks/data'\n\n\nfrom pathlib import Path\n\n\npath = Path(data_dir)\n\n\n# make your Google Drive accessible\nfrom google.colab import drive\ndrive.mount('/content/gdrive', force_remount=True)\n\nroot_dir = \"/content/gdrive/My Drive\"\ndrive_data = f\"{root_dir}/Colab Notebooks/data/vaccination_tweet\"\n\nGo to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n\nEnter your authorization code:\n¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\nMounted at /content/gdrive\n\n\n\nfrom fastai2.text.all import *\n\n\nPath(drive_data).ls()\n\n(#4) [Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/SampleSubmission.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/Train.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/Test.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/models')]\n\n\n\nvaccinate_tweets = Path(drive_data)\n\n\nPath.BASE_PATH = vaccinate_tweets\n\n\nPath(vaccinate_tweets).ls()\n\n(#4) [Path('SampleSubmission.csv'),Path('Train.csv'),Path('Test.csv'),Path('models')]\n\n\n\ntrain_df = pd.read_csv(vaccinate_tweets/'Train.csv')\n\n\ntrain_df.head()\n\n\n\n\n\n\n\n\ntweet_id\nsafe_text\nlabel\nagreement\n\n\n\n\n0\nCL1KWCMY\nMe &amp; The Big Homie meanboy3000 #MEANBOY #MB #MBS #MMR #STEGMANLIFE @ Stegman St. &lt;url&gt;\n0.0\n1.0\n\n\n1\nE3303EME\nI'm 100% thinking of devoting my career to proving autism isn't caused by vaccines due to the IDIOTIC posts I've seen about World Autism Day\n1.0\n1.0\n\n\n2\nM4IVFSMS\n#whatcausesautism VACCINES, DO NOT VACCINATE YOUR CHILD\n-1.0\n1.0\n\n\n3\n1DR6ROZ4\nI mean if they immunize my kid with something that won't secretly kill him years down the line then I'm all for it, but I don't trust that\n-1.0\n1.0\n\n\n4\nJ77ENIIE\nThanks to &lt;user&gt; Catch me performing at La Nuit NYC 1134 1st ave. Show starts at 6! #jennifair #mmr‚Ä¶ &lt;url&gt;\n0.0\n1.0\n\n\n\n\n\n\n\n\nlen(train_df)\n\n10001\n\n\n\ntrain_df.dtypes\n\ntweet_id      object\nsafe_text     object\nlabel        float64\nagreement    float64\ndtype: object\n\n\n\ntest_df = pd.read_csv(vaccinate_tweets/'Test.csv')\n\n\nlen(test_df)\n\n5177\n\n\n\ntest_df.head()\n\n\n\n\n\n\n\n\ntweet_id\nsafe_text\n\n\n\n\n0\n00BHHHP1\n&lt;user&gt; &lt;user&gt; ... &amp; 4 a vaccine given 2 healthy peeps, FDA think just not worth the AE risk unfortunately.\n\n\n1\n00UNMD0E\nStudents starting school without whooping cough vaccinations &lt;url&gt; #scpick\n\n\n2\n01AXPTJF\nI'm kinda over every ep of &lt;user&gt; being \"ripped from the headlines.\" Measles? Let's get back to crime. #SVU\n\n\n3\n01HOEQJW\nHow many innocent children die for lack of vaccination each year? Around 1.5 million. Too bad all their parents couldn't be here. #SB277\n\n\n4\n01JUKMAO\nCDC eyeing bird flu vaccine for humans, though risk is low: Federal officials said Wednesday they're taking steps‚Ä¶ &lt;url&gt;"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#cleaning-the-data",
    "href": "drafts/zindi_vaccination.html#cleaning-the-data",
    "title": "Outline",
    "section": "Cleaning the data",
    "text": "Cleaning the data\n\n# Find the missing values\ntrain_df.isna().sum()\n\ntweet_id     0\nsafe_text    0\nlabel        1\nagreement    2\ndtype: int64\n\n\n\ntest_df.isna().sum()\n\ntweet_id     0\nsafe_text    1\ndtype: int64\n\n\nlabel has 1 missing value and agreement has 2 missing values in train_df and safe_text has 1 missing value in safe_text"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#filter-outfill-the-missing-values-in-train_df-and-test_df",
    "href": "drafts/zindi_vaccination.html#filter-outfill-the-missing-values-in-train_df-and-test_df",
    "title": "Outline",
    "section": "Filter out/Fill the missing values in train_df and test_df",
    "text": "Filter out/Fill the missing values in train_df and test_df\n\n# In case of train dataframe, it's okay to drop the missing value\ntrain_df.dropna(axis=0, inplace=True) # axis=0 \n\n\nlen(train_df)\n\n9999\n\n\nWe have removed the two missing rows due to missing values in label & agreement\nSince we have to submit our predictions on our Test.csv, we cannot remove the missing value. So we are going to fill the missing value with space\n\ntest_df.fillna(value=\" \", axis=0, inplace=True)\n\n\nlen(test_df)\n\n5177"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#create-dataloader-using-datablock-api",
    "href": "drafts/zindi_vaccination.html#create-dataloader-using-datablock-api",
    "title": "Outline",
    "section": "Create DataLoader using DataBlock API",
    "text": "Create DataLoader using DataBlock API\n\ndoc(DataBlock)\n\nclass DataBlock[source]DataBlock(blocks=None, dl_type=None, getters=None, n_inp=None, item_tfms=None, batch_tfms=None, get_items=None, splitter=None, get_y=None, get_x=None)\n\nGeneric container to quickly build Datasets and DataLoaders\nShow in docs\n\n\nimdb = DataBlock(blocks=(TextBlock.from_folder(path), CategoryBlock), get_items=get_text_files, get_y=parent_label, splitter=GrandparentSplitter(valid_name=‚Äòtest‚Äô))\n\ndoc(TextBlock.from_df)\n\nTextBlock.from_df[source]TextBlock.from_df(text_cols, vocab=None, is_lm=False, seq_len=72, min_freq=3, max_vocab=60000, tok_func='SpacyTokenizer', rules=None, sep=' ', n_workers=2, mark_fields=None, res_col_name='text', **kwargs)\n\nBuild a TextBlock from a dataframe using text_cols\nShow in docs\n\n\n\ndoc(RegressionBlock)\n\nRegressionBlock[source]RegressionBlock(n_out=None)\n\nTransformBlock for float targets\nShow in docs\n\n\n\ndoc(SentencePieceTokenizer)\n\nclass SentencePieceTokenizer[source]SentencePieceTokenizer(lang='en', special_toks=None, sp_model=None, vocab_sz=None, max_vocab_sz=30000, model_type='unigram', char_coverage=None, cache_dir='tmp')\n\nSpacy tokenizer for lang\nShow in docs\n\n\n\ntxt_blk_lm = TextBlock.from_df(text_cols='safe_text', is_lm=True, tok_func=SpacyTokenizer, res_col_name='text' ) #Next : SentencePiece https://forums.fast.ai/t/fastai-v2-text/53529/293\n\n\ndoc(ColReader)\n\nclass ColReader[source]ColReader(cols, pref='', suff='', label_delim=None)\n\nRead cols in row with potential pref and suff\nShow in docs\n\n\nThe task of Languagemodel is to predict the words based on the given text\n\nvaccinate_lm = DataBlock(blocks=(txt_blk_lm), # There is no dependent variable \n                      get_x=ColReader(cols='text'), # Independent variable\n                      splitter=RandomSplitter(valid_pct=0.15,seed=42)\n                      )\n\n\n#vaccinate_lm.summary(source=train_df)\n\n\ndoc(DataBlock.dataloaders)\n\nDataBlock.dataloaders[source]DataBlock.dataloaders(source, path='.', verbose=False, lens=None, cache=2, bs=64, seq_len=72, num_workers=0, shuffle=False, do_setup=True, pin_memory=False, timeout=0, batch_size=None, drop_last=False, indexed=None, n=None, device=None, wif=None, before_iter=None, after_item=None, before_batch=None, after_batch=None, after_iter=None, create_batches=None, create_item=None, create_batch=None, retain=None, get_idxs=None, sample=None, shuffle_fn=None, do_batch=None)\n\nCreate a DataLoaders object from source\nShow in docs\n\n\n\nvaccinate_lm\n\n&lt;fastai2.data.block.DataBlock at 0x7fee3255b6a0&gt;\n\n\n\ndls_lm = vaccinate_lm.dataloaders(source=train_df, bs=64, seq_len=72, verbose=True) \n\n\n\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: \n\n\n\ndls_lm.one_batch()\n\n(LMTensorText([[   2,    8,  375,  ...,   62,   15,    0],\n         [  48,   36,    2,  ...,   15,  818,  625],\n         [ 488,   22,    8,  ...,  113,   42, 2207],\n         ...,\n         [  79,   11,   88,  ...,   86,   18,  110],\n         [   9,    8,   49,  ..., 1314,   93,   19],\n         [  13,    9,    8,  ..., 4422,  197,   22]], device='cuda:0'),\n tensor([[   8,  375,   32,  ...,   15,    0,    0],\n         [  36,    2,    8,  ...,  818,  625,    7],\n         [  22,    8,  104,  ...,   42, 2207,   58],\n         ...,\n         [  11,   88,   10,  ...,   18,  110,    0],\n         [   8,   49,  200,  ...,   93,   19,  594],\n         [   9,    8,  108,  ...,  197,   22,   10]], device='cuda:0'))\n\n\n\nx, y = dls_lm.one_batch()\n\n\nx.shape, y.shape\n\n(torch.Size([64, 72]), torch.Size([64, 72]))\n\n\n\ndls_lm.show_batch()\n\n\n\n\n\ntext\ntext_\n\n\n\n\n0\nxxbos xxmaj eradicating polio . xxmaj using xxup gis to fight this disease . # esriuc vaccinations and improved xxunk conditions . &lt; url &gt; xxbos xxmaj third xxup mmr xxmaj dose well - tolerated xxmaj during xxmaj mumps xxmaj outbreak xxunk &lt; url &gt; # xxunk # newyork # xxup ny xxbos ‚Äú &lt; user &gt; xxmaj how about parents xxmaj guide to xxmaj dealing xxmaj with xxmaj people xxmaj who\nxxmaj eradicating polio . xxmaj using xxup gis to fight this disease . # esriuc vaccinations and improved xxunk conditions . &lt; url &gt; xxbos xxmaj third xxup mmr xxmaj dose well - tolerated xxmaj during xxmaj mumps xxmaj outbreak xxunk &lt; url &gt; # xxunk # newyork # xxup ny xxbos ‚Äú &lt; user &gt; xxmaj how about parents xxmaj guide to xxmaj dealing xxmaj with xxmaj people xxmaj who have\n\n\n1\n&gt; &lt; user &gt; &lt; user &gt; parents would no longer be able to xxunk on the herd immunity of protected children . xxbos xxmaj all i wanted was an opportunity , a bad nigga , & & some immunity ; to keep a xxunk ' safe from the snakes .. xxbos &lt; user &gt; xxmaj news you might 've missed , from &lt; user &gt; xxmaj no . 5 : 400\n&lt; user &gt; &lt; user &gt; parents would no longer be able to xxunk on the herd immunity of protected children . xxbos xxmaj all i wanted was an opportunity , a bad nigga , & & some immunity ; to keep a xxunk ' safe from the snakes .. xxbos &lt; user &gt; xxmaj news you might 've missed , from &lt; user &gt; xxmaj no . 5 : 400 million\n\n\n2\nurl &gt; xxbos ‚Äú &lt; user &gt; xxmaj the ethical xxunk of parents who refuse to vaccinate their children &lt; url &gt; # vaccineswork # vaccinations xxbos xxmaj how well - vaccinated is your child ‚Äôs kindergarten ? : xxmaj the xxmaj california xxmaj health and xxmaj safety xxmaj code requires elementary school ‚Ä¶ &lt; url &gt; xxbos &lt; user &gt; \" i never got my kids vaccinated and they are fine\n&gt; xxbos ‚Äú &lt; user &gt; xxmaj the ethical xxunk of parents who refuse to vaccinate their children &lt; url &gt; # vaccineswork # vaccinations xxbos xxmaj how well - vaccinated is your child ‚Äôs kindergarten ? : xxmaj the xxmaj california xxmaj health and xxmaj safety xxmaj code requires elementary school ‚Ä¶ &lt; url &gt; xxbos &lt; user &gt; \" i never got my kids vaccinated and they are fine \"\n\n\n3\nxxmaj vaccination xxmaj records for xxmaj kids &lt; url &gt; xxbos &lt; user &gt; my mmr shows that because xxmaj i 've only xxunk with my xxup xxunk friend for like two weeks . xxmaj and alright xxmaj i 'll believe it when i see it lmao xxbos xxmaj german xxmaj xxunk xxmaj who xxmaj denied xxmaj measles xxmaj exists xxmaj xxunk xxmaj to xxmaj pay xxmaj more xxmaj than $ 100\nvaccination xxmaj records for xxmaj kids &lt; url &gt; xxbos &lt; user &gt; my mmr shows that because xxmaj i 've only xxunk with my xxup xxunk friend for like two weeks . xxmaj and alright xxmaj i 'll believe it when i see it lmao xxbos xxmaj german xxmaj xxunk xxmaj who xxmaj denied xxmaj measles xxmaj exists xxmaj xxunk xxmaj to xxmaj pay xxmaj more xxmaj than $ 100 ,\n\n\n4\nworried about , it 's not worse than the measles . xxmaj shut up . \\n xxmaj xxunk , \\n xxmaj everyone xxbos xxmaj how can we convince parents to vaccinate ? xxmaj acknowledge their fears . &lt; url &gt; via &lt; user &gt; # autism # antivaxxer xxbos xxmaj is this year 's flu vaccine effective ? xxmaj the experts xxunk in : xxmaj the xxmaj centers for xxmaj disease xxmaj\nabout , it 's not worse than the measles . xxmaj shut up . \\n xxmaj xxunk , \\n xxmaj everyone xxbos xxmaj how can we convince parents to vaccinate ? xxmaj acknowledge their fears . &lt; url &gt; via &lt; user &gt; # autism # antivaxxer xxbos xxmaj is this year 's flu vaccine effective ? xxmaj the experts xxunk in : xxmaj the xxmaj centers for xxmaj disease xxmaj control\n\n\n5\n&lt; user &gt; forms it for sure asked about vaccines . i went to private school but i seem to remember it was required . xxmaj maybe public is xxunk ? xxbos xxmaj neurosurgeon xxmaj exposes xxmaj vaccines , xxup cdc , xxup fda and xxmaj xxunk ‚Ä¶ : &lt; url &gt; xxbos xxmaj xxunk about running up to xxmaj jenny mccarthy and xxunk \" vaccinate xxmaj this ! \" - xxmaj\nuser &gt; forms it for sure asked about vaccines . i went to private school but i seem to remember it was required . xxmaj maybe public is xxunk ? xxbos xxmaj neurosurgeon xxmaj exposes xxmaj vaccines , xxup cdc , xxup fda and xxmaj xxunk ‚Ä¶ : &lt; url &gt; xxbos xxmaj xxunk about running up to xxmaj jenny mccarthy and xxunk \" vaccinate xxmaj this ! \" - xxmaj than\n\n\n6\nschool . xxmaj you must send your kids to some type of school . xxmaj you need vaccines to work most places . xxmaj force xxbos xxmaj state vaccination exemptions up xxunk over 30 years : xxmaj religious and medical exemptions claimed by xxmaj bay xxmaj state ‚Ä¶ &lt; url &gt; xxbos xxmaj vaccines ‚Ä¶ xxunk xxmaj childrens xxmaj clinic ) &lt; url &gt; xxbos y' all better be xxunk your measles\n. xxmaj you must send your kids to some type of school . xxmaj you need vaccines to work most places . xxmaj force xxbos xxmaj state vaccination exemptions up xxunk over 30 years : xxmaj religious and medical exemptions claimed by xxmaj bay xxmaj state ‚Ä¶ &lt; url &gt; xxbos xxmaj vaccines ‚Ä¶ xxunk xxmaj childrens xxmaj clinic ) &lt; url &gt; xxbos y' all better be xxunk your measles up\n\n\n7\nimmigrants bringing diseases into the xxup us ? a xxmaj child from xxmaj mexico got measles at xxmaj disney from an # xxunk xxbos &lt; user &gt; i choose not to vaccinate my kids and my xxunk & & xxunk make me feel good about it . xxbos i xxunk this : xxmaj why xxmaj did xxmaj vaccinated xxmaj people xxmaj get xxmaj measles at xxmaj disneyland ? | xxup xxunk &lt;\nbringing diseases into the xxup us ? a xxmaj child from xxmaj mexico got measles at xxmaj disney from an # xxunk xxbos &lt; user &gt; i choose not to vaccinate my kids and my xxunk & & xxunk make me feel good about it . xxbos i xxunk this : xxmaj why xxmaj did xxmaj vaccinated xxmaj people xxmaj get xxmaj measles at xxmaj disneyland ? | xxup xxunk &lt; url\n\n\n8\nxxmaj measles outbreak in xxmaj xxunk xxmaj county spreads to four &lt; url &gt; via &lt; url &gt; xxbos xxmaj good . xxmaj exempt from vaccines , exempt from school . xxmaj period . \" california xxmaj moves to xxmaj ban xxmaj all xxmaj vaccination xxmaj exemptions \" &lt; url &gt; xxbos xxmaj lol i ca n't with people who do n't vaccinate their children . xxmaj god is n't going to\nmeasles outbreak in xxmaj xxunk xxmaj county spreads to four &lt; url &gt; via &lt; url &gt; xxbos xxmaj good . xxmaj exempt from vaccines , exempt from school . xxmaj period . \" california xxmaj moves to xxmaj ban xxmaj all xxmaj vaccination xxmaj exemptions \" &lt; url &gt; xxbos xxmaj lol i ca n't with people who do n't vaccinate their children . xxmaj god is n't going to save\n\n\n\n\n\n\n# Vocab\n\n\nlen(dls_lm.train.vocab), dls_lm.train.vocab[:30]\n\n(4472,\n ['xxunk',\n  'xxpad',\n  'xxbos',\n  'xxeos',\n  'xxfld',\n  'xxrep',\n  'xxwrep',\n  'xxup',\n  'xxmaj',\n  '&gt;',\n  '&lt;',\n  '.',\n  '#',\n  'user',\n  'url',\n  'the',\n  'to',\n  ',',\n  'measles',\n  'a',\n  'of',\n  'i',\n  ':',\n  'in',\n  'and',\n  '‚Ä¶',\n  '!',\n  'is',\n  'vaccine',\n  'for'])"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#language-model",
    "href": "drafts/zindi_vaccination.html#language-model",
    "title": "Outline",
    "section": "Language Model",
    "text": "Language Model\n\ndoc(language_model_learner)\n\nlanguage_model_learner[source]language_model_learner(dls, arch, config=None, drop_mult=1.0, pretrained=True, pretrained_fnames=None, loss_func=None, opt_func='Adam', lr=0.001, splitter='trainable_params', cbs=None, metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95))\n\nCreate a Learner with a language model from dls and arch.\nShow in docs\n\n\n\nlearn_lm = language_model_learner(dls=dls_lm, arch=AWD_LSTM, pretrained=True, metrics=[accuracy, Perplexity()]) # Next : [accuracy, Perplexity()], AWD_QRNN\n\n\n\n\n\nlearn_lm.model\n\nSequentialRNN(\n  (0): AWD_LSTM(\n    (encoder): Embedding(4472, 400, padding_idx=1)\n    (encoder_dp): EmbeddingDropout(\n      (emb): Embedding(4472, 400, padding_idx=1)\n    )\n    (rnns): ModuleList(\n      (0): WeightDropout(\n        (module): LSTM(400, 1152, batch_first=True)\n      )\n      (1): WeightDropout(\n        (module): LSTM(1152, 1152, batch_first=True)\n      )\n      (2): WeightDropout(\n        (module): LSTM(1152, 400, batch_first=True)\n      )\n    )\n    (input_dp): RNNDropout()\n    (hidden_dps): ModuleList(\n      (0): RNNDropout()\n      (1): RNNDropout()\n      (2): RNNDropout()\n    )\n  )\n  (1): LinearDecoder(\n    (decoder): Linear(in_features=400, out_features=4472, bias=True)\n    (output_dp): RNNDropout()\n  )\n)\n\n\n\nlearn_lm.summary()\n\nSequentialRNN (Input shape: ['64 x 72'])\n================================================================\nLayer (type)         Output Shape         Param #    Trainable \n================================================================\nRNNDropout           64 x 72 x 400        0          False     \n________________________________________________________________\nRNNDropout           64 x 72 x 1152       0          False     \n________________________________________________________________\nRNNDropout           64 x 72 x 1152       0          False     \n________________________________________________________________\nLinear               64 x 72 x 4472       1,793,272  True      \n________________________________________________________________\nRNNDropout           64 x 72 x 400        0          False     \n________________________________________________________________\n\nTotal params: 1,793,272\nTotal trainable params: 1,793,272\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function Adam at 0x7fee89ac9d90&gt;\nLoss function: FlattenedLoss of CrossEntropyLoss()\n\nModel frozen up to parameter group number 3\n\nCallbacks:\n  - TrainEvalCallback\n  - Recorder\n  - ProgressCallback\n  - ModelReseter\n  - RNNRegularizer\n\n\nFind the learning rate\n\ndoc(Learner.lr_find)\n\nLearner.lr_find[source]Learner.lr_find(start_lr=1e-07, end_lr=10, num_it=100, stop_div=True, show_plot=True, suggestions=True)\n\nLaunch a mock training to find a good learning rate, return lr_min, lr_steep if suggestions is True\nShow in docs\n\n\n\nlearn_lm.lr_find(suggestions=True)\n\n\n\n\nSuggestedLRs(lr_min=0.06309573650360108, lr_steep=0.0691830962896347)\n\n\n\n\n\n\n\n\n\n\ndoc(Learner.fine_tune)\n\nLearner.fine_tune[source]Learner.fine_tune(epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False)\n\nFine tune with freeze for freeze_epochs then with unfreeze from epochs using discriminative LR\nShow in docs\n\n\n\nlr=1e-2\n\n\nlearn_lm.fine_tune(epochs=3, base_lr=lr)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n5.276912\n3.852003\n0.315016\n47.087299\n00:14\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n4.046282\n3.538154\n0.352984\n34.403339\n00:15\n\n\n1\n3.818501\n3.393587\n0.369528\n29.772551\n00:15\n\n\n2\n3.671724\n3.367341\n0.373283\n29.001310\n00:15\n\n\n\n\n\n\nlearn_lm.save_encoder(file='fine_tuned_enc')"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#datablock-for-textregression",
    "href": "drafts/zindi_vaccination.html#datablock-for-textregression",
    "title": "Outline",
    "section": "DataBlock for TextRegression",
    "text": "DataBlock for TextRegression\n\ndoc(TextBlock.from_df)\n\nTextBlock.from_df[source]TextBlock.from_df(text_cols, vocab=None, is_lm=False, seq_len=72, min_freq=3, max_vocab=60000, tok_func='SpacyTokenizer', rules=None, sep=' ', n_workers=2, mark_fields=None, res_col_name='text', **kwargs)\n\nBuild a TextBlock from a dataframe using text_cols\nShow in docs\n\n\n\ndoc(SpacyTokenizer)\n\nclass SpacyTokenizer[source]SpacyTokenizer(lang='en', special_toks=None, buf_sz=5000)\n\nSpacy tokenizer for lang\nShow in docs\n\n\n\ntxt_blk_reg = TextBlock.from_df(text_cols='safe_text', vocab=dls_lm.vocab, is_lm=False, tok_func=SpacyTokenizer)\n\n\nvaccinate_reg = DataBlock(blocks=(txt_blk_reg, RegressionBlock),\n                          get_x=ColReader(cols='text'),\n                          get_y=ColReader(cols='label'),\n                          splitter=RandomSplitter(valid_pct=0.2, seed=42))\n\n\nvaccinate_reg.summary(source=train_df)\n\nSetting-up type transforms pipelines\nCollecting items from        tweet_id  ... agreement\n0      CL1KWCMY  ...  1.000000\n1      E3303EME  ...  1.000000\n2      M4IVFSMS  ...  1.000000\n3      1DR6ROZ4  ...  1.000000\n4      J77ENIIE  ...  1.000000\n...         ...  ...       ...\n9996   IU0TIJDI  ...  1.000000\n9997   WKKPCJY6  ...  0.666667\n9998   ST3A265H  ...  1.000000\n9999   6Z27IJGD  ...  1.000000\n10000  P6190L3Q  ...  0.666667\n\n[9999 rows x 4 columns]\nFound 9999 items\n2 datasets of sizes 8000,1999\nSetting up Pipeline: ColReader -&gt; Tokenizer -&gt; Numericalize\n\n\n\n\n\nSetting up Pipeline: ColReader -&gt; RegressionSetup\n\nBuilding one sample\n  Pipeline: ColReader -&gt; Tokenizer -&gt; Numericalize\n    starting from\n      tweet_id                                                                                                                                                                                                                    HF5RFML5\nlabel                                                                                                                                                                                                                              0\nagreement                                                                                                                                                                                                                   0.666667\ntext           [xxbos, i, find, it, hard, to, believe, that, no, one, saw, a, problem, with, \", diplomatic, immunity, ., \", xxmaj, it, 's, like, a, pass, to, go, on, a, killing, spree, ,, says, xxmaj, law, &, &, xxmaj, order, .]\ntext_length                                                                                                                                                                                                                       40\nName: 3327, dtype: object\n    applying ColReader gives\n      (#40) ['xxbos','i','find','it','hard','to','believe','that','no','one'...]\n    applying Tokenizer gives\n      (#40) ['xxbos','i','find','it','hard','to','believe','that','no','one'...]\n    applying Numericalize gives\n      TensorText of size 40\n  Pipeline: ColReader -&gt; RegressionSetup\n    starting from\n      tweet_id                                                                                                                                                                                                                    HF5RFML5\nlabel                                                                                                                                                                                                                              0\nagreement                                                                                                                                                                                                                   0.666667\ntext           [xxbos, i, find, it, hard, to, believe, that, no, one, saw, a, problem, with, \", diplomatic, immunity, ., \", xxmaj, it, 's, like, a, pass, to, go, on, a, killing, spree, ,, says, xxmaj, law, &, &, xxmaj, order, .]\ntext_length                                                                                                                                                                                                                       40\nName: 3327, dtype: object\n    applying ColReader gives\n      0.0\n    applying RegressionSetup gives\n      tensor(0.)\n\nFinal sample: (TensorText([   2,   21,  455,   40,  574,   16,  228,   41,   68,  123,  792,   19,\n         622,   56,   31, 1667,   65,   11,   31,    8,   40,   38,   93,   19,\n         790,   16,  155,   49,   19,  340,    0,   17,  199,    8,  353,   39,\n          39,    8, 1697,   11]), tensor(0.))\n\n\n\n\n\n\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: partial\nSetting up after_batch: Pipeline: \n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ToTensor\n    starting from\n      (TensorText of size 40, tensor(0.))\n    applying ToTensor gives\n      (TensorText of size 40, tensor(0.))\n\nAdding the next 3 samples\n\nApplying before_batch to the list of samples\n  Pipeline: partial\n    starting from\n      [(TensorText of size 40, tensor(0.)), (TensorText of size 28, tensor(0.)), (TensorText of size 27, tensor(1.)), (TensorText of size 33, tensor(1.))]\n    applying partial gives\n      [(TensorText of size 40, tensor(0.)), (TensorText of size 40, tensor(0.)), (TensorText of size 40, tensor(1.)), (TensorText of size 40, tensor(1.))]\n\nCollating items in a batch\n\nNo batch_tfms to apply\n\n\n\ndls_reg = vaccinate_reg.dataloaders(source=train_df, verbose=True, bs=64, seq_len=72)\n\n\n\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: partial\nSetting up after_batch: Pipeline: \n\n\n\nx, y = dls_reg.one_batch()\n\n\nx.shape, y.shape # One would expect seq_len as 72 but we get 53 Why???\n\n(torch.Size([64, 53]), torch.Size([64]))\n\n\n\ndls_reg.show_batch()\n\n\n\n\n\ntext\ntext_\n\n\n\n\n0\nxxbos &lt; user &gt; xxmaj how xxmaj could xxmaj you xxmaj put xxmaj ur xxmaj child xxmaj in xxmaj so xxmaj much xxmaj danger ? xxmaj most xxmaj of xxmaj the xxmaj diseases xxmaj that xxmaj they xxmaj have xxmaj vaccines 4 , i xxmaj had xxmaj xxunk . xxmaj vaccinate . xxunk\n1.0\n\n\n1\nxxbos am - news : xxmaj week xxmaj ahead : xxup xxunk in the xxup er , xxup hiv xxmaj news , xxmaj docs on xxmaj measles xxmaj vaccine : xxmaj week xxmaj ahead : xxup xxunk in the xxup er , xxup hiv xxmaj news , xxmaj docs ‚Ä¶ &lt; url &gt;\n0.0\n\n\n2\nxxbos xxup like xxup why xxup the xxup fuck xxup are xxup people xxup not xxup vaccinating xxup their xxup kids ? ! ? ! ? ! i xxup do nt xxup get xxup it & & xxup it xxup xxunk xxup me xxup off xxrep 3 !\n1.0\n\n\n3\nxxbos i xxmaj know xxmaj i 'm xxmaj late xxup af xxmaj but xxmaj happy # xxunk xxmaj to xxmaj my xxmaj xxunk &lt; user &gt; xxmaj turn xxmaj up xxmaj lil xxmaj bro ! üíØ # xxup mmr @ xxup xxunk xxmaj south ‚Ä¶ &lt; url &gt;\n0.0\n\n\n4\nxxbos &lt; user &gt; &lt; user &gt; xxmaj ppl xxmaj need 2 xxmaj immunize xxmaj their xxmaj kids . xxmaj do xxmaj research . xxmaj most xxmaj diseases xxup worse xxup than xxup inoculation . xxup stop xxmaj this xxmaj misguided xxmaj insanity , xxmaj shots !\n1.0\n\n\n5\nxxbos xxmaj let 's get this straight , first it was xxup isis , then it was xxmaj ebola , then back to xxup isis , and now ‚Ä¶ xxmaj measles ? xxmaj fake xxup isis threats ? xxmaj yep . xxmaj fear . xxmaj fear .\n0.0\n\n\n6\nxxbos &lt; user &gt; &lt; user &gt; &lt; user &gt; xxup smdh # xxmaj flu # xxmaj vaccines , # xxmaj pharma # xxmaj fraud , # xxmaj quack # xxmaj science , the # xxup cdc & & # xxup who # health &lt; url &gt;\n-1.0\n\n\n7\nxxbos xxup not inspiring to hear xxmaj another xxmaj xxunk in the xxmaj xxunk this xxup am . xxmaj my gr 8 class voted to sing it at grad ( xxunk ) & & then i heard it on xxup mmr , not xxup xxunk .\n0.0\n\n\n8\nxxbos [ to roommate ] \\n \" we should go get flu vaccines . \" \\n xxmaj why ? \\n \" well , the flu is n't a great a great disease . \" \\n xxmaj the flu is fine . \\n \" ok . \"\n0.0"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#regression-learner",
    "href": "drafts/zindi_vaccination.html#regression-learner",
    "title": "Outline",
    "section": "Regression Learner",
    "text": "Regression Learner\n\ndoc(text_classifier_learner)\n\ntext_classifier_learner[source]text_classifier_learner(dls, arch, seq_len=72, config=None, pretrained=True, drop_mult=0.5, n_out=None, lin_ftrs=None, ps=None, max_len=1440, y_range=None, loss_func=None, opt_func='Adam', lr=0.001, splitter='trainable_params', cbs=None, metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95))\n\nCreate a Learner with a text classifier from dls and arch.\nShow in docs\n\n\n\nlearn_reg = text_classifier_learner(dls=dls_reg, \n                                    arch=AWD_LSTM, \n                                    pretrained=True,\n                                    loss_func=MSELossFlat(),\n                                    metrics=[rmse],\n                                    y_range=(-1.2, 1.2)\n                                    ) # Loss func ?? Metrics ??? y_range??? encoder ???\n\n\nlearn_reg.load_encoder('fine_tuned_enc')\n\n&lt;fastai2.text.learner.TextLearner at 0x7fedb215c860&gt;\n\n\n\nlearn_reg.summary()\n\nSequentialRNN (Input shape: ['64 x 53'])\n================================================================\nLayer (type)         Output Shape         Param #    Trainable \n================================================================\nRNNDropout           64 x 53 x 400        0          False     \n________________________________________________________________\nRNNDropout           64 x 53 x 1152       0          False     \n________________________________________________________________\nRNNDropout           64 x 53 x 1152       0          False     \n________________________________________________________________\nBatchNorm1d          64 x 1200            2,400      True      \n________________________________________________________________\nDropout              64 x 1200            0          False     \n________________________________________________________________\nLinear               64 x 50              60,000     True      \n________________________________________________________________\nReLU                 64 x 50              0          False     \n________________________________________________________________\nBatchNorm1d          64 x 50              100        True      \n________________________________________________________________\nDropout              64 x 50              0          False     \n________________________________________________________________\nLinear               64 x 1               50         True      \n________________________________________________________________\nSigmoidRange         64 x 1               0          False     \n________________________________________________________________\n\nTotal params: 62,550\nTotal trainable params: 62,550\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function Adam at 0x7fee89ac9d90&gt;\nLoss function: FlattenedLoss of MSELoss()\n\nModel frozen up to parameter group number 4\n\nCallbacks:\n  - TrainEvalCallback\n  - Recorder\n  - ProgressCallback\n  - ModelReseter\n  - RNNRegularizer\n\n\n\nlearn_reg.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.017378008365631102, lr_steep=9.12010818865383e-07)\n\n\n\n\n\n\n\n\n\n\nlr=1e-2\n\n\nlearn_reg.fine_tune(epochs=3, base_lr=lr)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\n_rmse\ntime\n\n\n\n\n0\n0.517643\n0.393474\n0.627276\n00:11\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\n_rmse\ntime\n\n\n\n\n0\n0.407758\n0.361201\n0.601000\n00:13\n\n\n1\n0.345667\n0.342141\n0.584928\n00:13\n\n\n2\n0.292315\n0.336554\n0.580133\n00:13"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#predictions",
    "href": "drafts/zindi_vaccination.html#predictions",
    "title": "Outline",
    "section": "Predictions",
    "text": "Predictions\n\ntest_df\n\n\n\n\n\n\n\n\ntweet_id\nsafe_text\n\n\n\n\n0\n00BHHHP1\n&lt;user&gt; &lt;user&gt; ... &amp; 4 a vaccine given 2 healthy peeps, FDA think just not worth the AE risk unfortunately.\n\n\n1\n00UNMD0E\nStudents starting school without whooping cough vaccinations &lt;url&gt; #scpick\n\n\n2\n01AXPTJF\nI'm kinda over every ep of &lt;user&gt; being \"ripped from the headlines.\" Measles? Let's get back to crime. #SVU\n\n\n3\n01HOEQJW\nHow many innocent children die for lack of vaccination each year? Around 1.5 million. Too bad all their parents couldn't be here. #SB277\n\n\n4\n01JUKMAO\nCDC eyeing bird flu vaccine for humans, though risk is low: Federal officials said Wednesday they're taking steps‚Ä¶ &lt;url&gt;\n\n\n...\n...\n...\n\n\n5172\nZXVVNC5O\njenny mccarthy is on new years rockin eve. what has she done lately besides not vaccinate her kids and give us all goddamn polio??\n\n\n5173\nZYIANVI8\nMeasles reported in Clark Co. for 1st time since 2011 &lt;url&gt;\n\n\n5174\nZYITEHAH\n&lt;user&gt; issues alert regarding Measles in TX. Keep your DDx up to date, people! #Emergencymedicine\n\n\n5175\nZZ3BMBTG\nI can't believe people don't vaccinate their kids! I've been vaccinated for everything and then some.\n\n\n5176\nZZIYCVNH\n\"&lt;user&gt; Alternatives to #Flu Vaccine &lt;url&gt; #natural #health\" A good read with a few new tips &amp; many we #jerf folk know\n\n\n\n\n5177 rows √ó 2 columns\n\n\n\n\ndoc(learn_reg.dls.test_dl)\n\nDataLoaders.test_dl[source]DataLoaders.test_dl(test_items, rm_type_tfms=None, with_labels=False, **kwargs)\n\nCreate a test dataloader from test_items using validation transforms of dls\nShow in docs\n\n\n\ntest_dl = learn_reg.dls.test_dl(test_items=test_df['safe_text'], verbose=True)\n\n\ndoc(learn_reg.get_preds)\n\nLearner.get_preds[source]Learner.get_preds(ds_idx=1, dl=None, with_input=False, with_decoded=False, with_loss=False, act=None, inner=False, reorder=True, save_preds=None, save_targs=None, concat_dim=0)\n\nGet the predictions and targets on the ds_idx-th dbunchset or dl, optionally with_input and with_loss\nShow in docs\n\n\n\nlearn_reg.get_preds(dl=test_dl, with_decoded=True)\n\n\n\n\n(tensor([[-0.0063],\n         [ 0.8084],\n         [ 0.2124],\n         ...,\n         [ 0.2862],\n         [ 0.8839],\n         [ 0.4553]]), None, tensor([[-0.0063],\n         [ 0.8084],\n         [ 0.2124],\n         ...,\n         [ 0.2862],\n         [ 0.8839],\n         [ 0.4553]]))\n\n\n\npreds, _ , preds_raw = learn_reg.get_preds(dl=test_dl, with_decoded=True)"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#submissions",
    "href": "drafts/zindi_vaccination.html#submissions",
    "title": "Outline",
    "section": "Submissions",
    "text": "Submissions\n\nvaccinate_tweets.ls()\n\n(#4) [Path('SampleSubmission.csv'),Path('Train.csv'),Path('Test.csv'),Path('models')]\n\n\n\nsubmission = pd.read_csv(vaccinate_tweets/'SampleSubmission.csv')\n\n\nsubmission.head()\n\n\n\n\n\n\n\n\ntweet_id\nlabel\n\n\n\n\n0\n00BHHHP1\n0\n\n\n1\n00UNMD0E\n0\n\n\n2\n01AXPTJF\n0\n\n\n3\n01HOEQJW\n0\n\n\n4\n01JUKMAO\n0\n\n\n\n\n\n\n\n\nsubmission['label'] = preds.flatten()\n\n\nsubmission.head()\n\n\n\n\n\n\n\n\ntweet_id\nlabel\n\n\n\n\n0\n00BHHHP1\n-0.006287\n\n\n1\n00UNMD0E\n0.808357\n\n\n2\n01AXPTJF\n0.212385\n\n\n3\n01HOEQJW\n0.994107\n\n\n4\n01JUKMAO\n0.177752\n\n\n\n\n\n\n\nYaaaaaaaaaaaaaaaaaaayyyyyyy!!!!!\n\nsubmission.to_csv('first_baseline.csv', index=False)"
  },
  {
    "objectID": "drafts/zindi_vaccination.html#future-ideas-to-try",
    "href": "drafts/zindi_vaccination.html#future-ideas-to-try",
    "title": "Outline",
    "section": "Future Ideas to try",
    "text": "Future Ideas to try\n\n[Data] Preprocessing\n\nSentencePiece (subword tokenization)\n\n[Data] Language Model\n\nIncorporate additional data during language model (eg: test set, additional social media datasets)\n\n[Model] Ensembling using K fold cross validation [TODO : Link] : average of all predictions.\n[Model] Transformer variants, AWD_QRNN\n[Model] Blending : Models with higher predictions will get higher weights where as models with lower predictions will get lower weights [TODO: Link]\n[Model] Mixed Precision Training\n[Model] Use fit_one_cycle and customize the learning rate.\n[Optimizer] Currently we use ADAM, try with RADAM, Ranger optimizer variants\n[HyperParameter Tuning] Play with hyperparameters\n[Experiment Tracking] Integrating callbacks for Weights and Biases [TODO: Link]"
  },
  {
    "objectID": "drafts/welcome/index.html",
    "href": "drafts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome! I‚Äôm Manikandan Sivanesan, and this is my learning blog where I document my journey exploring search systems and agentic AI.\nThis blog serves as a medium to teach my past self and document discoveries for my future self about things I‚Äôm curious about. I write about information retrieval, search systems (particularly Apache Solr), NLP, and machine learning topics."
  },
  {
    "objectID": "about.html#current-work",
    "href": "about.html#current-work",
    "title": "About",
    "section": "Current Work",
    "text": "Current Work\nüî≠ AI Technical Strategy Lead | Senior Principal Software Engineer @ Red Hat\nFocus Areas: Agentic AI, Retrieval Augmented Generation (RAG), Reranking, Hybrid Search\nAdditional Links:\n\nfastai forum\nHugging Face Space"
  },
  {
    "objectID": "about.html#talks",
    "href": "about.html#talks",
    "title": "About",
    "section": "Talks",
    "text": "Talks\n2020\n\nUsing Intent Data to Optimize the Self-Solve Experience | YouTube\nActivate, Virtual Search & AI Conference\nHow Red Hat uses intent data to automate and improve workflow efficiency for support personnel and contact centers.\n\n2018\n\nEmpowering Customers to Self Solve: A Findability Journey\nActivate 2018\nEvolution of different search techniques in our Solution Engine and the customers‚Äô search journey."
  },
  {
    "objectID": "about.html#papers",
    "href": "about.html#papers",
    "title": "About",
    "section": "Papers",
    "text": "Papers\n2020\n\nHate Speech and Offensive Content Identification in Indo-European Languages\nHASOC 2020 conference\nAbstract: This article describes our team Chrestotes‚Äô approach to the solution submitted to HASOC 2020: Hate Speech and Offensive Content Identification in Indo-European Languages. We demonstrate an end to end solution to the fine-grained detection of hate speech in tweets. Our solution is focused on the English Task which has been split into two subtasks. Our model achieved macro-average f1-scores of 0.4969 and 0.2652 on the subtasks A and B respectively. This solution places us in the middle of the leaderboard for subtask A and first place for subtask B.\n\n\nCertifications\n2025\n\nElite AI Assisted Coding - Maven (Oct 2025)\nSkills: AI assisted Development, Software Development\nAI Evals For Engineers & PMs - Maven (May 2025)\nSolve It With Code - fast.ai / Answer.ai (Feb 2025)\nInstructors: Jeremy Howard, Johno Whitaker\nHands-on course on solving real-world problems using AI-assisted development. Focused on iterative coding practices, breaking down complexity, and building functional, maintainable software.\nCourse Link\n\n2024\n\nSystematically Improving RAG Applications - Maven (Aug 2024)\nMastering LLMs: A Conference For Developers & Data Scientists - Maven (Jun 2024)\n\n2022\n\nInternational Fellowship - fast.ai Deep Learning for Coders course (2022)\nSearch with Machine Learning - Co:rise (Mar 2022)\n\n2021\n\nFull Stack Deep Learning - Spring 2021\nfastclean : Experiments to find incorrect labels in the dataset and noisy training\n\n2020\n\nInternational Fellowship - fast.ai Deep Learning for Coders course (2020)\n\n2016\n\nMachine Learning Foundations: A Case Study Approach - Coursera (Jan 2016)\n\n2015\n\nCloud Computing Applications - Coursera (Oct 2015)\n\n2014\n\nFunctional Programming Principles in Scale - Coursera (Jun 2014)"
  },
  {
    "objectID": "posts/2020-06-29-tracking-data-model-using-dvc.html",
    "href": "posts/2020-06-29-tracking-data-model-using-dvc.html",
    "title": "Tracking Data and Model in Machine Learning projects",
    "section": "",
    "text": "Difference between software engineering and machine learning projects\ncode, config, dependencies vs code, data, model, config, dependencies\nMyriad of Data files (raw, intermediate)\nNot in repository\nReproducible ML models targeting metrics [Experiments Link]\nTuning and Experimentation tracking\nModel Deployment & Revert"
  },
  {
    "objectID": "posts/2020-06-29-tracking-data-model-using-dvc.html#why-should-you-track-everything",
    "href": "posts/2020-06-29-tracking-data-model-using-dvc.html#why-should-you-track-everything",
    "title": "Tracking Data and Model in Machine Learning projects",
    "section": "",
    "text": "Difference between software engineering and machine learning projects\ncode, config, dependencies vs code, data, model, config, dependencies\nMyriad of Data files (raw, intermediate)\nNot in repository\nReproducible ML models targeting metrics [Experiments Link]\nTuning and Experimentation tracking\nModel Deployment & Revert"
  },
  {
    "objectID": "posts/2020-06-29-tracking-data-model-using-dvc.html#dvc",
    "href": "posts/2020-06-29-tracking-data-model-using-dvc.html#dvc",
    "title": "Tracking Data and Model in Machine Learning projects",
    "section": "DVC",
    "text": "DVC\n\nDVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.\nExperiments are tracked by combining the Code + Data files\nData files\n\nLocal cache\nData remotes in S3, SSH etc\n\nMetrics per experiment\nPipelines\n\n\nInstall DVC on Fedora/RHEL/CentOS\nsudo wget https://dvc.org/rpm/dvc.repo -O /etc/yum.repos.d/dvc.repo\nsudo yum update\nsudo yum install dvc\nAdd DVC files in the Git (It is not required but it is good to have project and dvc config in the same directory)"
  },
  {
    "objectID": "posts/2020-06-29-tracking-data-model-using-dvc.html#workflow-for-model-packaging",
    "href": "posts/2020-06-29-tracking-data-model-using-dvc.html#workflow-for-model-packaging",
    "title": "Tracking Data and Model in Machine Learning projects",
    "section": "Workflow for Model Packaging",
    "text": "Workflow for Model Packaging\n\nSetup for connecting to model repository during training development process\nPush the trained model to the repository\nPull the model from the repository during the build process\n\n\nPushing model files\ndvc init\ndvc remote add -d gss-rdu-remote ssh://msivanes@gss-rdu-repo.usersys.redhat.com:/var/www/html/repo/config/ulmfit\n\ndvc add cases_small_sbr_08-06-2020.pkl\ngit commit -am ‚ÄúAdd ulmfit model to project‚Äù\ndvc push -v\n\n\nPulling the model\ngit clone $REPO\ngit pull\ndvc pull # Pulls the data from remote-storage. Equivalent to dvc fetch followed by dvc checkout\ndvc checkout #Update model files"
  },
  {
    "objectID": "posts/2020-06-29-tracking-data-model-using-dvc.html#food-for-thought",
    "href": "posts/2020-06-29-tracking-data-model-using-dvc.html#food-for-thought",
    "title": "Tracking Data and Model in Machine Learning projects",
    "section": "Food for thought",
    "text": "Food for thought\n\nRather than feature branches, think in terms of experiment branches targeting metrics.\nKeep Data and Model files stored outside the repository\nModel files built as part of build process.\nSmoke test should validate the constructed model using validation set."
  },
  {
    "objectID": "posts/2020-06-29-tracking-data-model-using-dvc.html#references",
    "href": "posts/2020-06-29-tracking-data-model-using-dvc.html#references",
    "title": "Tracking Data and Model in Machine Learning projects",
    "section": "References",
    "text": "References\n\nDVC\nCheatsheet\nhttps://christophergs.github.io/machine%20learning/2019/05/13/first-impressions-of-dvc/\nhttps://www.slideshare.net/DmitryPetrov15/pydata-berlin-2018-dvcorg"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html",
    "href": "posts/2020-09-20-intent-detection.html",
    "title": "Intent detection",
    "section": "",
    "text": "Intent Classification is a type of supervised text classification problem. It is used to predict the motivation of the user providing search keywords in a given search engine. Each user visiting the site have a goal in mind and they express in the form of keywords. We have to optimize for satisfying their goal in addition to returning the best documents matching the keywords as search results. In Search for intent, Not inventory[1] Daniel Tunkelang author of Query Understanding publication emphasizes that\n\n‚ÄúSearchers don‚Äôt care what you have in your catalog or how you organize your inventory. They care about what they want. Make it easy for searchers to express their intent, and craft the search experience around their intent.‚Äù\n\nHence as part of Red Hat Customer Portal personalization, we started working on identifying the intents for our site visitor and classify the search queries in order to craft the search experience based on their intent."
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#introduction",
    "href": "posts/2020-09-20-intent-detection.html#introduction",
    "title": "Intent detection",
    "section": "",
    "text": "Intent Classification is a type of supervised text classification problem. It is used to predict the motivation of the user providing search keywords in a given search engine. Each user visiting the site have a goal in mind and they express in the form of keywords. We have to optimize for satisfying their goal in addition to returning the best documents matching the keywords as search results. In Search for intent, Not inventory[1] Daniel Tunkelang author of Query Understanding publication emphasizes that\n\n‚ÄúSearchers don‚Äôt care what you have in your catalog or how you organize your inventory. They care about what they want. Make it easy for searchers to express their intent, and craft the search experience around their intent.‚Äù\n\nHence as part of Red Hat Customer Portal personalization, we started working on identifying the intents for our site visitor and classify the search queries in order to craft the search experience based on their intent."
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#related-work",
    "href": "posts/2020-09-20-intent-detection.html#related-work",
    "title": "Intent detection",
    "section": "Related Work",
    "text": "Related Work\nClassification of searches is domain specific and there is no one size fits all approach. Taxonomy of Web Searches[2] classify the queries into these categories 1. Navigational 2. Informational 3. Transactional\n\nTaxonomy of Ecommerce Searches[3] classifies them into following categories & each with a distinct search behavior.\n\n\nShallow Exploration Queries are short vague queries that a user may use initially in exploring the product space.\nTargeted Purchase Queries are queries used by users to purchase items that they are generally familiar with, thus without much decision making.\nMajor-Item Shopping Queries are used by users shopping a major item relatively expensive & requires some serious exploration, but typically in a limited scope of choices.\nMinor-Item Shopping Queries are provided by users to shop for minor items that are generally not very expensive, but still require some exploration of choices.\nHard-Choice Shopping Queries are used by users who want to deeply explore all the candidate products before finalizing the choice often appropriate when multiple products must be carefully compared with each other.\n\n\nExamples\n\n\nWe looked at how others are handling the intent classification problem because of the shorter length of the text and latency requirement. Deep Search Query Intent Understanding from LinkedIn provides two designs for modeling 1. Predicting the intent in typeahead search using character models 2. Predicting the intent for complete search queries to do Seach Results Page(SERP) Blending. Each linkedin usecase has its own latency and accuracy requirement, former having higher latency but with lower accuracy whereas the latter with acceptable latency but with higher accuracy. Though they explored deep learning CNN, LSTM, BERT models in this paper, production baseline model used is the traditional logistic regression model. Unless you can reduce the size of the model, latency will pose a challenge for productionizing these bigger models.\n\n‚ÄúThe production baseline model is a logistic regression model, with bag-of-words features, and user profile/behavioral features, etc.‚Äù\n\n\nThe next one is Discovering and Classifying In-app Message Intent at Airbnb where they classify the guest messaging intents such as Parking, Checking In and try to provide timely responses. Here they found CNN performed better and has faster training with a shorter serving time."
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#problem-framing",
    "href": "posts/2020-09-20-intent-detection.html#problem-framing",
    "title": "Intent detection",
    "section": "Problem Framing",
    "text": "Problem Framing\nWe leveraged the redhat customer portal top tasks survey in which most of the users indicated the aspect of problem solving as the main reason for the visit in 4/5 responses. Keeping that in mind, we analyzed the search queries for the patterns and came up with these four buckets.\n\nBroad Explorational are single worded vague queries provided by users exploring the product portfolio, a component, vulnerability.\nTargeted Queries (Eg: CVE-1243, RHBA-2351, RHSA-3194) such as Bug fixes, Advisories, Enhancements.\nUsage Queries are provided by users wanting to know how to use specific component or aspect of a product.\nTroubleshoot are provided by users facing an issue and more likely leading to case creation.\n\nInstead of providing the model with poor data, we wanted the model to focus only on classifying the Usage and Troublesoot. We avoided passing the models with vague queries and targeted queries and let the keyword search, onebox and curated searches to handle these types of queries.\n\n\n\n\n\n\n\n\nINTENT\nDEFINITION\nEXAMPLES\n\n\n\n\nTroublesoot\nWhen a user reports an issue/error message or expresses the text in negative forms explicitly that he/she wants to troubleshoot/debug, then such statements are considered to be of TROUBLESHOOT intent\nX Window won‚Äôt start after updating xorg-x11-server-Xorg package?\n\n\n\n\n\n\n\nUsage\nUSAGE intent is considered when it is more along the lines of how to use a product/component or perform an action.\nHow can I block video streaming using squid ?"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#data-collection",
    "href": "posts/2020-09-20-intent-detection.html#data-collection",
    "title": "Intent detection",
    "section": "Data Collection",
    "text": "Data Collection\nIn order to iterate and perform rapid prototyping, we leverage ML teaching and annotation tool Prodigy. The recommendation is to perform manual annotation with binary label in order to keep cognitive load low. This allows the annotator to focus only one concept at a time. We started looking at the Portal Search queries and manually annotated them as TROUBLESHOOT or not. Please check this excellent text classification tutorial by Ines Montani.\n\nIf you only provide a single label, the annotation decision becomes much simpler: does the label apply or not? In this case, Prodigy will present the question as a binary task using the classification interface. You can then hit ACCEPT or REJECT. Even if you have more than one label, it can sometimes be very efficient to make several passes over the data instead of selecting from a list of options. The annotators can focus on one concept at a time, which can reduce the potential for human error ‚Äì especially when working with complicated texts and label schemes.\n\n$ prodigy textcat.teach troubleshoot en_core_web_sm troubleshoot.jsonl\nWe collected around 1500 training samples to bootstrap the project with the help of Active learning features provided. Learn more about active learning from here\nExample training samples\n{\"text\":\"add new nfs storage\",\"label\":\"USAGE\",\"answer\":\"accept\"}\n{\"text\":\"sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\",\"label\":\"TROUBLESHOOT\",\"answer\":\"accept\"}"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#imports-and-utilty-functions",
    "href": "posts/2020-09-20-intent-detection.html#imports-and-utilty-functions",
    "title": "Intent detection",
    "section": "Imports and Utilty functions",
    "text": "Imports and Utilty functions\ndef common_word_removal(df, common_words):\n    \"\"\"This is to remove manually identified common words from the training corpus.\"\"\"\n    \n    # Manual list of common words to be removed based on above data. This will change based on text data input.\n    # We cannot automate this to directly omit the first x words as these words are very relevant to our use case\n    # of inferring intent\n    \n    df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in common_words))\n    return df\n\ndef drop_nulls(df):\n    \"\"\"This function drops the rows in data with null values. \n    It takes in the raw queries as the input.\"\"\"\n    \n    print('Number of queries with null values: ' + str(df['text'].isnull().sum()))\n    df.dropna(inplace=True)\n    return df\n\n\ndef avg_word(sentence):\n    \"\"\"Used in EDA to calculate average wordlength per query\"\"\"\n    words = sentence.split()\n    return (sum(len(word) for word in words)/len(words))"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#data-preparation",
    "href": "posts/2020-09-20-intent-detection.html#data-preparation",
    "title": "Intent detection",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nLoading the data in dataframe\nThis reads the dataset in json lines format into pandas dataframe.\ndef load():\n    raw_data - load_raw_data(pd.read_json(f\"{DATA_DIR}/portalsearch/portalsearch.jsonl\", lines=True))\n\n    # Joining the 'label' and 'answer' column to become one. \n    # E.g.: Troubleshoot-accept, Troubleshoot-reject, Usage-accept, Usage-reject\n    raw_data['label'] = raw_data['label'] + '-' + raw_data['answer']\n\n    # Only selecting the 'accept' labels.\n    raw_data = raw_data[raw_data['label'].isin(['TROUBLESHOOT-accept','USAGE-accept'])]\n\n    raw_data.drop('answer',axis=1,inplace=True)\n    return raw_data\n\n\nPreprocessing\nWe performed the following preprocessing\n\nAdding standard features like word count, character count and average word length per query to the dataframe.\nRemove captalization by lowercasing all words\nRemoving punctuations and special characters\nRemove traditional stopwords like ‚Äúthe‚Äù, ‚Äúa‚Äù, ‚Äúan‚Äù and ‚Äúand‚Äù\nRemove noisy data that are of low value and commonly occuring terms\nLemmatization\n\ndef eda(df):\n    \"\"\"Classic EDA like word count per query, average word length per query etc.\"\"\"\n    \n    # Word count per query\n    df['word_count'] = df['text'].apply(lambda x: len(str(x).split(\" \")))\n    \n    # Character count per query\n    df['character_count'] = df['text'].str.len() # this also includes spaces\n\n    # Average word length per query\n    df['avg_wordlength_per_query'] = df['text'].apply(lambda x: avg_word(x))\n\n    return df\ndef preprocessing(df):\n    \"\"\"Traditional Pre-processing steps.\"\"\"\n    \n    # Lower case for all words. This helps in removing duplicates later\n    df['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    \n    # Removing Punctuations and special characters using regular expression\n    df['text'] = df['text'].str.replace('[^\\w\\s]','')\n    \n    # Removing non-english words\n    df['text'] = df['text'].apply(lambda row: row.encode('ascii',errors='ignore').decode())\n    \n    # Removing stopwords\n    df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n    \n    # Lemmatization\n    lemmatizer = WordNetLemmatizer()\n    df['text'] = df['text'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n    \n    # Common word removal\n    # Remove commonly occurring words from our text data \n    # as their presence will not of any use in classification of our text data.\n    # This step would require manual scanning through the list.\n    frequent_words = pd.Series(' '.join(df['text']).split()).value_counts()\n    print('Top 20 frequent words are \\n' + str(frequent_words[:20]))\n    print('\\n')\n    \n    \n    # Rare words removal\n    # Because they‚Äôre so rare, the association between them and other words is dominated by noise. \n    # Hence we can remove them and later decide whether or not the results improved based on it.\n    \n    # Printing out rare words occuring less than 50 times.\n    rare_words = frequent_words[frequent_words &lt; 50]\n    print('Top 10 rare words are: \\n' + str(rare_words[-10:]))\n    print('\\n')\n    \n    # Dropping queries which are empty after all the pre-processing\n    df['text'].replace('', np.nan, inplace=True)\n    df.dropna(inplace=True)\n    print('The final number of queries after preprocessing are: ' + str(df.shape))\n    \n    return df"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#modeling",
    "href": "posts/2020-09-20-intent-detection.html#modeling",
    "title": "Intent detection",
    "section": "Modeling",
    "text": "Modeling\n\nTrain Test Split\nNext we split the data into training and test data with 3:1 split\ndef train_test_splits(df, test_size):\n    \"\"\"Splitting raw data into training and test data.\"\"\"\n    \n    X = raw_data['text']\n    y = raw_data['label']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    \n    return X_train, X_test, y_train, y_test\n# Splitting raw dataset into testing and training data\nX_train, X_test, y_train, y_test = train_test_splits(raw_data,0.33)\n\n\nVectorization\nThe text needs to be converted into a format that a model could interpret ie numbers. This process is called vectorization. We started with Term Frequency-Inverse Document Frequency(TF-IDF) method using sklearn TfidfVectorizer\n\nTF-IDF is a way to calculate the ‚Äòimportance‚Äô of each word in the dataset. This vectorizer calculates how often a given word appears in the string, and downscales words that appear across different strings.\n\nAn example illustrating how tfidf is calculate for two strings\n\nCode sample\ndef text_vectorization_tfidf(X_train,X_test):\n    \"\"\"tf-idf vectorization using sklearn\"\"\"\n    \n    vectorizer = TfidfVectorizer()\n    X_train_vec = vectorizer.fit_transform(X_train)\n    X_test_vec = vectorizer.transform(X_test)\n    \n    # pretty printing the vocabulary built by the tf-idf vectorizer\n    # pprint.pprint(vectorizer.vocabulary_)\n    \n    return vectorizer, X_train_vec, X_test_vec\n\n# Text Vectorization of training data\nvectorizer, X_train_vec, X_test_vec = text_vectorization_tfidf(X_train, X_test)\n\n\nLinearSVC\n\nThe algorithm we used is a linear support vector classifier (SVC), a commonly used text classification algorithm that works by finding the line or hyper-plane that best differentiates two groups of data points. It is a Support Vector Machine with a linear kernel.Learn more about SVM from here\n\nHere we are performing 3-fold cross validation to improve the generalization and minimize the overfitting on validation set. With the given training set, it is split into 3 folds and one of the fold is used for validation and a score is calculated. Similarity this process repeated with the rest of the folds and the average of all the scores is used as the final score. Please see the excellent scikit guide here for additional details\ndef build_model(X_train_vec, y_train):\n    \"\"\"Build an SVM model with a linear kernel\"\"\"\n    \n    svm = LinearSVC(class_weight=\"balanced\")\n    linear_svc = CalibratedClassifierCV(svm,method='sigmoid') \n     \n    # 3-fold Cross-Validation\n    print(cross_val_score(linear_svc, X_train_vec, y_train, cv=3))\n    \n    # Fitting the model after tuning parameters based on cross-validation results\n    linear_svc.fit(X_train_vec, y_train) \n    \n    return linear_svc\n\n# Building, cross validation and fitting of model\nmodel = build_model(X_train_vec, y_train)"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#evaluation",
    "href": "posts/2020-09-20-intent-detection.html#evaluation",
    "title": "Intent detection",
    "section": "Evaluation",
    "text": "Evaluation\nThe model can be interpreted by looking at the confusion matrix of true labels and predicted labels. Confusion matrix(a.k.a error matrix) allows to understand the performance of the model on unseen or test queries. We can see the accuracy of the model to be around 95%\ndef make_predictions(model,X_test_vec):\n    \"\"\"Makes predictions and spits out confusion matrix.\"\"\"\n    \n    # Predicting results on test data\n    predictions = model.predict(X_test_vec)\n    \n    # Accuracy of model\n    print('Accuracy of model is ' + str(round(accuracy_score(y_test, predictions)*100,2)) + '%')\n    \n    # Precision, Recall and other metrics\n    print(str(classification_report(y_test, predictions, target_names=['TROUBLESHOOT-accept', 'USAGE-accept'])))\n\n    # Confusion Matrix\n    labels = ['TROUBLESHOOT-accept','USAGE-accept']\n    Confusion_Matrix = confusion_matrix(y_test, predictions, labels)\n\n    # Plotting the confusion matrix\n    df_cm = pd.DataFrame(Confusion_Matrix, index = labels,\n                      columns = labels)\n\n    ax = plt.figure(figsize = (12,8))\n    sns.heatmap(df_cm, annot=True,fmt='g')\n\n    # labels, title and ticks\n    ax.suptitle('Confusion Matrix')\n    plt.xlabel('Predicted labels', fontsize=18)\n    plt.ylabel('True labels', fontsize=16)"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#deployment",
    "href": "posts/2020-09-20-intent-detection.html#deployment",
    "title": "Intent detection",
    "section": "Deployment",
    "text": "Deployment\n\nWeb application\nOnce the model is trained and saved, the next step is to create a simple REST endpoint called introspect where the client applications can send the text blob and get the intent prediction as the response. The application endpoint is built using python, flask web framework, gunicorn web containier with 4 parallel workers. The overview of the application is as follows\n\nLoading the model at the application startup\nOnce the query is received, send it to the model only if it is not a known lexical pattern like CVE, Errata, numeric id, url. This prevents garbage inputs and invalid predictions.\nIf the query is one of the known patten return the response with the intent as OTHER.\nTransform the query into a vectorized form before model prediction\nPredict the query and return the prediction, probablity of the predicted class as confidence score.\n\n\n\nContainerizing the application\nBasically containeraization is a modern way to package the application with your code, dependencies, configuration into a format (Eg: Docker Image) suitable to run anywhere whether it‚Äôs public cloud like aws or in your own datacenter.\nCode Sample for util.py\nfrom flask import request, Response\nfrom validators import ValidationFailure\nfrom validators.url import url\nfrom functools import wraps\nimport logging\n\ndef is_url(keyword: str) -&gt; bool:\n    '''\n    Returns True if the keyword contains a valid url else return False\n    '''\n    try:\n        value = url(keyword)\n    except ValidationFailure:\n        return False\n    return value\n\ndef has_known_lexical_pattern(keyword: str) -&gt; bool:\n    '''\n    Return True if the keyword contains known navigational intent like CVE, Errata, id or url else\n    returns False\n    '''\n    #Navigational intent for CVE, Errata, id or url\n    return keyword.lower().startswith(('cve', '(cve', 'rhsa', '(rhsa', 'rhba', '(rhba')) or keyword.isdigit() or is_url(keyword)\n\ndef has_other_intent(query: str) -&gt; bool:\n    return has_known_lexical_pattern(query)\n\ndef invalid_input():\n    \"\"\"Sends a 400 Bad Request\"\"\"\n    return Response(\n    'ensure this value has at mininum 3 characters and most 500 characters', 400,\n    {})\n\ndef validate_input (f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        try:\n            query = request.args ['query']\n            if len (query) &lt; 3 or len (query) &gt; 500:\n                raise Exception ()\n            return f(*args, **kwargs)\n        except:\n            logging.info(\"Input validation failed\")\n            return invalid_input ()\n    return decorated\nCode Sample for predict.py Intent Prediction\nfrom flask import Flask, jsonify, request, Response\nfrom functools import wraps\nfrom util import validate_input, has_other_intent\nimport json, pickle, datetime, logging, uuid\nfrom typing import Dict\n\napplication = Flask(__name__)\n\ndef load_model() -&gt; None:\n    '''\n    Initialize the global variables to load the model.\n    '''\n    global vectorizer, model\n    vectorizer = pickle.load(open(\"./models/tfidf_vectorizer.pkl\", \"rb\")) \n    model = pickle.load(open(\"./models/intent_clf.pkl\", \"rb\"))     \n\ndef initialize ():\n    load_model()\n\ndef strip_accept_label(prediction:str) -&gt; str:\n    if '-' in prediction:\n        return prediction.split('-')[0]\n    return prediction\n\n@application.route(\"/introspect\", methods = [\"GET\"])\n@validate_input\ndef introspect() -&gt; Dict:\n    \"\"\"\n    Intent Prediction\n    \"\"\"\n    query = request.args ['query']\n    \n    # Return intent as OTHER for CVE, Errata (RHBA, RHSA), id or url\n    if has_other_intent(query):\n        response = {\"query\": query, \"intent\": 'OTHER', \"confidence_score\": 1, \n            \"req_id\": str(uuid.uuid4())}\n        logging.info(f\"Prediction response : {response}\")\n        return response\n \n    query_transformed = vectorizer.transform([query])\n    prediction = model.predict([query_transformed.toarray()[0]])[0]\n\n    prob = model.predict_proba([query_transformed.toarray()[0]])[0]\n    confidence_score = round(max(prob), 2)\n\n    response = {\"query\": query, \"intent\": strip_accept_label(prediction), \"confidence_score\": confidence_score, \n            \"req_id\": str(uuid.uuid4())}\n    \n    return response\n\ninitialize()\n\nif __name__ == \"__main__\":\n    initialize()\n    application.run(debug = True, host = \"0.0.0.0\", port = \"8080\")\nDockerfile sample\nFROM registry.access.redhat.com/rhscl/python-36-rhel7\n\nUSER root\n\nADD . /opt/customer-portal-search-intent/\n\nWORKDIR /opt/customer-portal-search-intent\n\nRUN wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/intent_clf-version1.pkl \\\n    && wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/intent_clf.pkl \\ \n    && wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/tfidf_vectorizer-version1.pkl \\ \n    && wget http://gss-rdu-repo.usersys.redhat.com/repo/config/intent-detection/tfidf_vectorizer.pkl \\ \n    && mv *.pkl models/ \\\n    && pip install -r requirements.txt\n\nUSER 1001\n\nEXPOSE 8080 8443\n\nENTRYPOINT [\"./run_intent_detection_service\"]\n\nCMD [ ]\n\nrun_intent_detection_service\n#!/bin/bash\n\n# Lay down the cert for this server\nKEYFILE=cert/rsa.key\nCERTFILE=cert/ssl.crt\nBIND=\"127.0.0.1:8080\"\n\nif [ -f $KEYFILE ] && [ -f $CERTFILE ] ; then\n   OPTS=\"--keyfile $KEYFILE --certfile $CERTFILE $OPTS\"\n   BIND=\"0.0.0.0:8443\"\nfi\n\n# num_workers = (2 * cpu) + 1 =&gt; 9 \nOPTS=\"$OPTS -b $BIND --workers 5 --log-level=DEBUG \"\n\nexport REQUESTS_CA_BUNDLE=$(pwd)/root.crt\n\nset -x\ngunicorn main:application $OPTS --access-logfile - --access-logformat  \"{'remote_ip':'%(h)s','request_id':'%({X-Request-Id}i)s','response_code':'%(s)s','request_method':'%(m)s','request_path':'%(U)s','request_querystring':'%(q)s','request_timetaken':'%(D)s','response_length':'%(B)s'}\"\n\n\nBuilding Image and Deployment on OpenShift\nIn order to make the service available to our users we are going to lean on OpenShift, container platform for building the image and deployment.\nOnce we have trained our model and have the application packaged in a Dockerfile, all we need to provide is BuildTemplate and DeploymentTemplate.\nBuildTemplate provides information such as source repository where the Dockerfile(assumed to be at the root), what buildstrategy to use(in this docker strategy) and finally about the ImageRepository for storing the built images.\nDeploymentTemplate contains info about cpu, memory requirements on the deployments, number of pods(instances of the services) for availablility and seamlessly transition between the instances during deployment rollouts without service interruption."
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#integration",
    "href": "posts/2020-09-20-intent-detection.html#integration",
    "title": "Intent detection",
    "section": "Integration",
    "text": "Integration\nLet‚Äôs see how this model is integrated with the overall product ecosystem.As a first step in using the service, we wanted to avoid any risks and carefully provide the options to the user to choose troubleshoot experience when we detect the same intent from the query.\nIn the below example user searching for kernel panic occuring in Red Hat Linux systems, the service predicted the TROUBLESHOOT with greater 70% confidence and a banner showing option to user allowing them to choose TROUBLESHOOT experience.\n\n\n\nRed Hat Customer Portal Search Integration"
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#conclusion",
    "href": "posts/2020-09-20-intent-detection.html#conclusion",
    "title": "Intent detection",
    "section": "Conclusion",
    "text": "Conclusion\n\nIn this post, we covered how we went from ideation, data collection, module building, deployment and finally integrating with the product.\nThere are constraints such as shorter text, latency, model size in choosing a modeling technique for intent classification and creating a simpler & traditional model such as LinearSVC can always be a better fit for such scenarios."
  },
  {
    "objectID": "posts/2020-09-20-intent-detection.html#references",
    "href": "posts/2020-09-20-intent-detection.html#references",
    "title": "Intent detection",
    "section": "References",
    "text": "References\n\nSearch: Intent, Not Inventory\nBroder, Andrei. (2002). A Taxonomy of Web Search. SIGIR Forum. 36. 3-10. 10.1145/792550.792552\nSondhi, Parikshit & Sharma, Mohit & Kolari, Pranam & Zhai, ChengXiang. (2018). A Taxonomy of Queries for E-commerce Search. 1245-1248. 10.1145/3209978.3210152\nDeep Search Query Intent Understanding\nDiscovering and Classifying In-app Message Intent at Airbnb\nProdigy - ML teaching and annotation tool\nSVM\nActive Learning\nOpenShift Builds 10.OpenShift Deployments"
  },
  {
    "objectID": "posts/2025-11-22-hybrid-search-solr.html",
    "href": "posts/2025-11-22-hybrid-search-solr.html",
    "title": "Hybrid Search in Apache Solr - Learning Notes",
    "section": "",
    "text": "This post explores hybrid search and reranking in Apache Solr. If you‚Äôre new to these concepts:\n\nKeyword search (also called lexical search) finds documents by matching exact words or phrases\nVector search (also called semantic search) finds documents by understanding meaning and similarity\nHybrid search combines both approaches to get the best of both worlds\nReranking is a technique where you first retrieve candidates using one method, then reorder them using another method\n\n\n\n\n\n\n\n\n\n\nNoteThe Problem with Single-Method Search\n\n\n\nImagine you‚Äôre searching for ‚Äúhow to fix memory leaks in Kubernetes‚Äù.\n\nKeyword search alone might miss relevant docs that use different terminology (e.g., ‚Äúmemory management‚Äù instead of ‚Äúmemory leaks‚Äù)\nVector search alone might return semantically similar but irrelevant docs (e.g., general memory management articles)\nReranking lets you use keyword search to find relevant candidates, then use vector search to surface the most semantically relevant ones\n\n\n\n\n\n\n\nBackground: Search practitioner, intermediate Python coder, familiar with lexical search in Solr\nGoal: Understand hybrid search and re-ranking features in Solr\nApplication: Lightspeed core implementation for OpenShift documentation\n\n\n\n\nThis implementation uses a keyword-first hybrid search strategy. Let‚Äôs break down what that means and how it works.\n\n\n\n\n\nUse traditional keyword search to find candidate documents\nRetrieve k*2 documents (twice as many as you need)\nThis acts as a filter: only documents matching your keywords are considered\n\n\n\n\n\nTake those k*2 candidates from Stage 1\nUse vector/semantic similarity to reorder them\nReturn the top k documents based on the combined score\n\n\n\n\n\nUser Query: \"how to deploy nodejs on openshift\"\n    ‚Üì\nStage 1: Keyword Search\n    ‚Üí Find top k*2 documents matching \"deploy\", \"nodejs\", \"openshift\"\n    ‚Üí Example: Gets 20 documents (if k=10)\n    ‚Üì\nStage 2: Semantic Reranking  \n    ‚Üí Calculate semantic similarity for those 20 documents\n    ‚Üí Reorder by combining keyword score + semantic score\n    ‚Üì\nFinal Results: Top k documents (10 in this case)\n\n\n\n\n\n\n\n\n\nTipThe k√ó2 Strategy\n\n\n\nRetrieving k*2 candidates gives the reranker a larger pool to work with. This is important because:\n\nThe keyword search might rank documents highly that aren‚Äôt semantically the best match\nThe reranker can ‚Äúrescue‚Äù semantically relevant documents that ranked lower in keyword search\nIt‚Äôs a balance: too few candidates = missed opportunities, too many = slower performance\n\n\n\n\n\n\nImagine you‚Äôre asking two librarians to help you find books:\n\nLibrarian #1 (Keyword Search):\n\nYou ask: ‚ÄúFind books about deploying applications‚Äù\nThey search the catalog by keywords and bring you 20 books\nThey put them on a table, roughly sorted by how many times ‚Äúdeploy‚Äù and ‚Äúapplication‚Äù appear\n\nLibrarian #2 (Vector Reranker):\n\nTakes those same 20 books from the table\nReads through them to understand the actual content and meaning\nReorders them based on how well they match what you‚Äôre really looking for\nGives you the top 10 most relevant books\n\n\nThe key insight: Librarian #2 can only work with what Librarian #1 found. If a book doesn‚Äôt match the keywords, it never makes it to the table.\n\n\n\n\nLightspeed implementation: solr_vector_io/solr.py\n\n\n\n\n\nNow let‚Äôs look at how this is actually implemented in code.\n\n\nasync def query_hybrid(\n    embedding: NDArray,           # Query vector (converted from text to numbers)\n    query_string: str,             # Original query text for keyword search\n    k: int,                        # Final number of results wanted\n    score_threshold: float,        # Minimum score to include a result\n    reranker_type: str,           # Type of reranking strategy\n    reranker_params: dict          # Contains boost values (reRankWeight, etc.)\n)\nKey inputs:\n\nembedding: The query converted to a vector (array of numbers) that represents its meaning\nquery_string: The original text query for keyword matching\nk: How many final results you want (e.g., 10)\nreranker_params: Configuration like reRankWeight that controls how much semantic similarity matters\n\n\n\n\nHere‚Äôs what gets sent to Solr:\ndata_params = {\n    # Stage 1: Initial keyword retrieval\n    \"q\": query_string,                    # Your keyword query (e.g., \"deploy nodejs\")\n    \"defType\": \"edismax\",                 # Extended DisMax parser (flexible keyword matching)\n    \"rows\": k,                            # Final result count (but we'll rerank k*2 first)\n    \n    # Stage 2: Reranking configuration\n    \"rq\": \"{{!rerank reRankQuery=$rqq reRankDocs={k*2} reRankWeight={vector_boost}}}\",\n    # rq = rerank query instruction\n    # reRankQuery=$rqq = use the query defined in rqq parameter\n    # reRankDocs={k*2} = rerank the top k*2 documents from keyword search\n    # reRankWeight={vector_boost} = how much to weight semantic score vs keyword score\n    \n    \"rqq\": \"{{!knn f={vector_field} topK={k*2}}}{vector_str}\",\n    # rqq = the actual rerank query (KNN = K-Nearest Neighbors, a vector similarity search)\n    # f={vector_field} = which field contains the document vectors\n    # topK={k*2} = consider top k*2 candidates\n    # {vector_str} = the query vector as a string\n    \n    # Other parameters\n    \"fl\": \"*, score\",                     # Return all fields + relevance score\n    \"fq\": [\"product:*openshift*\"],       # Filter query (only OpenShift docs)\n    \"wt\": \"json\"                          # Response format (JSON)\n}\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nWhat It Does\nExample Value\nWhy It Matters\n\n\n\n\nq\nThe keyword search query\n\"deploy nodejs openshift\"\nFinds initial candidates based on word matches\n\n\nrq\nRerank instruction\n\"{!rerank ...}\"\nTells Solr to rerank results\n\n\nreRankDocs\nHow many docs to rerank\n20 (if k=10)\nLarger pool = better reranking, but slower\n\n\nreRankQuery\nWhat to use for reranking\n$rqq (references rqq param)\nPoints to the vector similarity query\n\n\nreRankWeight\nSemantic score importance\n5.0 (medium)\nControls balance: low = keyword wins, high = semantic wins\n\n\nrqq\nThe vector similarity query\n\"{!knn f=vector topK=20}...\"\nPerforms semantic search on candidates\n\n\n\n\n\n\nThe reRankWeight parameter is crucial. It controls how the final score is calculated:\n\n\n\n\n\n\nImportantScore Formula\n\n\n\nfinal_score = keyword_score + (reRankWeight √ó semantic_score)\n\n\nExamples:\n\nreRankWeight = 1: Semantic score has equal weight to keyword score\nreRankWeight = 5: Semantic score is 5√ó more important (balanced approach)\nreRankWeight = 20: Semantic score dominates (for conceptual queries)\n\nWhy this matters: Different types of queries need different balances. A query like ‚ÄúCVE-2024-1234‚Äù needs exact keyword matching (low weight), while ‚Äúhow to improve security‚Äù benefits from semantic understanding (high weight).\n\n\n\n\nOne of the key insights from this implementation is that different query types need different reranking strategies. You can‚Äôt use the same reRankWeight for everything.\n\n\nConsider these three queries:\n\n\"CVE-2024-1234\" - You want the exact security advisory\n\"how to improve application performance\" - You want conceptually relevant guides\n\"how to patch CVE-2024-1234\" - You need both the exact CVE and conceptual guidance\n\nEach needs a different balance between keyword matching and semantic understanding.\n\n\n\n\n\n\nNoteQuick Strategy Reference\n\n\n\n\n\n\n\n\n\n\n\n\nStrategy\nreRankWeight\nBest For\nExample Queries\n\n\n\n\nExact Match\n1-2\nSpecific IDs, codes, commands\n\"CVE-2024-1234\", \"error 404\"\n\n\nBalanced\n5-8\nTech + action combinations\n\"deploy nodejs on k8s\"\n\n\nSemantic Heavy\n15-20\nConcepts, how-to, best practices\n\"how to improve performance\"\n\n\n\n\n\n\n\n\nWhen to use: Queries that require precise keyword matching\nExamples:\n\n\"CVE-2024-1234\" - Specific security advisory ID\n\"error code 404\" - Exact error code\n\"kubectl get pods\" - Specific command syntax\n\"API endpoint /v1/users\" - Exact API path\n\nStrategy: Low reRankWeight (1-2)\nWhy:\n\nThese queries have very specific, unambiguous intent\nExact keyword matches are more important than semantic similarity\nYou don‚Äôt want semantic search to ‚Äúhelpfully‚Äù return similar but different CVEs or error codes\nThe keyword search already finds the right documents; reranking should only make minor adjustments\n\nExample scenario:\nQuery: \"CVE-2024-1234\"\nKeyword search finds: Document about CVE-2024-1234 (score: 10.0)\n                      Document about CVE-2024-1235 (score: 8.0)  # Similar but wrong!\n                      \nWith low reRankWeight (1.0):\n- CVE-2024-1234 stays on top (keyword score dominates)\n- CVE-2024-1235 stays lower (even if semantically similar)\n\nWith high reRankWeight (20.0):\n- Risk: CVE-2024-1235 might jump ahead if it's semantically similar\n- Problem: User gets wrong CVE!\n\n\n\nWhen to use: Queries about concepts, best practices, or ‚Äúhow-to‚Äù questions\nExamples:\n\n\"how to improve performance\" - Broad conceptual question\n\"best practices for security\" - General guidance\n\"troubleshooting slow deployments\" - Problem-solving query\n\"scaling applications\" - Conceptual topic\n\nStrategy: High reRankWeight (15-20)\nWhy:\n\nThese queries are about concepts, not exact terms\nUsers might use different words than the documentation\nSemantic understanding helps find relevant content even if terminology differs\nKeyword search might miss relevant docs that use synonyms or related terms\n\nExample scenario:\nQuery: \"how to improve performance\"\nKeyword search finds: Doc mentioning \"improve performance\" (score: 9.0)\n                      Doc about \"optimization techniques\" (score: 6.0)  # Relevant but different words!\n                      \nWith low reRankWeight (1.0):\n- \"improve performance\" doc stays on top\n- \"optimization techniques\" stays lower (missed opportunity)\n\nWith high reRankWeight (20.0):\n- \"optimization techniques\" jumps ahead (semantically very relevant)\n- User gets better results!\n\n\n\nWhen to use: Queries that combine specific terms with conceptual needs\nExamples:\n\n\"how to patch CVE-2024-1234\" - Specific CVE + general patching guidance\n\"deploy nodejs on kubernetes\" - Specific technologies + deployment concept\n\"troubleshoot openshift authentication errors\" - Specific product + general troubleshooting\n\"configure SSL for nginx\" - Specific tech + configuration concept\n\nStrategy: Medium reRankWeight (5-8)\nWhy:\n\nNeed to match specific keywords (technology names, product names, error codes)\nBut also benefit from semantic understanding of the action/concept\nBalance ensures specific terms are matched while still finding conceptually relevant content\n\nExample scenario:\nQuery: \"deploy nodejs on kubernetes\"\nKeyword search finds: \"Deploying Node.js on Kubernetes\" (score: 10.0)\n                      \"Running Node.js apps in K8s\" (score: 7.0)  # Different words, same concept\n                      \nWith medium reRankWeight (6.0):\n- Both documents are considered\n- Exact match stays high, but semantic match can surface if very relevant\n- Good balance between precision and recall\n\n\n\n\n\n\n\n\n\nNoteQuick Reference: Choosing reRankWeight\n\n\n\nWhen choosing reRankWeight, ask yourself:\n\nIs this query about a specific, unambiguous thing? (CVE, error code, exact command)\n\n‚Üí Use low weight (1-2)\n\nIs this query about a concept or general topic? (how-to, best practices, troubleshooting)\n\n‚Üí Use high weight (15-20)\n\nDoes it combine specific terms with concepts? (specific tech + general action)\n\n‚Üí Use medium weight (5-8)\n\n\n\n\n\n\n\n\nNow that we understand the concepts, let‚Äôs see how to implement this in practice.\n\n\nInstead of trying to pick the perfect reRankWeight for every query, we can classify queries into three tiers:\n\n\n\n\n\n\n\n\n\nTier\nQuery Characteristics\nreRankWeight\nWhen to Use\n\n\n\n\nExact Match Critical\nSecurity IDs (CVE, Errata), error codes, exact commands\n1-2\nQueries that must match exact keywords\n\n\nBalanced\nTechnology + action combinations, mixed queries\n5-8\nDefault for most queries (covers majority of cases)\n\n\nSemantic Heavy\nQuestions, how-to guides, best practices, troubleshooting\n15-20\nConceptual queries where meaning matters most\n\n\n\n\n\n\nExample Classification Logic:\ndef classify_query(query: str) -&gt; str:\n    # Exact match critical: CVE, Errata, specific error codes\n    if re.search(r'CVE-\\d{4}-\\d+', query) or 'errata' in query.lower():\n        return \"exact_match\"\n    \n    # Semantic heavy: questions, how-to, best practices\n    if query.lower().startswith(('how', 'what', 'why', 'when')) or \\\n       'best practice' in query.lower() or 'troubleshoot' in query.lower():\n        return \"semantic_heavy\"\n    \n    # Default: balanced\n    return \"balanced\"\n\n# Map to reRankWeight\nweight_map = {\n    \"exact_match\": 1.5,\n    \"balanced\": 6.0,\n    \"semantic_heavy\": 18.0\n}\n\n\n\n\nExtend intent detection to classify queries into three tiers\n\nUse pattern matching (regex, keywords)\nLeverage existing intent detection if available\nStart simple, refine based on data\n\nMap each tier to reRankWeight value\n\nStart with suggested ranges (1-2, 5-8, 15-20)\nFine-tune based on your specific use case\n\nTest on historical query logs\n\nRun queries through both old and new systems\nCompare result quality (relevance, user satisfaction)\nMeasure performance impact\n\nMonitor and iterate\n\nTrack which queries get which classification\nCollect user feedback on result quality\nAdjust weights and classification rules based on data\n\n\n\n\n\n\n\n\n\n\n\nTipWhy Three Tiers Work\n\n\n\n\nPractical starting point: Three tiers cover most use cases without being too complex\nData-driven refinement: Start with defaults, improve based on real queries\nExplainable: Easy to understand why a query got a certain weight\nExtensible: Can add more tiers or dynamic weights later\n\n\n\n\n\n\nThis implementation uses keyword-first reranking, but there are other hybrid search strategies:\n\nUnion-based: Run keyword and vector search separately, merge results\nRRF (Reciprocal Rank Fusion): Combine rankings from multiple search methods\nLearning to Rank (LTR): Use machine learning to automatically optimize weights\nDynamic weights: Adjust reRankWeight based on query features (length, term frequency, etc.)\n\n\n\n\n\n\n\n\n\n\n\n\nImportantCore Concepts\n\n\n\n\nReranking is a two-stage process: Keyword search finds candidates, semantic search refines the ranking\nreRankWeight controls the balance: It determines how much semantic similarity matters vs.¬†keyword matching\n\nLow (1-2): Keyword matching dominates\nMedium (5-8): Balanced approach\nHigh (15-20): Semantic similarity dominates\n\nDifferent query types need different strategies:\n\nExact technical queries ‚Üí Low weight\nConceptual queries ‚Üí High weight\n\nMixed queries ‚Üí Medium weight\n\nStart simple, iterate based on data:\n\nThree tiers is a practical starting point\nRefine weights and classification rules based on real query performance\n\nThis is keyword-first hybrid:\n\nOnly documents matching keywords are considered\nReranking refines within that set\nThis is different from union-based approaches that merge separate results\n\nWhy retrieve k*2 candidates?\n\nGives reranker a larger pool to work with\nAllows semantically relevant docs to ‚Äúrescue‚Äù from lower keyword ranks\nBalance between quality and performance\n\n\n\n\n\n\n\n\n\nExperiment design: How to systematically test reranking strategies with query logs\nAlternative hybrid approaches: Union-based search, RRF (Reciprocal Rank Fusion)\nDynamic reRankWeight: Adjusting weights based on query features automatically\nLearning to Rank (LTR): Using machine learning to optimize reranking weights\nPerformance optimization: Balancing reranking quality with query latency\n\n\n\n\n\n\nSease.io blog: Hybrid Search with Apache Solr - Comprehensive guide to hybrid search concepts\nLightspeed implementation: solr_vector_io/solr.py - Real-world code example\nSolveit Dialog: Hybrid Search in Solr - Interactive learning resource ```"
  },
  {
    "objectID": "posts/2025-11-22-hybrid-search-solr.html#what-is-this-about",
    "href": "posts/2025-11-22-hybrid-search-solr.html#what-is-this-about",
    "title": "Hybrid Search in Apache Solr - Learning Notes",
    "section": "",
    "text": "This post explores hybrid search and reranking in Apache Solr. If you‚Äôre new to these concepts:\n\nKeyword search (also called lexical search) finds documents by matching exact words or phrases\nVector search (also called semantic search) finds documents by understanding meaning and similarity\nHybrid search combines both approaches to get the best of both worlds\nReranking is a technique where you first retrieve candidates using one method, then reorder them using another method"
  },
  {
    "objectID": "posts/2025-11-22-hybrid-search-solr.html#why-reranking-matters",
    "href": "posts/2025-11-22-hybrid-search-solr.html#why-reranking-matters",
    "title": "Hybrid Search in Apache Solr - Learning Notes",
    "section": "",
    "text": "NoteThe Problem with Single-Method Search\n\n\n\nImagine you‚Äôre searching for ‚Äúhow to fix memory leaks in Kubernetes‚Äù.\n\nKeyword search alone might miss relevant docs that use different terminology (e.g., ‚Äúmemory management‚Äù instead of ‚Äúmemory leaks‚Äù)\nVector search alone might return semantically similar but irrelevant docs (e.g., general memory management articles)\nReranking lets you use keyword search to find relevant candidates, then use vector search to surface the most semantically relevant ones"
  },
  {
    "objectID": "posts/2025-11-22-hybrid-search-solr.html#context-goal",
    "href": "posts/2025-11-22-hybrid-search-solr.html#context-goal",
    "title": "Hybrid Search in Apache Solr - Learning Notes",
    "section": "",
    "text": "Background: Search practitioner, intermediate Python coder, familiar with lexical search in Solr\nGoal: Understand hybrid search and re-ranking features in Solr\nApplication: Lightspeed core implementation for OpenShift documentation"
  },
  {
    "objectID": "posts/2025-11-22-hybrid-search-solr.html#understanding-the-reranking-approach",
    "href": "posts/2025-11-22-hybrid-search-solr.html#understanding-the-reranking-approach",
    "title": "Hybrid Search in Apache Solr - Learning Notes",
    "section": "",
    "text": "This implementation uses a keyword-first hybrid search strategy. Let‚Äôs break down what that means and how it works.\n\n\n\n\n\nUse traditional keyword search to find candidate documents\nRetrieve k*2 documents (twice as many as you need)\nThis acts as a filter: only documents matching your keywords are considered\n\n\n\n\n\nTake those k*2 candidates from Stage 1\nUse vector/semantic similarity to reorder them\nReturn the top k documents based on the combined score\n\n\n\n\n\nUser Query: \"how to deploy nodejs on openshift\"\n    ‚Üì\nStage 1: Keyword Search\n    ‚Üí Find top k*2 documents matching \"deploy\", \"nodejs\", \"openshift\"\n    ‚Üí Example: Gets 20 documents (if k=10)\n    ‚Üì\nStage 2: Semantic Reranking  \n    ‚Üí Calculate semantic similarity for those 20 documents\n    ‚Üí Reorder by combining keyword score + semantic score\n    ‚Üì\nFinal Results: Top k documents (10 in this case)\n\n\n\n\n\n\n\n\n\nTipThe k√ó2 Strategy\n\n\n\nRetrieving k*2 candidates gives the reranker a larger pool to work with. This is important because:\n\nThe keyword search might rank documents highly that aren‚Äôt semantically the best match\nThe reranker can ‚Äúrescue‚Äù semantically relevant documents that ranked lower in keyword search\nIt‚Äôs a balance: too few candidates = missed opportunities, too many = slower performance\n\n\n\n\n\n\nImagine you‚Äôre asking two librarians to help you find books:\n\nLibrarian #1 (Keyword Search):\n\nYou ask: ‚ÄúFind books about deploying applications‚Äù\nThey search the catalog by keywords and bring you 20 books\nThey put them on a table, roughly sorted by how many times ‚Äúdeploy‚Äù and ‚Äúapplication‚Äù appear\n\nLibrarian #2 (Vector Reranker):\n\nTakes those same 20 books from the table\nReads through them to understand the actual content and meaning\nReorders them based on how well they match what you‚Äôre really looking for\nGives you the top 10 most relevant books\n\n\nThe key insight: Librarian #2 can only work with what Librarian #1 found. If a book doesn‚Äôt match the keywords, it never makes it to the table.\n\n\n\n\nLightspeed implementation: solr_vector_io/solr.py"
  },
  {
    "objectID": "posts/2025-11-22-hybrid-search-solr.html#code-implementation-details",
    "href": "posts/2025-11-22-hybrid-search-solr.html#code-implementation-details",
    "title": "Hybrid Search in Apache Solr - Learning Notes",
    "section": "",
    "text": "Now let‚Äôs look at how this is actually implemented in code.\n\n\nasync def query_hybrid(\n    embedding: NDArray,           # Query vector (converted from text to numbers)\n    query_string: str,             # Original query text for keyword search\n    k: int,                        # Final number of results wanted\n    score_threshold: float,        # Minimum score to include a result\n    reranker_type: str,           # Type of reranking strategy\n    reranker_params: dict          # Contains boost values (reRankWeight, etc.)\n)\nKey inputs:\n\nembedding: The query converted to a vector (array of numbers) that represents its meaning\nquery_string: The original text query for keyword matching\nk: How many final results you want (e.g., 10)\nreranker_params: Configuration like reRankWeight that controls how much semantic similarity matters\n\n\n\n\nHere‚Äôs what gets sent to Solr:\ndata_params = {\n    # Stage 1: Initial keyword retrieval\n    \"q\": query_string,                    # Your keyword query (e.g., \"deploy nodejs\")\n    \"defType\": \"edismax\",                 # Extended DisMax parser (flexible keyword matching)\n    \"rows\": k,                            # Final result count (but we'll rerank k*2 first)\n    \n    # Stage 2: Reranking configuration\n    \"rq\": \"{{!rerank reRankQuery=$rqq reRankDocs={k*2} reRankWeight={vector_boost}}}\",\n    # rq = rerank query instruction\n    # reRankQuery=$rqq = use the query defined in rqq parameter\n    # reRankDocs={k*2} = rerank the top k*2 documents from keyword search\n    # reRankWeight={vector_boost} = how much to weight semantic score vs keyword score\n    \n    \"rqq\": \"{{!knn f={vector_field} topK={k*2}}}{vector_str}\",\n    # rqq = the actual rerank query (KNN = K-Nearest Neighbors, a vector similarity search)\n    # f={vector_field} = which field contains the document vectors\n    # topK={k*2} = consider top k*2 candidates\n    # {vector_str} = the query vector as a string\n    \n    # Other parameters\n    \"fl\": \"*, score\",                     # Return all fields + relevance score\n    \"fq\": [\"product:*openshift*\"],       # Filter query (only OpenShift docs)\n    \"wt\": \"json\"                          # Response format (JSON)\n}\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nWhat It Does\nExample Value\nWhy It Matters\n\n\n\n\nq\nThe keyword search query\n\"deploy nodejs openshift\"\nFinds initial candidates based on word matches\n\n\nrq\nRerank instruction\n\"{!rerank ...}\"\nTells Solr to rerank results\n\n\nreRankDocs\nHow many docs to rerank\n20 (if k=10)\nLarger pool = better reranking, but slower\n\n\nreRankQuery\nWhat to use for reranking\n$rqq (references rqq param)\nPoints to the vector similarity query\n\n\nreRankWeight\nSemantic score importance\n5.0 (medium)\nControls balance: low = keyword wins, high = semantic wins\n\n\nrqq\nThe vector similarity query\n\"{!knn f=vector topK=20}...\"\nPerforms semantic search on candidates\n\n\n\n\n\n\nThe reRankWeight parameter is crucial. It controls how the final score is calculated:\n\n\n\n\n\n\nImportantScore Formula\n\n\n\nfinal_score = keyword_score + (reRankWeight √ó semantic_score)\n\n\nExamples:\n\nreRankWeight = 1: Semantic score has equal weight to keyword score\nreRankWeight = 5: Semantic score is 5√ó more important (balanced approach)\nreRankWeight = 20: Semantic score dominates (for conceptual queries)\n\nWhy this matters: Different types of queries need different balances. A query like ‚ÄúCVE-2024-1234‚Äù needs exact keyword matching (low weight), while ‚Äúhow to improve security‚Äù benefits from semantic understanding (high weight)."
  },
  {
    "objectID": "posts/2025-11-22-hybrid-search-solr.html#choosing-the-right-reranking-strategy",
    "href": "posts/2025-11-22-hybrid-search-solr.html#choosing-the-right-reranking-strategy",
    "title": "Hybrid Search in Apache Solr - Learning Notes",
    "section": "",
    "text": "One of the key insights from this implementation is that different query types need different reranking strategies. You can‚Äôt use the same reRankWeight for everything.\n\n\nConsider these three queries:\n\n\"CVE-2024-1234\" - You want the exact security advisory\n\"how to improve application performance\" - You want conceptually relevant guides\n\"how to patch CVE-2024-1234\" - You need both the exact CVE and conceptual guidance\n\nEach needs a different balance between keyword matching and semantic understanding.\n\n\n\n\n\n\nNoteQuick Strategy Reference\n\n\n\n\n\n\n\n\n\n\n\n\nStrategy\nreRankWeight\nBest For\nExample Queries\n\n\n\n\nExact Match\n1-2\nSpecific IDs, codes, commands\n\"CVE-2024-1234\", \"error 404\"\n\n\nBalanced\n5-8\nTech + action combinations\n\"deploy nodejs on k8s\"\n\n\nSemantic Heavy\n15-20\nConcepts, how-to, best practices\n\"how to improve performance\"\n\n\n\n\n\n\n\n\nWhen to use: Queries that require precise keyword matching\nExamples:\n\n\"CVE-2024-1234\" - Specific security advisory ID\n\"error code 404\" - Exact error code\n\"kubectl get pods\" - Specific command syntax\n\"API endpoint /v1/users\" - Exact API path\n\nStrategy: Low reRankWeight (1-2)\nWhy:\n\nThese queries have very specific, unambiguous intent\nExact keyword matches are more important than semantic similarity\nYou don‚Äôt want semantic search to ‚Äúhelpfully‚Äù return similar but different CVEs or error codes\nThe keyword search already finds the right documents; reranking should only make minor adjustments\n\nExample scenario:\nQuery: \"CVE-2024-1234\"\nKeyword search finds: Document about CVE-2024-1234 (score: 10.0)\n                      Document about CVE-2024-1235 (score: 8.0)  # Similar but wrong!\n                      \nWith low reRankWeight (1.0):\n- CVE-2024-1234 stays on top (keyword score dominates)\n- CVE-2024-1235 stays lower (even if semantically similar)\n\nWith high reRankWeight (20.0):\n- Risk: CVE-2024-1235 might jump ahead if it's semantically similar\n- Problem: User gets wrong CVE!\n\n\n\nWhen to use: Queries about concepts, best practices, or ‚Äúhow-to‚Äù questions\nExamples:\n\n\"how to improve performance\" - Broad conceptual question\n\"best practices for security\" - General guidance\n\"troubleshooting slow deployments\" - Problem-solving query\n\"scaling applications\" - Conceptual topic\n\nStrategy: High reRankWeight (15-20)\nWhy:\n\nThese queries are about concepts, not exact terms\nUsers might use different words than the documentation\nSemantic understanding helps find relevant content even if terminology differs\nKeyword search might miss relevant docs that use synonyms or related terms\n\nExample scenario:\nQuery: \"how to improve performance\"\nKeyword search finds: Doc mentioning \"improve performance\" (score: 9.0)\n                      Doc about \"optimization techniques\" (score: 6.0)  # Relevant but different words!\n                      \nWith low reRankWeight (1.0):\n- \"improve performance\" doc stays on top\n- \"optimization techniques\" stays lower (missed opportunity)\n\nWith high reRankWeight (20.0):\n- \"optimization techniques\" jumps ahead (semantically very relevant)\n- User gets better results!\n\n\n\nWhen to use: Queries that combine specific terms with conceptual needs\nExamples:\n\n\"how to patch CVE-2024-1234\" - Specific CVE + general patching guidance\n\"deploy nodejs on kubernetes\" - Specific technologies + deployment concept\n\"troubleshoot openshift authentication errors\" - Specific product + general troubleshooting\n\"configure SSL for nginx\" - Specific tech + configuration concept\n\nStrategy: Medium reRankWeight (5-8)\nWhy:\n\nNeed to match specific keywords (technology names, product names, error codes)\nBut also benefit from semantic understanding of the action/concept\nBalance ensures specific terms are matched while still finding conceptually relevant content\n\nExample scenario:\nQuery: \"deploy nodejs on kubernetes\"\nKeyword search finds: \"Deploying Node.js on Kubernetes\" (score: 10.0)\n                      \"Running Node.js apps in K8s\" (score: 7.0)  # Different words, same concept\n                      \nWith medium reRankWeight (6.0):\n- Both documents are considered\n- Exact match stays high, but semantic match can surface if very relevant\n- Good balance between precision and recall\n\n\n\n\n\n\n\n\n\nNoteQuick Reference: Choosing reRankWeight\n\n\n\nWhen choosing reRankWeight, ask yourself:\n\nIs this query about a specific, unambiguous thing? (CVE, error code, exact command)\n\n‚Üí Use low weight (1-2)\n\nIs this query about a concept or general topic? (how-to, best practices, troubleshooting)\n\n‚Üí Use high weight (15-20)\n\nDoes it combine specific terms with concepts? (specific tech + general action)\n\n‚Üí Use medium weight (5-8)"
  },
  {
    "objectID": "posts/2025-11-22-hybrid-search-solr.html#putting-it-all-together-a-practical-implementation-plan",
    "href": "posts/2025-11-22-hybrid-search-solr.html#putting-it-all-together-a-practical-implementation-plan",
    "title": "Hybrid Search in Apache Solr - Learning Notes",
    "section": "",
    "text": "Now that we understand the concepts, let‚Äôs see how to implement this in practice.\n\n\nInstead of trying to pick the perfect reRankWeight for every query, we can classify queries into three tiers:\n\n\n\n\n\n\n\n\n\nTier\nQuery Characteristics\nreRankWeight\nWhen to Use\n\n\n\n\nExact Match Critical\nSecurity IDs (CVE, Errata), error codes, exact commands\n1-2\nQueries that must match exact keywords\n\n\nBalanced\nTechnology + action combinations, mixed queries\n5-8\nDefault for most queries (covers majority of cases)\n\n\nSemantic Heavy\nQuestions, how-to guides, best practices, troubleshooting\n15-20\nConceptual queries where meaning matters most\n\n\n\n\n\n\nExample Classification Logic:\ndef classify_query(query: str) -&gt; str:\n    # Exact match critical: CVE, Errata, specific error codes\n    if re.search(r'CVE-\\d{4}-\\d+', query) or 'errata' in query.lower():\n        return \"exact_match\"\n    \n    # Semantic heavy: questions, how-to, best practices\n    if query.lower().startswith(('how', 'what', 'why', 'when')) or \\\n       'best practice' in query.lower() or 'troubleshoot' in query.lower():\n        return \"semantic_heavy\"\n    \n    # Default: balanced\n    return \"balanced\"\n\n# Map to reRankWeight\nweight_map = {\n    \"exact_match\": 1.5,\n    \"balanced\": 6.0,\n    \"semantic_heavy\": 18.0\n}\n\n\n\n\nExtend intent detection to classify queries into three tiers\n\nUse pattern matching (regex, keywords)\nLeverage existing intent detection if available\nStart simple, refine based on data\n\nMap each tier to reRankWeight value\n\nStart with suggested ranges (1-2, 5-8, 15-20)\nFine-tune based on your specific use case\n\nTest on historical query logs\n\nRun queries through both old and new systems\nCompare result quality (relevance, user satisfaction)\nMeasure performance impact\n\nMonitor and iterate\n\nTrack which queries get which classification\nCollect user feedback on result quality\nAdjust weights and classification rules based on data\n\n\n\n\n\n\n\n\n\n\n\nTipWhy Three Tiers Work\n\n\n\n\nPractical starting point: Three tiers cover most use cases without being too complex\nData-driven refinement: Start with defaults, improve based on real queries\nExplainable: Easy to understand why a query got a certain weight\nExtensible: Can add more tiers or dynamic weights later\n\n\n\n\n\n\nThis implementation uses keyword-first reranking, but there are other hybrid search strategies:\n\nUnion-based: Run keyword and vector search separately, merge results\nRRF (Reciprocal Rank Fusion): Combine rankings from multiple search methods\nLearning to Rank (LTR): Use machine learning to automatically optimize weights\nDynamic weights: Adjust reRankWeight based on query features (length, term frequency, etc.)"
  },
  {
    "objectID": "posts/2025-11-22-hybrid-search-solr.html#key-takeaways",
    "href": "posts/2025-11-22-hybrid-search-solr.html#key-takeaways",
    "title": "Hybrid Search in Apache Solr - Learning Notes",
    "section": "",
    "text": "ImportantCore Concepts\n\n\n\n\nReranking is a two-stage process: Keyword search finds candidates, semantic search refines the ranking\nreRankWeight controls the balance: It determines how much semantic similarity matters vs.¬†keyword matching\n\nLow (1-2): Keyword matching dominates\nMedium (5-8): Balanced approach\nHigh (15-20): Semantic similarity dominates\n\nDifferent query types need different strategies:\n\nExact technical queries ‚Üí Low weight\nConceptual queries ‚Üí High weight\n\nMixed queries ‚Üí Medium weight\n\nStart simple, iterate based on data:\n\nThree tiers is a practical starting point\nRefine weights and classification rules based on real query performance\n\nThis is keyword-first hybrid:\n\nOnly documents matching keywords are considered\nReranking refines within that set\nThis is different from union-based approaches that merge separate results\n\nWhy retrieve k*2 candidates?\n\nGives reranker a larger pool to work with\nAllows semantically relevant docs to ‚Äúrescue‚Äù from lower keyword ranks\nBalance between quality and performance"
  },
  {
    "objectID": "posts/2025-11-22-hybrid-search-solr.html#next-learning-topics",
    "href": "posts/2025-11-22-hybrid-search-solr.html#next-learning-topics",
    "title": "Hybrid Search in Apache Solr - Learning Notes",
    "section": "",
    "text": "Experiment design: How to systematically test reranking strategies with query logs\nAlternative hybrid approaches: Union-based search, RRF (Reciprocal Rank Fusion)\nDynamic reRankWeight: Adjusting weights based on query features automatically\nLearning to Rank (LTR): Using machine learning to optimize reranking weights\nPerformance optimization: Balancing reranking quality with query latency"
  },
  {
    "objectID": "posts/2025-11-22-hybrid-search-solr.html#reference-materials",
    "href": "posts/2025-11-22-hybrid-search-solr.html#reference-materials",
    "title": "Hybrid Search in Apache Solr - Learning Notes",
    "section": "",
    "text": "Sease.io blog: Hybrid Search with Apache Solr - Comprehensive guide to hybrid search concepts\nLightspeed implementation: solr_vector_io/solr.py - Real-world code example\nSolveit Dialog: Hybrid Search in Solr - Interactive learning resource ```"
  },
  {
    "objectID": "posts/2025-11-27-stop-operating-blind.html",
    "href": "posts/2025-11-27-stop-operating-blind.html",
    "title": "Stop Operating Blind: Why AI Teams Need a ‚ÄòDiagnostician‚Äô Mindset",
    "section": "",
    "text": "You‚Äôre reviewing user feedback on your agentic RAG system. The complaints are piling up:\n\n‚ÄúThe answer was inaccurate.‚Äù\n‚ÄúThe information is outdated.‚Äù\n‚ÄúIt didn‚Äôt solve my issue.‚Äù\n\nAs an AI or QE engineer, your ‚ÄúBuilder‚Äù instinct kicks in. Maybe we need a better model. Maybe we need to add more documents. Maybe we need a multi-agent system. These are tangible actions, things we can control.\nBut here‚Äôs the problem: these user complaints are symptoms, not the disease.\n‚ÄúInaccurate‚Äù could mean retrieval pulled the wrong document, or the model hallucinated, or the prompt was bad. ‚ÄúOutdated‚Äù could mean stale content, or the user is asking about a product that isn‚Äôt even in your system.\nThis is the ‚ÄúBuilder‚Äôs Trap‚Äù: jumping to solutions before we understand the actual problem. This natural instinct often leads teams further away from fixing what‚Äôs really broken."
  },
  {
    "objectID": "posts/2025-11-27-stop-operating-blind.html#stop-operating-blind-why-ai-teams-need-a-diagnostician-mindset",
    "href": "posts/2025-11-27-stop-operating-blind.html#stop-operating-blind-why-ai-teams-need-a-diagnostician-mindset",
    "title": "Stop Operating Blind: Why AI Teams Need a ‚ÄòDiagnostician‚Äô Mindset",
    "section": "",
    "text": "You‚Äôre reviewing user feedback on your agentic RAG system. The complaints are piling up:\n\n‚ÄúThe answer was inaccurate.‚Äù\n‚ÄúThe information is outdated.‚Äù\n‚ÄúIt didn‚Äôt solve my issue.‚Äù\n\nAs an AI or QE engineer, your ‚ÄúBuilder‚Äù instinct kicks in. Maybe we need a better model. Maybe we need to add more documents. Maybe we need a multi-agent system. These are tangible actions, things we can control.\nBut here‚Äôs the problem: these user complaints are symptoms, not the disease.\n‚ÄúInaccurate‚Äù could mean retrieval pulled the wrong document, or the model hallucinated, or the prompt was bad. ‚ÄúOutdated‚Äù could mean stale content, or the user is asking about a product that isn‚Äôt even in your system.\nThis is the ‚ÄúBuilder‚Äôs Trap‚Äù: jumping to solutions before we understand the actual problem. This natural instinct often leads teams further away from fixing what‚Äôs really broken."
  },
  {
    "objectID": "posts/2025-11-27-stop-operating-blind.html#the-two-paths-why-were-flying-blind",
    "href": "posts/2025-11-27-stop-operating-blind.html#the-two-paths-why-were-flying-blind",
    "title": "Stop Operating Blind: Why AI Teams Need a ‚ÄòDiagnostician‚Äô Mindset",
    "section": "The Two Paths: Why We‚Äôre ‚ÄúFlying Blind‚Äù",
    "text": "The Two Paths: Why We‚Äôre ‚ÄúFlying Blind‚Äù\nThis reactive approach stems from a deeper issue. Most AI teams are ‚Äúflying blind‚Äù by focusing on the wrong signals. It‚Äôs easy to fall into the tool-first trap (debating vector databases) or rely on generic evals (like RAGAS) for a false sense of security.\nThese generic model evaluations are essential health checks, but a dashboard showing your hallucination score improved from 0.85 to 0.88 doesn‚Äôt tell you if you‚Äôve solved the user‚Äôs problem.\nThis creates two very different paths for an engineering team:\n\n\n\n\n\n\n\n\nAspect\nPath 1: The Builder‚Äôs Trap (Operating Blind)\nPath 2: The Diagnostician‚Äôs Path (Gaining Clarity)\n\n\n\n\nSymptom\nVague user complaint (‚ÄúInaccurate‚Äù)\nVague user complaint (‚ÄúInaccurate‚Äù)\n\n\nAction\nJump to a tangible solution (‚ÄúSwap Model!‚Äù)\nSystematic 4-Step Error Analysis\n\n\nMeasurement\nRely on generic evals (‚ÄúRAGAS score is 0.88‚Äù)\nA specific, quantified diagnosis (‚Äú40% of failures are ‚ÄòIncorrect Product Focus‚Äô‚Äù)\n\n\nOutcome\nWe‚Äôre still operating blind, and users are still unhappy.\nA targeted fix that measurably improves the real user-facing problem."
  },
  {
    "objectID": "posts/2025-11-27-stop-operating-blind.html#the-solution-trade-the-hard-hat-for-a-lab-coat",
    "href": "posts/2025-11-27-stop-operating-blind.html#the-solution-trade-the-hard-hat-for-a-lab-coat",
    "title": "Stop Operating Blind: Why AI Teams Need a ‚ÄòDiagnostician‚Äô Mindset",
    "section": "The Solution: Trade the Hard-Hat for a Lab Coat",
    "text": "The Solution: Trade the Hard-Hat for a Lab Coat\nThe solution is to adopt a new identity. We must shift from being just ‚ÄúBuilders‚Äù to also being ‚ÄúDiagnosticians.‚Äù\nInstead of operating blind, we start by running analysis and tests, just as a physician would order labs before making a diagnosis. This diagnostic approach follows a four-step process borrowed from qualitative research.\n\nStep 1: Annotate Failures (Open Coding)\nLike a doctor reviewing symptoms, we start by examining individual user interactions. A domain expert reviews a trace and writes a free-form critique: What went wrong and why?\nExample Annotation:\n\n\n\n\n\n\n\nField\nValue\n\n\n\n\nTrace ID\n#47\n\n\nUser Query\n‚ÄúHow do I configure SSO for Product A?‚Äù\n\n\nSystem Response\nRetrieved documentation for Product B deployment on Azure\n\n\nPass/Fail\n‚ùå Fail\n\n\nAnnotation (detailed enough for a new teammate to understand)\n‚ÄúUser asked about Product A (SSO), but retrieval returned docs for Product B. The retrieved content is technically accurate but completely irrelevant to the user‚Äôs actual question.‚Äù\n\n\n\n\n\nStep 2: Categorize into Failure Modes (Axial Coding)\nOnce you have dozens of annotations, patterns emerge. You group similar failures, transforming scattered observations into a clear taxonomy of failure modes. This is where you move from ‚Äúusers say it‚Äôs inaccurate‚Äù to discovering and naming the specific types of failures.\nExample ‚Äî From Annotations to Taxonomy:\nAfter reviewing 70+ traces, you notice patterns in your annotations:\n\n\n\n\n\n\n\nRaw Annotations (Step 1)\nFailure Mode Category (Step 2)\n\n\n\n\n‚ÄúReturned docs for Product B instead of Product A‚Äù\nIncorrect Product Focus\n\n\n‚ÄúUser asked about Ansible, got Keycloak docs‚Äù\nIncorrect Product Focus\n\n\n‚ÄúRelevant doc exists but system said ‚ÄòNo Sources Found‚Äô‚Äù\nRetrieval Recall Failure\n\n\n‚ÄúFound right doc but only used intro section, missed solution in Chapter 3‚Äù\nIncomplete Documentation Context\n\n\n\nYou‚Äôve now transformed vague ‚Äúinaccurate‚Äù complaints into a named taxonomy you can measure and fix.\n\n\nStep 3: Quantify and Validate (Selective Coding)\nAfter identifying your failure modes, you go back and count them. How prevalent is each problem? This confirms your diagnosis and reveals which diseases are most critical.\nExample ‚Äî Quantifying Your Failure Modes:\nAfter labeling all 70 traces against your taxonomy:\n\n\n\nFailure Mode\nCount\n% of Failures\n\n\n\n\nMissing Tool Call\n15\n21%\n\n\nIncorrect Product Focus\n12\n17%\n\n\nRetrieval Recall Failure\n10\n14%\n\n\nContext Misprocessing\n8\n11%\n\n\nOutdated Information\n6\n9%\n\n\nOther (5 categories)\n19\n28%\n\n\n\nNow you know: ‚ÄúMissing Tool Call‚Äù and ‚ÄúIncorrect Product Focus‚Äù account for nearly 40% of all failures. These are your highest-leverage targets.\n\n\nStep 4: Prioritize and Fix (Targeted Interventions)\nWith a confirmed, quantified diagnosis, you can prescribe the right treatment. Instead of generic solutions, you create targeted experiments.\nExample ‚Äî From Diagnosis to Action Plan:\n\n\n\n\n\n\n\n\nFailure Mode\n% of Failures\nTargeted Fix\n\n\n\n\nMissing Tool Call\n21%\nRefine tool-triggering logic; add query patterns that should always invoke search\n\n\nIncorrect Product Focus\n17%\nImprove retrieval filtering; add product-name extraction to query preprocessing\n\n\nRetrieval Recall Failure\n14%\nAudit content coverage gaps; expand indexing to include missing documentation\n\n\n\nEach fix now directly addresses a quantified, user-facing problem ‚Äî not a hunch."
  },
  {
    "objectID": "posts/2025-11-27-stop-operating-blind.html#our-aha-moment-the-disease-we-couldnt-see",
    "href": "posts/2025-11-27-stop-operating-blind.html#our-aha-moment-the-disease-we-couldnt-see",
    "title": "Stop Operating Blind: Why AI Teams Need a ‚ÄòDiagnostician‚Äô Mindset",
    "section": "Our ‚ÄúAha!‚Äù Moment: The Disease We Couldn‚Äôt See",
    "text": "Our ‚ÄúAha!‚Äù Moment: The Disease We Couldn‚Äôt See\nThis process gives you surprising, critical insights. When our team ran through ‚ÄúCategorize into Failure Modes,‚Äù we discovered a failure mode we hadn‚Äôt even considered: ‚ÄúIncorrect Product Focus.‚Äù\nDefinition: The assistant retrieves a solution for a different product than the one the user asked about.\n\nQuery: ‚ÄúHow do I configure SSO for Product A?‚Äù\nResponse: A setup guide for Product B‚Äôs authentication system.\n\nThe system wasn‚Äôt technically broken. It called the tool, retrieved a source, and generated a fluent answer. But the source was for the wrong product entirely.\nA generic, automated eval would have missed this. It might have even scored the answer as ‚Äúhigh quality‚Äù because it was factually grounded in the retrieved (but irrelevant) source.\nThis single, human-driven discovery proved the value of the entire process. It gave us a high-priority, user-facing problem that we could never have found by just looking at generic dashboards."
  },
  {
    "objectID": "posts/2025-11-27-stop-operating-blind.html#how-to-do-this-overcoming-the-its-too-manual-objection",
    "href": "posts/2025-11-27-stop-operating-blind.html#how-to-do-this-overcoming-the-its-too-manual-objection",
    "title": "Stop Operating Blind: Why AI Teams Need a ‚ÄòDiagnostician‚Äô Mindset",
    "section": "How to Do This: Overcoming the ‚ÄúIt‚Äôs Too Manual‚Äù Objection",
    "text": "How to Do This: Overcoming the ‚ÄúIt‚Äôs Too Manual‚Äù Objection\nThe most common objection is: ‚ÄúThis sounds time-consuming and manual. How do we scale it?‚Äù\nThis is where we must be pragmatic. Don‚Äôt let the lack of perfect tooling prevent you from beginning the diagnostic process.\n\nHow to Start Today (With No Tooling)\nStart with a spreadsheet. This is how we began. Each row was a trace, with columns for the user query, AI response, pass/fail judgment, and our free-form annotations (detailed enough for a new teammate to understand).\nWas it elegant? No.¬†Did it work? Absolutely. We reviewed 70+ traces, built our initial failure mode taxonomy, and had a prioritized backlog in a matter of days. Starting simple with a spreadsheet beats operating blind.\n\n\nHow We Evolved: Custom Tooling for Multi-Turn Conversations\nAs we scaled, two pain points emerged:\n\nMulti-turn conversations are hard to review in spreadsheets. Long back-and-forth dialogues require constant scrolling and context-switching.\nAnnotation is time-consuming. Even with a clear taxonomy, writing detailed annotations for every trace takes effort.\n\nWe built a custom annotation interface that addressed both:\n\nTurn-level review: Each turn in a multi-turn conversation can be annotated individually, making it easier to pinpoint exactly where the system failed.\nLLM-suggested annotations: The tool prompts an LLM with our 11-mode failure taxonomy and the conversation trace. The LLM suggests an annotation as a starting point.\n\n\n\nThe Human + LLM Partnership (Not Replacement)\nHere‚Äôs the critical part: the LLM suggests, the human decides.\nThe workflow looks like this:\n\nLLM generates a suggested annotation based on the trace and the failure mode taxonomy\nHuman reviewer reads the suggestion ‚Äî it might be 80% correct, or it might miss a nuance\nHuman accepts, edits, or overwrites ‚Äî adding domain-specific insight the LLM can‚Äôt provide\n\nWhy this works: The LLM reduces the ‚Äúblank page‚Äù problem and speeds up annotation, but the human stays in control. You‚Äôre not blindly trusting the LLM‚Äôs judgment ‚Äî you‚Äôre using it to reduce context-switching and cognitive load.\nThis keeps the process scalable while preserving the quality and insight that only human domain experts can provide."
  },
  {
    "objectID": "posts/2025-11-27-stop-operating-blind.html#the-mindset-that-fixes-ai",
    "href": "posts/2025-11-27-stop-operating-blind.html#the-mindset-that-fixes-ai",
    "title": "Stop Operating Blind: Why AI Teams Need a ‚ÄòDiagnostician‚Äô Mindset",
    "section": "The Mindset That Fixes AI",
    "text": "The Mindset That Fixes AI\nYou can keep cranking out features, but if you don‚Äôt shift your team‚Äôs focus from tools to measurement, you‚Äôll be operating blind. The danger is real: you‚Äôll prioritize new capabilities while unaddressed failure modes compound, making your product progressively worse until users abandon it.\nThe mindset shift is simple but profound: move from Builder mode to Diagnostician mode.\nStart by understanding the failures your users are actually facing. Establish a regular cadence to turn those complaints into a failure mode taxonomy. This diagnostic process is the foundation for everything. Once you have it, you can build custom evaluations that matter, monitor them in production, and catch regressions before they become user complaints.\nThe best AI engineers aren‚Äôt the ones with the fanciest architectures. They‚Äôre the ones who know how to systematically diagnose what‚Äôs broken and prove they‚Äôve fixed it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chrestotes",
    "section": "",
    "text": "Stop Operating Blind: Why AI Teams Need a ‚ÄòDiagnostician‚Äô Mindset\n\n\n\nAI\n\nproduct-management\n\nAI Evals\n\n\n\nA practical guide for AI and product teams to move beyond reactive fixes‚Äîlearn why recurring model and retrieval tweaks don‚Äôt solve user complaints, and discover a four-step diagnostic approach to systematically identify, categorize, and address the real causes of error in agentic RAG systems.\n\n\n\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHybrid Search in Apache Solr - Learning Notes\n\n\n\ninformation-retrieval\n\napache-solr\n\nsearch\n\n\n\nUnderstanding hybrid search and reranking strategies in Apache Solr\n\n\n\n\n\nNov 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDocument Expansion by Query Prediction to Improve Retrieval Effectiveness\n\n\n\ninformation-retrieval\n\nmachine-learning\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification with fastai‚Äôs MidLevel API\n\n\n\nimage-classification\n\nfastai\n\nmachine-learning\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPoetry First Impressions\n\n\n\npython\n\nreproducibility\n\n\n\nGetting started with poetry for dependency management & packaging.\n\n\n\n\n\nJan 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nIntent detection\n\n\n\nintent-detection\n\ntext-classification\n\ninformation-retrieval\n\nquery-understanding\n\nmachine-learning\n\n\n\nBlog post about end to end text classification application for detecting intent in Red Hat Customer Portal search queries using machine learning and integrating the intent detection as part of user journey.\n\n\n\n\n\nSep 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nTracking Data and Model in Machine Learning projects\n\n\n\nmachine-learning\n\n\n\nDVC as tool for model and data versioning and create reproducible ML project.\n\n\n\n\n\nJun 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset\n\n\n\ninformation-retrieval\n\ndeep-learning\n\npapers\n\n\n\nPaper Summary of covidex.ai\n\n\n\n\n\nMay 25, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html",
    "href": "posts/2023-04-14-fashion-mnist.html",
    "title": "Image Classification with fastai‚Äôs MidLevel API",
    "section": "",
    "text": "In this blog post, we will explore a toy classification example using the Fashion MNIST dataset. We will discuss the limitations of the fastai DataBlock API and how learning about the Mid-level API can help overcome these limitations using Transforms, Pipeline, and Datasets. We will also cover the debugging steps involved in creating the Dataloaders for training image classifier & deploy the app using Gradio & HuggingFace spaces.\nSee the complete app Code & Space"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#context",
    "href": "posts/2023-04-14-fashion-mnist.html#context",
    "title": "Image Classification with fastai‚Äôs MidLevel API",
    "section": "Context",
    "text": "Context\nWhen working with the fastai library, we may encounter situations where the Data block API is not flexible enough to handle our specific data processing needs. In such cases, we can use fastai‚Äôs mid-level API, which provides a more comprehensive set of tools for processing data. The mid-level API includes a range of features such as Transforms, Pipeline, TfmdLists, Datasets, Callbacks, and General Optimizer. By using these, we can overcome the limitations of the Data block API and create more customized data processing pipelines to suit our specific use case.\nTo read more about Mid-Level API, please refer Chapter 11 - Data Munging with fastai‚Äôs Mid-Level API in fastbook.\n\n\n\nfastai - a Layered API\n\n\nIn this post, I wanted to share a toy classification example using Fashion MNIST dataset where the DataBlock API is not flexibile and how learning about Mid-level API using Transforms, Pipeline and Datasets helped to create the Dataloaders for training the model. We will use the model to create a image classifier predicting the target class given an black & white image of a fashion apparel using Gradio and HuggingFace spaces. We will also cover the debugging steps involved at the relevant step.\nThe end to end workflow is as follows"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#exploring-the-dataset",
    "href": "posts/2023-04-14-fashion-mnist.html#exploring-the-dataset",
    "title": "Image Classification with fastai‚Äôs MidLevel API",
    "section": "Exploring the Dataset",
    "text": "Exploring the Dataset\n\nDownload and load dataset with the name fashion_mnist from https://huggingface.co/datasets/fashion_mnist using HuggingFace datasets library.\n\n\nFashion-MNIST is a dataset of Zalando‚Äôs article images‚Äîconsisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 apparels such as t-shirt, ankle boot,\n\n\nExplore the dataset using load_dataset_builder inspecting the dataset info such as description, features, splits etc\n\n\nname='fashion_mnist'\nds_builder = load_dataset_builder(name)\n\n\nprint(ds_builder.info.description)\n\nFashion-MNIST is a dataset of Zalando's article images‚Äîconsisting of a training set of\n60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\nassociated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\nreplacement for the original MNIST dataset for benchmarking machine learning algorithms.\nIt shares the same image size and structure of training and testing splits.\n\n\n\n\nprint(ds_builder.info.features)\n\n{'image': Image(decode=True, id=None), 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\n\nprint(ds_builder.info.splits)\n\n{'train': SplitInfo(name='train', num_bytes=31296655, num_examples=60000, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=5233818, num_examples=10000, shard_lengths=None, dataset_name=None)}\n\n\n\nds_builder.info.features['label']\n\nClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)\n\n\n\nds_builder.info.features['label'].int2str(9)\n\n'Ankle boot'\n\n\n\nLoad the dataset from the Hugging Face Hub specifying the name.\n\n\ndset = load_dataset(name)\n\nThis is a DatasetDict containing a train and test dataset dictionary within this object.\n\ndset\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\n\nWe can inspect the individual item within train and test and the different labels.\n\n\ndset['train'][0], dset['test'][0]\n\n({'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n  'label': 9},\n {'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n  'label': 9})"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#create-fastai-dataloaders",
    "href": "posts/2023-04-14-fashion-mnist.html#create-fastai-dataloaders",
    "title": "Image Classification with fastai‚Äôs MidLevel API",
    "section": "Create fastai DataLoaders",
    "text": "Create fastai DataLoaders\nWe eventually want to create Datasets object in fastai from HuggingFace Dataset. We will first attempt with the the high level DataBlock API and then transition to fastai Datasets.\n\nDataBlock is a high level API in fastai allowing the user to define the standard steps to prepare the data for deep learning model.\n\nSteps involved to prepare the data - Identify the types of inputs/targets for your data and define them as ‚ÄúBlocks‚Äù. - Specify how to fetch and define any transformations that need to be applied to the inputs using the ‚Äúget_x‚Äù function. - Specify how to fetch and define any transformations that need to be applied to the targets using the ‚Äúget_y‚Äù function. - Split the data into training and validation sets using the ‚Äúsplitter‚Äù function. - Apply any additional transformations to the items using the ‚Äúitem_tfms‚Äù function. - Apply any additional transformations to the batches using the ‚Äúbatch_tfms‚Äù function.\nLet‚Äôs create training and test sets.\n\ntrain, test = dset['train'], dset['test']\n\nCreate an Image from argument using PILImageBW.create function\n\nim = PILImageBW.create(train[0]['image'])\n\nDisplay the image using show method\n\nim.show()\n\n\n\n\n\n\n\n\nLet‚Äôs examine the type of the features (ie Image) and label\n\ntype(train[0]['image'])\n\nPIL.PngImagePlugin.PngImageFile\n\n\nLet‚Äôs examine the first 3 training samples\n\ntrain[:3]\n\n{'image': [&lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n  &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n  &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;],\n 'label': [9, 0, 0]}\n\n\nThe type of label is an int but since fastai also performs Categorize transform we can create a separate target which contains the original apparel name. This can be achieved using ClassLabel.int2str method.\n\nclassLabel = ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)\n\ndef add_target(x:dict):\n    x['target'] = classLabel.int2str(x['label'])\n    return x\n\ntrain = train.map(lambda x: add_target(x))\nvalid = test.map(lambda x: add_target(x))\n\n\n\n\n\n\n\n\ntrain[:3], valid[:3]\n\n({'image': [&lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n   &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n   &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;],\n  'label': [9, 0, 0],\n  'target': ['Ankle boot', 'T - shirt / top', 'T - shirt / top']},\n {'image': [&lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n   &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n   &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;],\n  'label': [9, 2, 1],\n  'target': ['Ankle boot', 'Pullover', 'Trouser']})\n\n\nNow we can concatenate training and validation datasets into a single set of items which can be passed to fastai Datasets with index from 60000 to 70000 set aside as validation set.\n\nconcat_dsets = concatenate_datasets([train, valid])\n\n\nconcat_dsets\n\nDataset({\n    features: ['image', 'label', 'target'],\n    num_rows: 70000\n})\n\n\n\nAttempting to use DataBlock\nLet‚Äôs first create a DataBlock and then learn how to create Datasets. In order to inform fastai on how to turn the data into DataLoaders object, 4 key pieces of info are needed. 1. the kind of data used for inputs and the target 2. Getters for the list of items 3. Labeling the items 4. Validation set creation\n\n\n\n\n\n\nTip\n\n\n\nIt‚Äôs best to create DataBlock in an iterative manner and running datablock.summary to understand the pieces that fastai adds behind the scenes.\n\n\n\ndef get_image_attr(x): return x['image']\ndef get_target_attr(x): return x['target']\n\n\nImage.fromarray(array(train[0]['image']))\n\n\n\n\n\n\n\n\n\ndef image2tensor(img):\n    \"Transform image to byte tensor in `c*h*w` dim order.\"\n    res = tensor(img)\n    if res.dim()==2: res = res.unsqueeze(-1)\n    return res.permute(2,0,1)\n\n\ntype(image2tensor(train[0]['image']))\n\ntorch.Tensor\n\n\nWe will discuss image2tensor function when we discuss Attempting with Datasets\nThis shows how to specify the indices that need to be part of validation set. Indices 6, 7, 8 & 9 are in validation and rest are in training set.\n\nIndexSplitter(valid_idx=L(range(6, 10)))(concat_dsets)\n\n((#69996) [0,1,2,3,4,5,10,11,12,13...], (#4) [6,7,8,9])\n\n\nDataBlock definition is as follows\n\ndblock = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),\n                   get_x=Pipeline([get_image_attr, image2tensor]),\n                   get_y=get_target_attr,\n                   splitter=IndexSplitter(valid_idx=L(range(60000, 70000))),\n                   )\n\nRun the DataBlock.summary to understand how fastai set up the data pipeline and perform the necessary transforms.\n\n#Output cleared\n#dblock.summary(concat_dsets)\n\nTraceback while running dblock.summary(concat_dsets)\nSetting-up type transforms pipelines\nCollecting items from Dataset({\n    features: ['image', 'label', 'target'],\n    num_rows: 70000\n})\nFound 70000 items\n2 datasets of sizes 60000,10000\nSetting up Pipeline: get_image_attr -&gt; image2tensor -&gt; PILBase.create\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n~/miniconda3/envs/fastai/lib/python3.9/site-packages/PIL/Image.py in fromarray(obj, mode)\n   2812         try:\n-&gt; 2813             mode, rawmode = _fromarray_typemap[typekey]\n   2814         except KeyError as e:\n\nKeyError: ((1, 1, 28), '|u1')\n\nEssentially the data pipeline is as follows - Get the image attribute from the item, Convert the image to tensor. But fastai also adds the PILBase.create since we specified ImageBlock as our independent variable. This caused an issue KeyError: ((1, 1, 28), '|u1') due to Image.fromarray function used in PILBase.create. From https://stackoverflow.com/questions/57621092/keyerror-1-1-1280-u1-while-using-pils-image-fromarray-pil &gt; Pillow‚Äôs fromarray function can only do a MxNx3 array (RGB image), or an MxN array (grayscale).\n\nInternally fastai calls load_image with the item during the data pipeline creation as part of PILBase.create which fastai adds by default if we specify ImageBlock as part of blocks section in the DataBlock.\n\n\ndef load_image(fn, mode=None):\n    \"Open and load a `PIL.Image` and convert to `mode`\"\n    im = Image.open(fn)\n    im.load()\n    im = im._new(im.im)\n    return im.convert(mode) if mode else im\n\nload_image requires a filename but dataset already is in PIL.PngImagePlugin.PngImageFile\nImage.open(train[0]['image']) will lead to the following error from pillow library.\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-18-ed0e9de325b8&gt; in &lt;module&gt;\n----&gt; 1 Image.open(train[0]['image'])\n\n1 frames\n/usr/local/lib/python3.8/dist-packages/PIL/Image.py in __getattr__(self, name)\n    544             )\n    545             return self._category\n--&gt; 546         raise AttributeError(name)\n    547 \n    548     @property\n\n\n\nAttempt with MidLevel API Datasets\nWe want to convert the Image of size [28 ,28] into [1, 28, 28] as our end goal and decided to perform the item transforms using Mid Level API instead. - Datasets in MidLevel API provides more flexibility and full control over the individual item transforms performed.\n\nDatasets is part of MidLevel API allowing the user to customize the steps involved in data processing that are not possible with DataBlockAPI\n\nDatasets need the following pieces of information - raw items - the list of transforms that builds our inputs from the raw items - the list of transforms that builds our targets from the raw items - the split for training and validation\nFor a deeper dive, refer loading the data with mid level api section on Training Imagenette tutorial and Wayde Gilliam blog post\nLet‚Äôs iterate on the individual pieces of info.\nLet‚Äôs investigate the item transforms that we need for the Image. PyTorch Model expects the items to of type torch.Tensor. So I used ToTensor but it did not convert to tensors as I expected for the Image of type PIL.PngImagePlugin.PngImageFile. So I created img2tensor instead.\n\ntype(train[0]['image'])\n\nPIL.PngImagePlugin.PngImageFile\n\n\n\ntype(ToTensor()(train[0]['image']))\n\nPIL.PngImagePlugin.PngImageFile\n\n\nReferred what fastai does underneath using the source code. This is the function that takes the image and converts the image to a byte tensor of shape c*h*w ie channel x height x width\n\ndef image2tensor(img):\n    \"Transform image to byte tensor in `c*h*w` dim order.\"\n    res = tensor(img)\n    if res.dim()==2: res = res.unsqueeze(-1)\n    return res.permute(2,0,1)\n\n\ntype(image2tensor(train[0]['image']))\n\ntorch.Tensor\n\n\nI created my own version of it in img2tensor\n\ndef img2tensor(im: Image.Image):\n    return TensorImageBW(array(im)).unsqueeze(0)\n\n\nimg2tensor(train[0]['image']).shape\n\ntorch.Size([1, 28, 28])\n\n\n\ntensor(train[0]['image']).unsqueeze(-1).permute(2,0,1).shape\n\ntorch.Size([1, 28, 28])\n\n\n\nTensorImageBW(array(im)).unsqueeze(0).shape\n\ntorch.Size([1, 28, 28])\n\n\n\ntest_eq([1, 28, 28], img2tensor(train[0]['image']).shape)\ntest_eq([1, 28, 28], image2tensor(train[0]['image']).shape)\n\nAs you can see both image2tensor and img2tensor behaves the same way.Now the input item is ready. Let‚Äôs look at the target.\n\nconcat_dsets\n\nDataset({\n    features: ['image', 'label', 'target'],\n    num_rows: 70000\n})\n\n\nfasti internally converts the label and encodes them into numbers by creating a vocabulary of labels using Categorize transform.\n\nCategorize(vocab=ds_builder.info.features['label'].names, sort=False)\n\nCategorize -- {'vocab': ['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], 'sort': False, 'add_na': False}:\nencodes: (Tabular,object) -&gt; encodes\n(object,object) -&gt; encodes\ndecodes: (Tabular,object) -&gt; decodes\n(object,object) -&gt; decodes\n\n\nCreate the Datasets definition as follows\n\nsplits = IndexSplitter(valid_idx=L(range(60000, 70000)))(concat_dsets)\ndsets = Datasets(concat_dsets, \n                 [[get_image_attr], \n                  [get_target_attr, Categorize]],\n                 splits=splits)\n\nDefine the item transformations and batch transformations\n\nitem_tfms = [img2tensor] # convert PILImage to tensors\nbatch_tfms = [IntToFloatTensor] # convert the int tensors from images to floats, and divide every pixel by 255\ndls = dsets.dataloaders(after_item=item_tfms, after_batch=batch_tfms, bs=64, num_workers=8)\n\nVisualize the items in a batch\n\ndls.show_batch()\n\n\n\n\n\n\n\n\nVerify the items and their shapes in a single batch\n\nxb, yb = dls.one_batch()\n\n\nxb.shape, yb.shape\n\n(torch.Size([64, 1, 28, 28]), torch.Size([64]))\n\n\n\ndls.c # 10 classes as targets\n\n10\n\n\n\ndls.vocab # targets vocabulary\n\n['Ankle boot', 'Bag', 'Coat', 'Dress', 'Pullover', 'Sandal', 'Shirt', 'Sneaker', 'T - shirt / top', 'Trouser']"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#train-the-model",
    "href": "posts/2023-04-14-fashion-mnist.html#train-the-model",
    "title": "Image Classification with fastai‚Äôs MidLevel API",
    "section": "Train the Model",
    "text": "Train the Model\nFor training a image classification model, resnet architecture (a form on convolution neural network) is used as our backbone and fully connected (fc) linear layer as our head. In order for the linear layer to predict the outputs as one of the classes, pass the number of classes in order to configure the final layer. For overview review this resource about transfer learning and fine tuning.\nReview the model layers\n\n#gpu required\nmodel = resnet34(num_classes=dls.c).cuda()\n\n\n#gpu required\nmodel.avgpool, model.fc\n\n\n# Uncomment this line to validate if the model accepts single batch as input\n# model(xb)\n\nWe can access the convolutional layers as attributes of the model.\nInput to the convolutional layer is set as 3 channel (RGB) image but the images used as inputs are single channel images. Let‚Äôs update the input channel to single channel.\n\n#gpu required\nmodel.conv1\n\nConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n\n#gpu required\nmodel.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n#gpu required\nmodel.cuda();\n\n\n#gpu required\nmodel(xb);\n\n\nLearner\nLearner is a class that combines the ingredients such as data, model and metrics used to train a model.\n\nlearn = Learner(dls, model, metrics=accuracy)\n\npretrained=False type setting is used since the fashnion mnist is not similar dataset as Imagenet so keeping all the layers are trainable. If in case pretrained=True, we may want to freeze the layers except the head and do a bit of fine tuning the head first followed by unfreeze & then train all the layers.\nReview the learner summary to know about the input shape, output shape, different layers involved, parameters, trainable, Optimizer used and loss function used.\n\nlearn.summary()\n\n\n\n\n\n\n\n\nResNet (Input shape: 64 x 1 x 28 x 28)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     64 x 64 x 14 x 14   \nConv2d                                    3136       True      \nBatchNorm2d                               128        True      \nReLU                                                           \n____________________________________________________________________________\n                     64 x 64 x 7 x 7     \nMaxPool2d                                                      \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \n____________________________________________________________________________\n                     64 x 128 x 4 x 4    \nConv2d                                    73728      True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nConv2d                                    8192       True      \nBatchNorm2d                               256        True      \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \n____________________________________________________________________________\n                     64 x 256 x 2 x 2    \nConv2d                                    294912     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    32768      True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \n____________________________________________________________________________\n                     64 x 512 x 1 x 1    \nConv2d                                    1179648    True      \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nConv2d                                    131072     True      \nBatchNorm2d                               1024       True      \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nAdaptiveAvgPool2d                                              \n____________________________________________________________________________\n                     64 x 10             \nLinear                                    5130       True      \n____________________________________________________________________________\n\nTotal params: 21,283,530\nTotal trainable params: 21,283,530\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function Adam&gt;\nLoss function: FlattenedLoss of CrossEntropyLoss()\n\nCallbacks:\n  - TrainEvalCallback\n  - CastToTensor\n  - Recorder\n  - ProgressCallback\n\n\nLearning rate, a hyperparameter used for training can be determined using learning rate finder function in fastai\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0004786300996784121)\n\n\n\n\n\n\n\n\n\nTrain the deep learning model with 6 epochs and with the learning rate obtained from previous step\n\nlearn.fit_one_cycle(6, 5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.350417\n0.380862\n0.860200\n01:17\n\n\n1\n0.350359\n0.350519\n0.867500\n01:18\n\n\n2\n0.266588\n0.297127\n0.889900\n01:19\n\n\n3\n0.215826\n0.248561\n0.909700\n01:16\n\n\n4\n0.173092\n0.230798\n0.918000\n01:18\n\n\n5\n0.139866\n0.226217\n0.922900\n01:17\n\n\n\n\n\nModel is trained with an accuracy of 92.3%.\n\n\nExporting the trained model\n\n\n\n\n\n\nTip\n\n\n\nAvoid using lambda as Getters during data processing in order to export the model correctly.\n\n\nlearn.export(fname=‚Äòexport.pkl‚Äô) will lead to following Pickling error if lambda is used in the Data pipeline\ndsets = Datasets(concat_dsets, [[lambda x: x['image']], [lambda x: x['label'], Categorize]], splits=splits)\nPicklingError: Can‚Äôt pickle &lt;function  at 0x7f378c5aeaf0&gt;: attribute lookup  on main failed\n\n::: {#cell-119 .cell quarto-private-1='{\"key\":\"vscode\",\"value\":{\"languageId\":\"python\"}}'}\n``` {.python .cell-code}\nlearn.export(fname='export.pkl')\n:::"
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#inference",
    "href": "posts/2023-04-14-fashion-mnist.html#inference",
    "title": "Image Classification with fastai‚Äôs MidLevel API",
    "section": "Inference",
    "text": "Inference\nNow load the learner from the exported model. Predict the item ie image using Learner.predict. This function performs the necessary transforms on the item used as part of training using the learner.\n\nitem, expected = valid[0]['image'], valid[0]['target']; item, expected\n\n(&lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;, 'Ankle boot')\n\n\n\nlearn.predict(item)\n\n\n\n\n\n\n\n\n('9',\n tensor(9),\n tensor([5.0262e-13, 4.2399e-12, 1.5671e-10, 1.6854e-12, 4.1097e-14, 1.6369e-06,\n         1.2831e-10, 1.0092e-05, 1.8003e-10, 9.9999e-01]))\n\n\n\nlearn.dls.after_item, learn.dls.after_batch\n\n(Pipeline: img2tensor,\n Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1})\n\n\n\nRecreating Learner.predict from the source code\nWe can run learn.predict?? to examine what fastai does and review each line carefully.\ndef predict(self, item, rm_type_tfms=None, with_input=False):\n        dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)\n        inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n        i = getattr(self.dls, 'n_inp', -1)\n        inp = (inp,) if i==1 else tuplify(inp)\n        dec = self.dls.decode_batch(inp + tuplify(dec_preds))[0]\n        dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])\n        res = dec_targ,dec_preds[0],preds[0]\n        if with_input: res = (dec_inp,) + res\n        return res\nConverting the item by applying the transforms and create a dataloader out of it.\n\ndl = learn.dls.test_dl([item]); dl\n\n&lt;fastai.data.core.TfmdDL&gt;\n\n\nInternally learn.predict calls the get_preds method which accepts data loader and returns the input, predictions, the decoded predictions. This applies the same transforms done during training applied on the input during inference.\n\ninp, preds, _, decoded_preds = learn.get_preds(dl=dl, with_input=True, with_decoded=True)\n\n\n\n\n\n\n\n\nExamine the input shape and type\n\ninp.shape, type(inp)\n\n(torch.Size([1, 1, 28, 28]), fastai.torch_core.TensorImageBW)\n\n\n\ntuplify(decoded_preds)\n\n(tensor([9]),)\n\n\n\ntype(learn.dls.decode_batch((inp,) + tuplify(decoded_preds))), len(learn.dls.decode_batch((inp,) + tuplify(decoded_preds)))\n\n(fastcore.foundation.L, 1)\n\n\n\nimage, prediction = learn.dls.decode_batch((inp,) + tuplify(decoded_preds))[0]; prediction\n\n'9'\n\n\n\nprediction, decoded_preds\n\n('9', tensor([9]))\n\n\nLet‚Äôs examine fastai transforms done after item and after batch.\n\nlearn.dls.after_item, learn.dls.after_batch\n\n(Pipeline: img2tensor,\n Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1})\n\n\n\ntype_tfms = Pipeline([get_image_attr])\nitem_tfms = Pipeline([img2tensor])\nbatch_tfms = Pipeline([IntToFloatTensor])\n\n\ntrain[0]\n\n{'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n 'label': 9,\n 'is_valid': False}\n\n\n\nimg = type_tfms(train[0]);img.shape\n\n(28, 28)\n\n\n\nitem_tfms(img).shape\n\ntorch.Size([1, 28, 28])\n\n\n\nbatch_tfms(item_tfms(img).cuda()).shape\n\ntorch.Size([1, 28, 28])\n\n\nFrom the previous steps, we can uncover fastai magic such as transforms happening behind the scenes."
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#gradio-app-deployment",
    "href": "posts/2023-04-14-fashion-mnist.html#gradio-app-deployment",
    "title": "Image Classification with fastai‚Äôs MidLevel API",
    "section": "Gradio app deployment",
    "text": "Gradio app deployment\nLet‚Äôs deploy the trained model and inference functionality in Gradio app and host the app in HuggingFace Space.\nSteps followed - Create a new space in HF Space (Profile -&gt; New Space) - Upload the exported model export.pkl - Move all the necessary functions used as part of the transforms along with the inference provided below. This includes all the getters. - Add all the dependencies to requirements.txt - Create a gradio interface passing the classify function, specifying the inputs(Image) and outputs(Label) - See the complete Code & Space\n\nfrom fastai.vision.core import PILImageBW, TensorImageBW\nfrom datasets import ClassLabel\nimport gradio as gr\nfrom fastai.learner import load_learner\n\ndef get_image_attr(x): return x['image']\ndef get_target_attr(x): return x['target']\n\ndef img2tensor(im: Image.Image):\n    return TensorImageBW(array(im)).unsqueeze(0)\n\nclassLabel = ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)\n\ndef add_target(x:dict):\n    x['target'] = classLabel.int2str(x['label'])\n    return x\n\nlearn = load_learner('export.pkl', cpu=True)\n\ndef classify(inp):\n    img = PILImageBW.create(inp)\n    item = dict(image=img)\n    pred, _, _ = learn.predict(item)\n    return classLabel.int2str(int(pred))\n\n\nfname='shoes.jpg'\nclassify(fname)\n\n\n\n\n\n\n\n\n'Ankle boot'\n\n\n\nfname1 = 't-shirt.jpg'\nclassify(fname1)\n\n\n\n\n\n\n\n\n'T - shirt / top'\n\n\nNote: This is my project write up for WalkWithFastai revisited course as one of my goal for this course is to get comfortable with low level API, debug issues diving into the source, uncovering the fastai magic. Thanks to Zach Mueller for an excellent course."
  },
  {
    "objectID": "posts/2023-04-14-fashion-mnist.html#references",
    "href": "posts/2023-04-14-fashion-mnist.html#references",
    "title": "Image Classification with fastai‚Äôs MidLevel API",
    "section": "References",
    "text": "References\n\nhttps://store.walkwithfastai.com/walk-with-fastai-revisited\nhttps://walkwithfastai.com/MNIST\nhttps://github.com/fastai/fastbook/blob/master/11_midlevel_data.ipynb"
  },
  {
    "objectID": "posts/2020-05-25-Paper_Summary_Covidex.html",
    "href": "posts/2020-05-25-Paper_Summary_Covidex.html",
    "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
    "section": "",
    "text": "This is a paper summary of deploying a Neural Search Engine to answer questions from the COVID-19 dataset.\n\nPaper\ncovidex.ai\nTwitter Announcement by Jimmy Lin\n\n\nNeural Covidex applies state-of-the-art neural network models and artificial intelligence (AI) techniques to answer questions using the COVID-19 Open Research Dataset (CORD-19) provided by the Allen Institute for AI (data release of April 3, 2020). This project is led by Jimmy Lin from the University of Waterloo and Kyunghyun Cho from NYU, with a small team of wonderful students: Edwin Zhang and Nikhil Gupta. Special thanks to Colin Raffel for his help in pretraining T5 models for the biomedical domain.‚Äù\n\n\n\n\nalt text\n\n\n\n\n\nThe ongoing pandemic crisis poses a huge challenge to get timely information for public health officials, clinicians, researchers, virologists. In order to respond to this challenge, Allen AI publishes a COVID-19 data set (CORD-19) in collaboration with other research groups. The source for this data set is both research articles published about coronovirus and other related research articles. The aim of this effort is to bring researchers\n\nto apply language processing techniques in order to generate insights & make data driven decisions.\nto provide ways for the front line to consume the recent developments in a digestible form & apply in the field.\n\n\n\n\nJimmy Lin & his research team responded to this call. The two strategies adopted were\n\nReal time users should be able to find answers to any questions associated with COVID\nOther Researchers should be able reuse the components they build. Providing a modular and reusable components is set as part of the requirements.\n\nThe team decided to build end to end real time search application called covidex.ai . They developed the components that powers this engine in a short span of time for the information retrieval need .\n\nkeyword based search interface : This also provides faceted navigation in the form of filters like author, article source, time range and highlighting words from the results that matches with user query.\nneural ranking component that sorts the results with the top most results answering user‚Äôs question.\n\n\n\n\nTraditional search architecture comprises of two phases Content Indexing and Keyword Searching. During indexing, content is transformed into an Indexable form called as Document The search engine convert this document into a fundamental data structure called as Inverted Index. This is similar to what you see in Book Appendix where terms are mapped towards the pages. Similarly Inverted index contains terms mapped towards docIds where the term appears & position in the document.\nSearching phase is further divided into two stages retrieval stage and ranking stage. In the first stage, given a search term(s) you will retrieve the list of matching documents from the inverted index. In the second stage, the matched documents are sorted based on the computed relevance score.\nModern Search Architectures\nMore modern multi-stage search architectures from Bing & Alibaba expand the Search Phase with additional reranking stages. Except for the first retrieval stage, the additional subsequent ranking stages will rerank and refine the results further from previous stages .\n\n\n\n\n\nAnserini is an opensource Information Retrieval toolkit in order to solve the reproducibility problems in research and bridge the gap between research and real world systems. This is a tool that is built on top of Lucene, a popular open source search library & enhanced with features specific for conducting IR research. pyserini is a python interface to Anserini.\n\n\n\nThe first challenge faced by the team is representing the Indexable Document. This is fundamental unit of search engine. Results are basically collection of documents. Eg: Tweets, WebPage, Article\nOne of the common challenge wrt Information Retrieval systems, they tend to favor longer documents. In order to give all documents a fairer chances irrespective of its length, normalization is needed.\nThe articles are in the following format\n{\n  title : ‚ÄúImpact of  Masks‚Äù,\n  abstract: ‚ÄúMasks protects others from you.‚Äù\n  body_text : ‚ÄúEffectiveness of mask /n /n This is being studied ‚Ä¶..‚Äù\n}\nIn order to compare the effectiveness, the team decide to index the articles in 3 different ways\n\nIndex only the title and abstract of the article as a document\nIndex the entire text of the article as a document combining title, abstract and body_text\nBreak the body_text of the article into paragraphs and each paragraph as separate documents. In this case, the Indexable Document is title, abstract & the paragraph.\n\n\n\n\nOnce the team built the lucene indices based on the above scheme for CORD-19, we can able to search for a given term, retrieve matching documents and ranked them using BM25 scoring function. These indices are available for the researchers to perform further research.\nThe full search pipeline for keyword search is demonstrated using notebooks using Pyserini.\nIn order to provide the users a live system that can be used to answer questions, the team leveraged their earlier work on anserini integration with Solr , open source search platform. The user search interface is built using Blacklight discovery interface with Ruby on Rails for faceting & highlighting.\n\n\n\nalt text\n\n\n\n\n\nIn addition to that the team also built a highlighting feature on of keyword search. This allows the user to quickly scan the results with the matched keywords in the document.\nIt is built using BioBERT. The idea behind it is that a) the candidate matched documents & convert them into sentences. b) Similarly the query is treated as a sentence. The sentences & query are in turn converted into its numerical representations (vectors). Top sentences closer to the query are obtained using the cosine similarity. The top-K words in the context are highlighted in these top sentences.\n\n\n\n\nThe research group was already working on the neural architectures specifically applying transfer learning on retrieval/ranking based problems.\n\nBERT for query based Passage Ranking : Applying transfer learning for passage reranking pre-trained on MS-MARCO dataset\nBERTserini for retrieval-based question answering: Incorporating Anserini Retriever to retrieve the top K segments of text followed by BERT based pre-trained model to retrieve the answer span.\n\nTypically the task of reranking is turn the problem into a classification task where we take the query, candidate_document & predict the target as (relevant, not-relevant). To avoid the costly operation of performing classification on the entire corpus, this is applied at the reranking stages. The engine gets the top K documents from the previous retrieval stage and rerank them using machine learning model. As part of reranking stage, the team leveraged Sequence to Sequence Model for reranking (Nogueira et al.¬†2020).\nStages involved in training the reranking model using Transfer Learning Methodology\nPre-training ‚ÜíFine Tuning ‚Üí Inference\n\n[Pre-training] Transformer based Language Model trained on MS Marco dataset\n[Fine Tuning] Given a query q, document D, the model is fine tuned to predict the output as either true or false as targets indicating the relevance.\n[Inference] In reranking setting, for each candidate documents, predict the prob distribution of (relevant, non-relevant) and sort the scores of relevant doc(true outputs) alone.\n\nTraining a language model and the encoder from this fined tuned language model is normally used for the downstream tasks like Classification in transfer learning methodology. But this method of applying Sequence to Sequence model (based on T5) is quite new for document ranking setting (Nogueira et al., 2020).\nThe reasoning provided was the predicted target words can capture the relatedness through pre-training. This is based on encoder-decoder architecture & uses a similar masked language modeling objective. Given a query, document the model is fine tuned to produce true or false if the document is relevant or not to the query.\n\n\n\n\nThe authors rightfully mention that the individual components comprising such a system is evaluated against various test datasets. But as this is specific to an evolving dataset like CORD-19, there is no such existing test collections.\nIt is not always necessary that ranking is the most important for such an end to end system. We have to switch to an outcome based measure rather than a single output based measure like batch retrieval evaluations(MRR, nDCG). Eg: ‚ÄúDid the researchers, practitioners get their questions answered?‚Äù How many of them are not finding the answers? So involving human in the loop to qualitatively evaluate the results is essential to know if the system is really contributing towards the efforts fighting the pandemic.\nWhat if the exploratory users do not know the right type of keywords to use ? In that case ranking is a wrong goal to pursue.\nCurrent challenge is all the targeted users are working on the front line and hard to provide qualitative feedback about search experience. So the author asks for more hallway usability testing to gather insights from the users.\n\n\n\n\nAn end to end system like Covidex is not possible without the power of the current Open Source Software ecosystem, Open culture of curating, cleaning & sharing the data with the community (Thanks to CORD-19 by Allen AI) and pre-trained language models like MS-Marco etc.\nGood software engineering practices is the foundation for a team and ensure that the underlying software components can be replicated & reused to provide this system. This is essential to rapidly explore and experiment with new ideas.\nBuilding a strong research culture to produce the results in the form of open source software artifacts aid the community in reproducing the results and build on top of it.\nReminder about the mismatch between producing research code for conference and building a system for a real users. For example concerns like latency of search requests, throughput about the number of users, deploying & managing a system in production, user experience does not arise in a research setting.\n\n\n\n\nLack of proper training data & human annotators is a common challenge. Leveraging pre-trained models on MS-MARCO is critical for ranking tasks in this type of situation.\nThe experimentation mindset need to be adopted and one need to interactive computing tools like pyserini to experiment with search index. This allows the search practitioners to constantly iterate and learn from these experiments.\nAdoption of Openness in not only the source but science and data & the way we work is truly inspiring."
  },
  {
    "objectID": "posts/2020-05-25-Paper_Summary_Covidex.html#overview-of-covidex.ai",
    "href": "posts/2020-05-25-Paper_Summary_Covidex.html#overview-of-covidex.ai",
    "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
    "section": "",
    "text": "This is a paper summary of deploying a Neural Search Engine to answer questions from the COVID-19 dataset.\n\nPaper\ncovidex.ai\nTwitter Announcement by Jimmy Lin\n\n\nNeural Covidex applies state-of-the-art neural network models and artificial intelligence (AI) techniques to answer questions using the COVID-19 Open Research Dataset (CORD-19) provided by the Allen Institute for AI (data release of April 3, 2020). This project is led by Jimmy Lin from the University of Waterloo and Kyunghyun Cho from NYU, with a small team of wonderful students: Edwin Zhang and Nikhil Gupta. Special thanks to Colin Raffel for his help in pretraining T5 models for the biomedical domain.‚Äù\n\n\n\n\nalt text"
  },
  {
    "objectID": "posts/2020-05-25-Paper_Summary_Covidex.html#motivation",
    "href": "posts/2020-05-25-Paper_Summary_Covidex.html#motivation",
    "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
    "section": "",
    "text": "The ongoing pandemic crisis poses a huge challenge to get timely information for public health officials, clinicians, researchers, virologists. In order to respond to this challenge, Allen AI publishes a COVID-19 data set (CORD-19) in collaboration with other research groups. The source for this data set is both research articles published about coronovirus and other related research articles. The aim of this effort is to bring researchers\n\nto apply language processing techniques in order to generate insights & make data driven decisions.\nto provide ways for the front line to consume the recent developments in a digestible form & apply in the field."
  },
  {
    "objectID": "posts/2020-05-25-Paper_Summary_Covidex.html#outcomes",
    "href": "posts/2020-05-25-Paper_Summary_Covidex.html#outcomes",
    "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
    "section": "",
    "text": "Jimmy Lin & his research team responded to this call. The two strategies adopted were\n\nReal time users should be able to find answers to any questions associated with COVID\nOther Researchers should be able reuse the components they build. Providing a modular and reusable components is set as part of the requirements.\n\nThe team decided to build end to end real time search application called covidex.ai . They developed the components that powers this engine in a short span of time for the information retrieval need .\n\nkeyword based search interface : This also provides faceted navigation in the form of filters like author, article source, time range and highlighting words from the results that matches with user query.\nneural ranking component that sorts the results with the top most results answering user‚Äôs question."
  },
  {
    "objectID": "posts/2020-05-25-Paper_Summary_Covidex.html#background-related-work",
    "href": "posts/2020-05-25-Paper_Summary_Covidex.html#background-related-work",
    "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
    "section": "",
    "text": "Traditional search architecture comprises of two phases Content Indexing and Keyword Searching. During indexing, content is transformed into an Indexable form called as Document The search engine convert this document into a fundamental data structure called as Inverted Index. This is similar to what you see in Book Appendix where terms are mapped towards the pages. Similarly Inverted index contains terms mapped towards docIds where the term appears & position in the document.\nSearching phase is further divided into two stages retrieval stage and ranking stage. In the first stage, given a search term(s) you will retrieve the list of matching documents from the inverted index. In the second stage, the matched documents are sorted based on the computed relevance score.\nModern Search Architectures\nMore modern multi-stage search architectures from Bing & Alibaba expand the Search Phase with additional reranking stages. Except for the first retrieval stage, the additional subsequent ranking stages will rerank and refine the results further from previous stages ."
  },
  {
    "objectID": "posts/2020-05-25-Paper_Summary_Covidex.html#modular-and-reusable-keyword-search",
    "href": "posts/2020-05-25-Paper_Summary_Covidex.html#modular-and-reusable-keyword-search",
    "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
    "section": "",
    "text": "Anserini is an opensource Information Retrieval toolkit in order to solve the reproducibility problems in research and bridge the gap between research and real world systems. This is a tool that is built on top of Lucene, a popular open source search library & enhanced with features specific for conducting IR research. pyserini is a python interface to Anserini.\n\n\n\nThe first challenge faced by the team is representing the Indexable Document. This is fundamental unit of search engine. Results are basically collection of documents. Eg: Tweets, WebPage, Article\nOne of the common challenge wrt Information Retrieval systems, they tend to favor longer documents. In order to give all documents a fairer chances irrespective of its length, normalization is needed.\nThe articles are in the following format\n{\n  title : ‚ÄúImpact of  Masks‚Äù,\n  abstract: ‚ÄúMasks protects others from you.‚Äù\n  body_text : ‚ÄúEffectiveness of mask /n /n This is being studied ‚Ä¶..‚Äù\n}\nIn order to compare the effectiveness, the team decide to index the articles in 3 different ways\n\nIndex only the title and abstract of the article as a document\nIndex the entire text of the article as a document combining title, abstract and body_text\nBreak the body_text of the article into paragraphs and each paragraph as separate documents. In this case, the Indexable Document is title, abstract & the paragraph.\n\n\n\n\nOnce the team built the lucene indices based on the above scheme for CORD-19, we can able to search for a given term, retrieve matching documents and ranked them using BM25 scoring function. These indices are available for the researchers to perform further research.\nThe full search pipeline for keyword search is demonstrated using notebooks using Pyserini.\nIn order to provide the users a live system that can be used to answer questions, the team leveraged their earlier work on anserini integration with Solr , open source search platform. The user search interface is built using Blacklight discovery interface with Ruby on Rails for faceting & highlighting.\n\n\n\nalt text\n\n\n\n\n\nIn addition to that the team also built a highlighting feature on of keyword search. This allows the user to quickly scan the results with the matched keywords in the document.\nIt is built using BioBERT. The idea behind it is that a) the candidate matched documents & convert them into sentences. b) Similarly the query is treated as a sentence. The sentences & query are in turn converted into its numerical representations (vectors). Top sentences closer to the query are obtained using the cosine similarity. The top-K words in the context are highlighted in these top sentences."
  },
  {
    "objectID": "posts/2020-05-25-Paper_Summary_Covidex.html#neural-covidex-for-reranking",
    "href": "posts/2020-05-25-Paper_Summary_Covidex.html#neural-covidex-for-reranking",
    "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
    "section": "",
    "text": "The research group was already working on the neural architectures specifically applying transfer learning on retrieval/ranking based problems.\n\nBERT for query based Passage Ranking : Applying transfer learning for passage reranking pre-trained on MS-MARCO dataset\nBERTserini for retrieval-based question answering: Incorporating Anserini Retriever to retrieve the top K segments of text followed by BERT based pre-trained model to retrieve the answer span.\n\nTypically the task of reranking is turn the problem into a classification task where we take the query, candidate_document & predict the target as (relevant, not-relevant). To avoid the costly operation of performing classification on the entire corpus, this is applied at the reranking stages. The engine gets the top K documents from the previous retrieval stage and rerank them using machine learning model. As part of reranking stage, the team leveraged Sequence to Sequence Model for reranking (Nogueira et al.¬†2020).\nStages involved in training the reranking model using Transfer Learning Methodology\nPre-training ‚ÜíFine Tuning ‚Üí Inference\n\n[Pre-training] Transformer based Language Model trained on MS Marco dataset\n[Fine Tuning] Given a query q, document D, the model is fine tuned to predict the output as either true or false as targets indicating the relevance.\n[Inference] In reranking setting, for each candidate documents, predict the prob distribution of (relevant, non-relevant) and sort the scores of relevant doc(true outputs) alone.\n\nTraining a language model and the encoder from this fined tuned language model is normally used for the downstream tasks like Classification in transfer learning methodology. But this method of applying Sequence to Sequence model (based on T5) is quite new for document ranking setting (Nogueira et al., 2020).\nThe reasoning provided was the predicted target words can capture the relatedness through pre-training. This is based on encoder-decoder architecture & uses a similar masked language modeling objective. Given a query, document the model is fine tuned to produce true or false if the document is relevant or not to the query."
  },
  {
    "objectID": "posts/2020-05-25-Paper_Summary_Covidex.html#challenges-in-results-evaluation",
    "href": "posts/2020-05-25-Paper_Summary_Covidex.html#challenges-in-results-evaluation",
    "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
    "section": "",
    "text": "The authors rightfully mention that the individual components comprising such a system is evaluated against various test datasets. But as this is specific to an evolving dataset like CORD-19, there is no such existing test collections.\nIt is not always necessary that ranking is the most important for such an end to end system. We have to switch to an outcome based measure rather than a single output based measure like batch retrieval evaluations(MRR, nDCG). Eg: ‚ÄúDid the researchers, practitioners get their questions answered?‚Äù How many of them are not finding the answers? So involving human in the loop to qualitatively evaluate the results is essential to know if the system is really contributing towards the efforts fighting the pandemic.\nWhat if the exploratory users do not know the right type of keywords to use ? In that case ranking is a wrong goal to pursue.\nCurrent challenge is all the targeted users are working on the front line and hard to provide qualitative feedback about search experience. So the author asks for more hallway usability testing to gather insights from the users."
  },
  {
    "objectID": "posts/2020-05-25-Paper_Summary_Covidex.html#author-reflections",
    "href": "posts/2020-05-25-Paper_Summary_Covidex.html#author-reflections",
    "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
    "section": "",
    "text": "An end to end system like Covidex is not possible without the power of the current Open Source Software ecosystem, Open culture of curating, cleaning & sharing the data with the community (Thanks to CORD-19 by Allen AI) and pre-trained language models like MS-Marco etc.\nGood software engineering practices is the foundation for a team and ensure that the underlying software components can be replicated & reused to provide this system. This is essential to rapidly explore and experiment with new ideas.\nBuilding a strong research culture to produce the results in the form of open source software artifacts aid the community in reproducing the results and build on top of it.\nReminder about the mismatch between producing research code for conference and building a system for a real users. For example concerns like latency of search requests, throughput about the number of users, deploying & managing a system in production, user experience does not arise in a research setting."
  },
  {
    "objectID": "posts/2020-05-25-Paper_Summary_Covidex.html#insights-takeaways-from-this-paper",
    "href": "posts/2020-05-25-Paper_Summary_Covidex.html#insights-takeaways-from-this-paper",
    "title": "Paper Summary - Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset",
    "section": "",
    "text": "Lack of proper training data & human annotators is a common challenge. Leveraging pre-trained models on MS-MARCO is critical for ranking tasks in this type of situation.\nThe experimentation mindset need to be adopted and one need to interactive computing tools like pyserini to experiment with search index. This allows the search practitioners to constantly iterate and learn from these experiments.\nAdoption of Openness in not only the source but science and data & the way we work is truly inspiring."
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "",
    "text": "Enhancing Search Engine Performance: Addressing Vocabulary Mismatch with Machine Learning"
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html#doc2query",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html#doc2query",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "Doc2Query",
    "text": "Doc2Query\nMost of the below are excerpts from this paper for my own reference.\nReference: Paper\nAbstract: - One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents‚Äô content. - From the perspective of a question answering system, this might comprise questions the document can potentially answer. - Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. - By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. - In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.\n\n\n\ndoc-expansion-by-query-pred\n\n\nAdvantage\n\nprimary advantage of this approach is that expensive neural inference is pushed to indexing time,\n‚Äúbag of words‚Äù queries are against an inverted index built on the augmented document collection.\n\nTwo important observations - Model tends to copy some words from the input document (e.g., Washington DC, River, chromosome), meaning that it can effectively perform term re-weighting (i.e., increasing the importance of key terms). - Nevertheless, the model also produces words not present in the input document (e.g., weather, relationship), which can be characterized as expansion by synonyms and other related terms.\nQuote about T5 for doc expansion and the performance on BEIR dataset\n\nIn contrast, document expansion based docT5query is able to add new relevant keywords to a document and performs strong on the BEIR datasets. It outperforms BM25 on 11/18 datasets while providing a competitive performance on the remaining datasets."
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html#related",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html#related",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "Related",
    "text": "Related\n\nBEIR - Heterogeneous Benchmark for Information Retrieval.\nImprove text ranking with few shot promptin by Jo Kristen Bergum\nhttps://huggingface.co/blog/how-to-generate"
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html#code",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html#code",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "Code",
    "text": "Code\n\n%pip install transformers sentencepiece -qqq &gt; /dev/null\n\nHere we are using pretrained T5 model from HuggingFace for document expansion.\nAs quoted in the model page\n\nDocument expansion: You generate for your paragraphs 20-40 queries and index the paragraphs and the generates queries in a standard BM25 index like Elasticsearch, OpenSearch, or Lucene. The generated queries help to close the lexical gap of lexical search, as the generate queries contain synonyms. Further, it re-weights words giving important words a higher weight even if they appear seldomn in a paragraph. In our BEIR paper we showed that BM25+docT5query is a powerful search engine. In the BEIR repository we have an example how to use docT5query with Pyserini.\nDomain Specific Training Data Generation: It can be used to generate training data to learn an embedding model. On SBERT.net we have an example how to use the model to generate (query, text) pairs for a given collection of unlabeled texts. These pairs can then be used to train powerful dense embedding models.\n\n\n# https://huggingface.co/doc2query/stackexchange-title-body-t5-small-v1\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\nmodel_name = 'doc2query/stackexchange-title-body-t5-small-v1'\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\ntext = \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n\n\ninput_ids = tokenizer.encode(text, max_length=384, truncation=True, return_tensors='pt')\noutputs = model.generate(\n    input_ids=input_ids,\n    max_length=64,\n    do_sample=True,\n    top_p=0.95,\n    num_return_sequences=5)\n\nprint(\"Text:\")\nprint(text)\n\nprint(\"\\nGenerated Queries:\")\nfor i in range(len(outputs)):\n    query = tokenizer.decode(outputs[i], skip_special_tokens=True)\n    print(f'{i + 1}: {query}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText:\nPython is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n\nGenerated Queries:\n1: What is Python's design philosophy?\n2: What are the characteristics of Python?\n3: What does \"python's design philosophy\" mean?\n4: What is the difference between Python and Java?\n5: What is the logic behind Python's programming languages?\n\n\n\ndef generate(text: str):\n    input_ids = tokenizer.encode(text, max_length=384, truncation=True, return_tensors='pt')\n    outputs = model.generate(\n      input_ids=input_ids,\n      max_length=64,\n      do_sample=True,\n      top_p=0.95,\n      num_return_sequences=5)\n\n    print(\"Text:\")\n    print(text)\n\n    gen_texts = []\n    print(\"\\nGenerated Queries:\")\n    for i in range(len(outputs)):\n        query = tokenizer.decode(outputs[i], skip_special_tokens=True)\n        #print(f'{i + 1}: {query}')\n        gen_texts.append(query)\n    return gen_texts\n\n\ntext = 'In certain situations it is needed to have source code preprocessed by `gcc` without undergoing the full compilation process. For example, this might be necessary when embedded SQL is included in C or C++ programs and the preprocesssed file will be passed on to another tool which will convert the SQL in native source code.  - Red Hat Enterprise Linux (RHEL)   --- ***Disclaimer:** Links contained herein to an external website(s) are provided for convenience only. Red Hat has not reviewed the links and is not responsible for the content or its availability. The inclusion of any link to an external website does not imply endorsement by Red Hat of the website or their entities, products or services. You agree that Red Hat is not responsible or liable for any loss or expenses that may result due to your use of (or reliance on) the external site or content.*--- To make GCC stop after the preprocessing stage, use the option `-E`, as explained in [GNU- GCC options](https://gcc.gnu.org/onlinedocs/gcc/Overall-Options.html#Overall-Options). In other words, Using the `-E` parameter with `gcc` or `g++` will produce **only** the preprocessed source code: $ gcc-E program.c-o program.preprocessed The `program.preprocessed` file will contain the file preprocessed by `gcc` (`Macros` will be expanded and all include files will be resolved). This preprocessed output will contain lines such as the following ones: ~~~ # 131 &quot;/usr/include/bits/types.h&quot; 3 4 # 1 &quot;/usr/include/bits/typesizes.h&quot; 1 3 4 # 132 &quot;/usr/include/bits/types.h&quot; 2 3 4 ~~~ These lines are line markers that show from which include files specific source code was taken. If those lines are not desired run the same command adding the `-P` parameter: $ gcc-E-P program.c-o program.preprocessed'\ngenerate(text)\n\nText:\nIn certain situations it is needed to have source code preprocessed by `gcc` without undergoing the full compilation process. For example, this might be necessary when embedded SQL is included in C or C++ programs and the preprocesssed file will be passed on to another tool which will convert the SQL in native source code.  - Red Hat Enterprise Linux (RHEL)   --- ***Disclaimer:** Links contained herein to an external website(s) are provided for convenience only. Red Hat has not reviewed the links and is not responsible for the content or its availability. The inclusion of any link to an external website does not imply endorsement by Red Hat of the website or their entities, products or services. You agree that Red Hat is not responsible or liable for any loss or expenses that may result due to your use of (or reliance on) the external site or content.*--- To make GCC stop after the preprocessing stage, use the option `-E`, as explained in [GNU- GCC options](https://gcc.gnu.org/onlinedocs/gcc/Overall-Options.html#Overall-Options). In other words, Using the `-E` parameter with `gcc` or `g++` will produce **only** the preprocessed source code: $ gcc-E program.c-o program.preprocessed The `program.preprocessed` file will contain the file preprocessed by `gcc` (`Macros` will be expanded and all include files will be resolved). This preprocessed output will contain lines such as the following ones: ~~~ # 131 &quot;/usr/include/bits/types.h&quot; 3 4 # 1 &quot;/usr/include/bits/typesizes.h&quot; 1 3 4 # 132 &quot;/usr/include/bits/types.h&quot; 2 3 4 ~~~ These lines are line markers that show from which include files specific source code was taken. If those lines are not desired run the same command adding the `-P` parameter: $ gcc-E-P program.c-o program.preprocessed\n\nGenerated Queries:\n\n\n['Make GCC stop after preprocessing',\n 'GCC: how to stop \"gcc\" after the full compilation process?',\n 'Retrieving Source code preprocessed by gcc without undergoing full compilation process',\n \"Why can't I make GCC stop after the full compilation process?\",\n 'Can the -E parameter for gcc or g++ prevent any preprocessed source code from being loaded?']\n\n\n\ntext = 'Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6 ? * Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6? Fails with the following message ~~~ ftp://X.X.X.X/Bootstrap \\\\X86PC\\\\BStrap.0... Permission denied (0x0212603c) ~~~ * Red Hat Enterprise Linux 6 * KVM GPXe boot * Two dhcp servers on different subnet and are accessed via relay agetns First dhcp server assign IP address Second dhcp server provides next-server details  If gpxe is used to pxe-boot a KVM guest, it uses the next-server from the first dhcp offer and if it fails, then does not re-try with the next-server option provided in the second dhcp server offer packet Update gpxe packages to below versions or above.- gpxe-bootimgs-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-qemu-0.9.7-6.10.el6.noarch.rpm This has been fixed by errata http://rhn.redhat.com/errata/RHBA-2013-1628.html'\ngenerate(text)\n\nText:\nWhy gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6 ? * Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6? Fails with the following message ~~~ ftp://X.X.X.X/Bootstrap \\X86PC\\BStrap.0... Permission denied (0x0212603c) ~~~ * Red Hat Enterprise Linux 6 * KVM GPXe boot * Two dhcp servers on different subnet and are accessed via relay agetns First dhcp server assign IP address Second dhcp server provides next-server details  If gpxe is used to pxe-boot a KVM guest, it uses the next-server from the first dhcp offer and if it fails, then does not re-try with the next-server option provided in the second dhcp server offer packet Update gpxe packages to below versions or above.- gpxe-bootimgs-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-0.9.7-6.10.el6.noarch.rpm- gpxe-roms-qemu-0.9.7-6.10.el6.noarch.rpm This has been fixed by errata http://rhn.redhat.com/errata/RHBA-2013-1628.html\n\nGenerated Queries:\n\n\n['gpxe boot fails when next-server details are offered by the second dhcp server',\n 'gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6?',\n 'gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6',\n 'gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6?',\n 'Why gpxe boot fails when next-server details are offered by the second dhcp server in Red Hat Enterprise Linux 6?']\n\n\nIf I provide the title included in the contents, the generated text replicates the title.\n\ntext = 'Unable change permission to NFS share mounted at the client. * The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.'\ngenerate(text)\n\nText:\nUnable change permission to NFS share mounted at the client. * The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.\n\nGenerated Queries:\n\n\n['Unable change permission to NFS share mounted at the client',\n 'Unable change permission to NFS share mounted at the client',\n 'NFS client \"Unable change permission to NFS share mounted at the client.\"',\n 'Unable change permission to NFS share mounted at the client',\n 'Unable change permission to NFS share mounted at the client']\n\n\nExclude the title. Provides much better variations.\n\ntext = '* The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.'\ngenerate(text)\n\nText:\n* The permissions for files can be changed inside the NFS share, but the directory permissions cannot be changed, even by using root at the client. * Here is an example of the failure, which includes the NFS client mount options: ~~~ # mount | grep nfs nfsd on /proc/fs/nfsd type nfsd (rw) 1.1.1.253:/xyz/export on /opt/oracle/foobar002 type nfs (rw,user=oracle,noexec,nosuid,nodev,user,noac,nfsvers=3,tcp,rsize=1048576,wsize=1048576,addr=1.1.1.253) # ls-ld /opt/oracle/foobar002 drwxrwxrwx 2 root root 2048 Mar 19 09:52 /opt/oracle/foobar002 $ touch /opt/oracle/foobar002/oracle-test.txt $ ls-l /opt/oracle/foobar002/ total 0-rw-r--r-- 1 oracle oinstall 0 Mar 19 15:19 oracle-test.txt-rw-rw-r-- 1 myuidgid myuidgid 0 Mar 19 15:18 test-2.txt-rw-r--r-- 1 4294967294 4294967294 0 Mar 19 15:17 test.txt # chown oracle /opt/oracle/foobar002/ chown: changing ownership of `/opt/oracle/foobar002/&#039;: Operation not permitted ~~~  * Red Hat Enterprise Linux 5.6 * NFS client * NFS server * /etc/exports: ~~~ /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) ~~~  * Gather sosreport or export options (&quot;showmount-e&quot; from NFS client or /etc/export file from the NFS server). * Here were the options seen: /xyz/export *(sync,rw,root_squash,anonuid=4294967294,anongid=4294967294,no_subtree_check,anonuid=4294967294,anongid=4294967294,fsid=29247) * Noted the following: 1) Anongid/anonuid was used twice. 2) root_squash is used which means request from root will also be mapped to anonuid=429496729, so trying to change as root id will also not work. 3) check the correct anonuid and anongid both at the server and client side, they should match both on server as well as client side.  * The anonuid and anongid at the server and client side did not match. * Corrected the anonuid and anongid while exporting the NFS share at the NFS server.\n\nGenerated Queries:\n\n\n['SSH NFS clients permissions can be changed, even by using root',\n 'NFS: root permissions not changing',\n \"Why can't the directory permissions be changed in the NFS share?\",\n 'NFS share fails to resolve NFS: directory permissions cannot be changed',\n 'How can I set NFS permissions to be changed with root?']\n\n\n\ntext = '- qpid process segfaulting with backtrace attached- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - MRG Messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::FrameSet::getContentSize (this=0x128) at qpid/framing/FrameSet.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::QueuePolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/QueuePolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::Queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::Queue::dequeueCommitted (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ Complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached C++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) MRG 2.1 (that is qpid 0.14)'.lower()\ngenerate(text)\n\nText:\n- qpid process segfaulting with backtrace attached- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - mrg messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::frameset::getcontentsize (this=0x128) at qpid/framing/frameset.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::queuepolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/queuepolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::queue::dequeuecommitted (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached c++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) mrg 2.1 (that is qpid 0.14)\n\nGenerated Queries:\n\n\n['qpid process segfaulting with backtrace attached',\n 'qpid segfaulting with backtrace attached',\n 'Can anyone help me with my data segfaulting qpid transaction?',\n 'qpid segfault with backtrace attached- coredump has mrg messaging 2.0',\n 'qpid process segfaulting with backtrace attached - coredump']\n\n\nSome post-processing required such lower casing in order to generate unique text.\nExclude the issue statements\n\ntext = '- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - MRG Messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::FrameSet::getContentSize (this=0x128) at qpid/framing/FrameSet.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::QueuePolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/QueuePolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::Queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::Queue::dequeueCommitted (this=0x9e1ca0, msg=...) at qpid/broker/Queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ Complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached C++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) MRG 2.1 (that is qpid 0.14)'.lower()\n\ngenerate(text)\n\nText:\n- basic usage of the broker is sending and receiving messages in transactions, with optional message release or rejection in a consumer - mrg messaging 2.0 or older - coredump has this backtrace (particular line numbers refer to qpid 0.10): ~~~ #0 qpid::framing::frameset::getcontentsize (this=0x128) at qpid/framing/frameset.cpp:82 #1 0x00002aaaaae9ca9c in qpid::broker::queuepolicy::dequeued (this=0x9e29b0, m=...) at qpid/broker/queuepolicy.cpp:105 #2 0x00002aaaaae856b9 in qpid::broker::queue::dequeued (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:709 #3 0x00002aaaaae8a8a0 in qpid::broker::queue::dequeuecommitted (this=0x9e1ca0, msg=...) at qpid/broker/queue.cpp:685 #4 0x00002aaaaaef3bb5 in operator() (this=0x9e17c0) at /usr/include/boost/bind/mem_fn_template.hpp:104... ~~~ complete backtrace is attached- reproducer: * send a message to some queue * within a transaction: fetch a message, release it and commit the transaction/session- see attached c++ source code reproducer: ~~~./transacted_release ~~~ - there is a bug in processing message release within a transaction of a consumer- the released message is not moved away from the list of messages sent to the consumer within a transaction- so when the consumer commits the transaction, a removal of already released message is attempted, what fails with pointing to 0x0 address - upgrade to (at least) mrg 2.1 (that is qpid 0.14)\n\nGenerated Queries:\n\n\n['coredump for mrg messaging in coredump',\n 'coredump backtrace problem with messaging 2.0',\n 'mrg messaging, coredump and coredump',\n 'mrg messaging 1.1 and coredump: why is the backtrace not used?',\n 'Why has my backAmplitude (_0) returned?']\n\n\n\ntext = 'We are passing through a migration from EPP with SP 5.1 to EPP with SP 5.2.0 we noticed some strange behaviors. For example, the context menu in &quot;Content Explorer&quot; is all messed. How can we solve this? - Red Hat JBoss Portal Platform (JPP also known as EPP)- 5.2.0- Site Publisher (SP)- 5.2.0  In Site Publisher Groovy templates are also contents from JCR (JCR nodes). When we are passing through a migration we need to keep the JCR shared folder and do not change it. It causes that some Groovy templates from the old installation are not correctly replaced to the new ones, which leads to compatibility issues with EPP+SP 5.2. Upgrade to newest EPP 5.2.x release and [migrate all templates to the new version](https://access.redhat.com/site/solutions/85613).'\ngenerate(text)\n\nText:\nWe are passing through a migration from EPP with SP 5.1 to EPP with SP 5.2.0 we noticed some strange behaviors. For example, the context menu in &quot;Content Explorer&quot; is all messed. How can we solve this? - Red Hat JBoss Portal Platform (JPP also known as EPP)- 5.2.0- Site Publisher (SP)- 5.2.0  In Site Publisher Groovy templates are also contents from JCR (JCR nodes). When we are passing through a migration we need to keep the JCR shared folder and do not change it. It causes that some Groovy templates from the old installation are not correctly replaced to the new ones, which leads to compatibility issues with EPP+SP 5.2. Upgrade to newest EPP 5.2.x release and [migrate all templates to the new version](https://access.redhat.com/site/solutions/85613).\n\nGenerated Queries:\n\n\n['Migrate from EPP to SP 5.2 has strange behavior',\n 'Magento migration for a different version from SharePoint with new environment is messed up',\n 'Disable JCR contents with custom templates, SP 5.1 to EPP + SP 5.2.0',\n 'Migrating JCR and context menu messes',\n 'JCR menu of JCR-Node is corrupted']"
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html#refactor",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html#refactor",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "Refactor",
    "text": "Refactor\nRefactor the code for reusability and also instead use Auto classes from transformers to load the Sequence 2 Sequence model.\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nfrom typing import List\n\nclass QueryGenerator:\n    def __init__(self, model_path:str, use_fast:bool=False):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=use_fast)\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n        \n    def generate(self, text: str, max_length:int=384, output_length=64, num_return_sequences:int=5, top_p:float=0.95)-&gt;List[str]:\n        input_ids = self.tokenizer.encode(text, max_length=max_length, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            outputs = self.model.generate(\n                input_ids=input_ids,\n                max_length=output_length,\n                do_sample=True,\n                top_p=top_p,\n                num_return_sequences=num_return_sequences)\n        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n\ntext = 'In certain situations it is needed to have source code preprocessed by `gcc` without undergoing the full compilation process. For example, this might be necessary when embedded SQL is included in C or C++ programs and the preprocesssed file will be passed on to another tool which will convert the SQL in native source code.  - Red Hat Enterprise Linux (RHEL)   --- ***Disclaimer:** Links contained herein to an external website(s) are provided for convenience only. Red Hat has not reviewed the links and is not responsible for the content or its availability. The inclusion of any link to an external website does not imply endorsement by Red Hat of the website or their entities, products or services. You agree that Red Hat is not responsible or liable for any loss or expenses that may result due to your use of (or reliance on) the external site or content.*--- To make GCC stop after the preprocessing stage, use the option `-E`, as explained in [GNU- GCC options](https://gcc.gnu.org/onlinedocs/gcc/Overall-Options.html#Overall-Options). In other words, Using the `-E` parameter with `gcc` or `g++` will produce **only** the preprocessed source code: $ gcc-E program.c-o program.preprocessed The `program.preprocessed` file will contain the file preprocessed by `gcc` (`Macros` will be expanded and all include files will be resolved). This preprocessed output will contain lines such as the following ones: ~~~ # 131 &quot;/usr/include/bits/types.h&quot; 3 4 # 1 &quot;/usr/include/bits/typesizes.h&quot; 1 3 4 # 132 &quot;/usr/include/bits/types.h&quot; 2 3 4 ~~~ These lines are line markers that show from which include files specific source code was taken. If those lines are not desired run the same command adding the `-P` parameter: $ gcc-E-P program.c-o program.preprocessed'\nprint(\"Text:\")\nprint(text)\n\n\nprint(\"\\nGenerated Queries:\")\nmodel_name = 'doc2query/stackexchange-title-body-t5-small-v1'\nqGen = QueryGenerator(model_path=model_name)\nqGen.generate(text, num_return_sequences=5)"
  },
  {
    "objectID": "posts/2023-07-07-doc-expansion-by-query-pred.html#conclusion",
    "href": "posts/2023-07-07-doc-expansion-by-query-pred.html#conclusion",
    "title": "Document Expansion by Query Prediction to Improve Retrieval Effectiveness",
    "section": "Conclusion",
    "text": "Conclusion\nThis post should serve as the initial starting point for the Document Expansion technique and Refer the Colbert paper for the results using these approaches in standard benchmarks such as BEIR. In order to use these technique in your domain, index your documents using tools such as pyserini and compare with BM25 baseline."
  },
  {
    "objectID": "posts/2021-01-09-poetry-first-impressions.html",
    "href": "posts/2021-01-09-poetry-first-impressions.html",
    "title": "Poetry First Impressions",
    "section": "",
    "text": "Recently I inherited a python project which did not have dependency management setup and this post is a summary of my investigation using Poetry.\n\n\n\nDifferent python versions used by developers and in production.\nsetup.py not maintained\nDifferentiating the dev dependencies vs production dependencies is hard with a single requirements.txt file\nTracking the transitive dependencies.\nLocking the dependencies when we are ready to push the code.\n\n\n\n\n\nReproducibility and Consistency are the key aspects for maintaining the project.\nManaging dependencies in a python project and relying on a single tool (eg: pip, conda)\n\n\n\n\nPoetry\n\n\nPoetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you.\n\n\nGetting Started on Linux\n\n$ curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - - To start a new project poetry new &lt;project-name&gt;. If you have an existing project then poetry init - guide you through setting a project specific pyproject.toml config.\n\nPackage, version, description, author, compatible versions\nMain Dependencies\nDevelopment Dependencies\n\n\nFormat of pyproject.toml config\n[tool.poetry]\nname = \"language-detection\"\nversion = \"0.1.0\"\ndescription = \"Detects the language of the provided text using fasttext.\"\nauthors = [\"Your Name &lt;you@example.com&gt;\"]\n[tool.poetry.dependencies]\npython = \"^3.6\"\nfasttext = \"0.9.1\"\nFlask = \"1.1.1\"\ngunicorn = \"20.0.4\"\n[tool.poetry.dev-dependencies]\ncodecov = \"2.1.7\"\nflake8 = \"^3.8.4\"\nblack = \"^20.8b1\"\npytest = \"^6.2.1\"\npytest-cov = \"^2.10.1\"\n[build-system]\nrequires = [\"poetry-core&gt;=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\nManage virtual environments - Poetry automatically creates a virtual environments for you based on the project when you ran new or init. You can know more info using the following commands & use either poetry shell or source &lt;cache_dir&gt;/pypoetry/virtualenvs/&lt;project_venv&gt;/bin/activate to activate the environment. deactivate if you want to exit the environment.\n\n$  poetry env info\n\nVirtualenv\nPython:         3.8.6\nImplementation: CPython\nPath:           /home/msivanes/.cache/pypoetry/virtualenvs/language-detection-ddSi-Wir-py3.8\nValid:          True\n\nSystem\nPlatform: linux\nOS:       posix\nPython:   /usr\n\n$ poetry config --list\ncache-dir = \"/home/msivanes/.cache/pypoetry\"\nexperimental.new-installer = true\ninstaller.parallel = true\nvirtualenvs.create = true\nvirtualenvs.in-project = null\nvirtualenvs.path = \"{cache-dir}/virtualenvs\"  # /home/msivanes/.cache/pypoetry/virtualenvs\n\n\nAdd dependency through poetry add &lt;package&gt; or poetry add &lt;package&gt;@&lt;version&gt;\n$ poetry add Flask\n$ poetry add fasttext@0.9.1\n$ poetry add pytest\nRemove a dependency using poetry remove &lt;package&gt;\n$ poetry remove pyfasttext\nOnce you finish adding the dependencies, run poetry install to install all the dependencies in your project. This will generate the poetry.lock file. This is where all the dependencies are locked and needs to be version controlled. If any other developer run poetry install then poetry uses the exact same versions specified in the dependencies while installing. This ensures all the developers are using a consistent dependencies across the board.\nIf you want to upgrade the dependencies and use the latest versions, then use poetry update. This is the equivalent of deleting the poetry.lock and using install .\nExporting the requirements.txt if you want to containarize your app.\n$ poetry export --without-hashes -f requirements.txt &gt; requirements.txt\nFor scenarios in libraries where you want to perform an editable install and hence you need a setup.py file. (Github user @albireox provided a script to quickly create setup.py which I found it here\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# @Author: Jos√© S√°nchez-Gallego (gallegoj@uw.edu)\n# @Date: 2019-12-18\n# @Filename: create_setup.py\n# @License: BSD 3-clause (http://www.opensource.org/licenses/BSD-3-Clause)\n\n# This is a temporary solution for the fact that pip install . fails with\n# poetry when there is no setup.py and an extension needs to be compiled.\n# See https://github.com/python-poetry/poetry/issues/1516. Running this\n# script creates a setup.py filled out with information generated by\n# poetry when parsing the pyproject.toml.\n\nimport os\nimport sys\nfrom distutils.version import StrictVersion\n\n\n# If there is a global installation of poetry, prefer that.\nlib = os.path.expanduser('~/.poetry/lib')\nvendors = os.path.join(lib, 'poetry', '_vendor')\ncurrent_vendors = os.path.join(\n    vendors, 'py{}'.format('.'.join(str(v) for v in sys.version_info[:2]))\n)\n\nsys.path.insert(0, lib)\nsys.path.insert(0, current_vendors)\n\ntry:\n    try:\n        from poetry.core.factory import Factory\n        from poetry.core.masonry.builders.sdist import SdistBuilder\n    except (ImportError, ModuleNotFoundError):\n        from poetry.masonry.builders.sdist import SdistBuilder\n        from poetry.factory import Factory\n    from poetry.__version__ import __version__\nexcept (ImportError, ModuleNotFoundError) as ee:\n    raise ImportError('install poetry by doing pip install poetry to use '\n                      f'this script: {ee}')\n\n\n# Generate a Poetry object that knows about the metadata in pyproject.toml\nfactory = Factory()\npoetry = factory.create_poetry(os.path.dirname(__file__))\n\n# Use the SdistBuilder to genrate a blob for setup.py\nif StrictVersion(__version__) &gt;= StrictVersion('1.1.0b1'):\n    sdist_builder = SdistBuilder(poetry, None)\nelse:\n    sdist_builder = SdistBuilder(poetry, None, None)\n\nsetuppy_blob = sdist_builder.build_setup()\n\nwith open('setup.py', 'wb') as unit:\n    unit.write(setuppy_blob)\n    unit.write(b'\\n# This setup.py was autogenerated using poetry.\\n')\n\n\n\n\n\nI still have issues with pyenv which I need to figure how to easily switch between python versions\n\n\n\n\n\nPoetry Documentation"
  },
  {
    "objectID": "posts/2021-01-09-poetry-first-impressions.html#issues-happening-in-the-project",
    "href": "posts/2021-01-09-poetry-first-impressions.html#issues-happening-in-the-project",
    "title": "Poetry First Impressions",
    "section": "",
    "text": "Different python versions used by developers and in production.\nsetup.py not maintained\nDifferentiating the dev dependencies vs production dependencies is hard with a single requirements.txt file\nTracking the transitive dependencies.\nLocking the dependencies when we are ready to push the code."
  },
  {
    "objectID": "posts/2021-01-09-poetry-first-impressions.html#key-requirements",
    "href": "posts/2021-01-09-poetry-first-impressions.html#key-requirements",
    "title": "Poetry First Impressions",
    "section": "",
    "text": "Reproducibility and Consistency are the key aspects for maintaining the project.\nManaging dependencies in a python project and relying on a single tool (eg: pip, conda)\n\n\n\n\nPoetry\n\n\nPoetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you.\n\n\nGetting Started on Linux\n\n$ curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - - To start a new project poetry new &lt;project-name&gt;. If you have an existing project then poetry init - guide you through setting a project specific pyproject.toml config.\n\nPackage, version, description, author, compatible versions\nMain Dependencies\nDevelopment Dependencies\n\n\nFormat of pyproject.toml config\n[tool.poetry]\nname = \"language-detection\"\nversion = \"0.1.0\"\ndescription = \"Detects the language of the provided text using fasttext.\"\nauthors = [\"Your Name &lt;you@example.com&gt;\"]\n[tool.poetry.dependencies]\npython = \"^3.6\"\nfasttext = \"0.9.1\"\nFlask = \"1.1.1\"\ngunicorn = \"20.0.4\"\n[tool.poetry.dev-dependencies]\ncodecov = \"2.1.7\"\nflake8 = \"^3.8.4\"\nblack = \"^20.8b1\"\npytest = \"^6.2.1\"\npytest-cov = \"^2.10.1\"\n[build-system]\nrequires = [\"poetry-core&gt;=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\nManage virtual environments - Poetry automatically creates a virtual environments for you based on the project when you ran new or init. You can know more info using the following commands & use either poetry shell or source &lt;cache_dir&gt;/pypoetry/virtualenvs/&lt;project_venv&gt;/bin/activate to activate the environment. deactivate if you want to exit the environment.\n\n$  poetry env info\n\nVirtualenv\nPython:         3.8.6\nImplementation: CPython\nPath:           /home/msivanes/.cache/pypoetry/virtualenvs/language-detection-ddSi-Wir-py3.8\nValid:          True\n\nSystem\nPlatform: linux\nOS:       posix\nPython:   /usr\n\n$ poetry config --list\ncache-dir = \"/home/msivanes/.cache/pypoetry\"\nexperimental.new-installer = true\ninstaller.parallel = true\nvirtualenvs.create = true\nvirtualenvs.in-project = null\nvirtualenvs.path = \"{cache-dir}/virtualenvs\"  # /home/msivanes/.cache/pypoetry/virtualenvs\n\n\nAdd dependency through poetry add &lt;package&gt; or poetry add &lt;package&gt;@&lt;version&gt;\n$ poetry add Flask\n$ poetry add fasttext@0.9.1\n$ poetry add pytest\nRemove a dependency using poetry remove &lt;package&gt;\n$ poetry remove pyfasttext\nOnce you finish adding the dependencies, run poetry install to install all the dependencies in your project. This will generate the poetry.lock file. This is where all the dependencies are locked and needs to be version controlled. If any other developer run poetry install then poetry uses the exact same versions specified in the dependencies while installing. This ensures all the developers are using a consistent dependencies across the board.\nIf you want to upgrade the dependencies and use the latest versions, then use poetry update. This is the equivalent of deleting the poetry.lock and using install .\nExporting the requirements.txt if you want to containarize your app.\n$ poetry export --without-hashes -f requirements.txt &gt; requirements.txt\nFor scenarios in libraries where you want to perform an editable install and hence you need a setup.py file. (Github user @albireox provided a script to quickly create setup.py which I found it here\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# @Author: Jos√© S√°nchez-Gallego (gallegoj@uw.edu)\n# @Date: 2019-12-18\n# @Filename: create_setup.py\n# @License: BSD 3-clause (http://www.opensource.org/licenses/BSD-3-Clause)\n\n# This is a temporary solution for the fact that pip install . fails with\n# poetry when there is no setup.py and an extension needs to be compiled.\n# See https://github.com/python-poetry/poetry/issues/1516. Running this\n# script creates a setup.py filled out with information generated by\n# poetry when parsing the pyproject.toml.\n\nimport os\nimport sys\nfrom distutils.version import StrictVersion\n\n\n# If there is a global installation of poetry, prefer that.\nlib = os.path.expanduser('~/.poetry/lib')\nvendors = os.path.join(lib, 'poetry', '_vendor')\ncurrent_vendors = os.path.join(\n    vendors, 'py{}'.format('.'.join(str(v) for v in sys.version_info[:2]))\n)\n\nsys.path.insert(0, lib)\nsys.path.insert(0, current_vendors)\n\ntry:\n    try:\n        from poetry.core.factory import Factory\n        from poetry.core.masonry.builders.sdist import SdistBuilder\n    except (ImportError, ModuleNotFoundError):\n        from poetry.masonry.builders.sdist import SdistBuilder\n        from poetry.factory import Factory\n    from poetry.__version__ import __version__\nexcept (ImportError, ModuleNotFoundError) as ee:\n    raise ImportError('install poetry by doing pip install poetry to use '\n                      f'this script: {ee}')\n\n\n# Generate a Poetry object that knows about the metadata in pyproject.toml\nfactory = Factory()\npoetry = factory.create_poetry(os.path.dirname(__file__))\n\n# Use the SdistBuilder to genrate a blob for setup.py\nif StrictVersion(__version__) &gt;= StrictVersion('1.1.0b1'):\n    sdist_builder = SdistBuilder(poetry, None)\nelse:\n    sdist_builder = SdistBuilder(poetry, None, None)\n\nsetuppy_blob = sdist_builder.build_setup()\n\nwith open('setup.py', 'wb') as unit:\n    unit.write(setuppy_blob)\n    unit.write(b'\\n# This setup.py was autogenerated using poetry.\\n')"
  },
  {
    "objectID": "posts/2021-01-09-poetry-first-impressions.html#challenges",
    "href": "posts/2021-01-09-poetry-first-impressions.html#challenges",
    "title": "Poetry First Impressions",
    "section": "",
    "text": "I still have issues with pyenv which I need to figure how to easily switch between python versions"
  },
  {
    "objectID": "posts/2021-01-09-poetry-first-impressions.html#references",
    "href": "posts/2021-01-09-poetry-first-impressions.html#references",
    "title": "Poetry First Impressions",
    "section": "",
    "text": "Poetry Documentation"
  },
  {
    "objectID": "drafts/post-with-code/index.html",
    "href": "drafts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "drafts/vaccination_tweets_sp.html",
    "href": "drafts/vaccination_tweets_sp.html",
    "title": "Starter Notebook for Text Classification using fastai",
    "section": "",
    "text": "Dataset from the competition of our choice\n\nUnderstand the problem and the evaluation metric used\n\nSetup fastai, Config to store the constants and imports\nDownload the data\nCleaning\nUnderstand the structure of the data in order to setup the datablock\n\ndata is balanced or imbalanced\nwhere to fetch the data from\ntrain-test split, with stratify or cross-validation(kfold, stratifiedKFold)\ndecide if preprocessing is required\ndecide on the tokenization(or subword)\ndebugging help\n\nPost DataBlock setup\n\ncheck the one-batch\ncheck the summary\n\nLanguage Model\n\nAdd Mixed Precision Training\nAWD_LSTM\nintegrate callbacks for weights and biases\ntrain the language model using discriminative learning rates\nsave the encoder\n\nClassification Model\n\nuse the vocab from language model dataloaders\nuse the encoder on the learner\ntrain the model\ngradual unfreezing\n\nFuture\n\nHow to extend it further(by making it public - add Tendo‚Äôs share here)\nadd pratik and amit‚Äôs on structuring the project\n\nQuestions/Reference\n\nhow to train a backward language model\nhow to combine forward and backward language model\nHow to incorporate blurr along with fastai\nText Interpretation https://muellerzr.github.io/fastinference/text.inference/\nCaptum for text interpretation\nDiscriminative Learning Rate https://harish3110.github.io/through-tinted-lenses/natural%20language%20processing/sentiment%20analysis/2020/06/27/Introduction-to-NLP-using-Fastai.html#Discriminative-Fine-tuning\n\nhttps://github.com/muellerzr/Practical-Deep-Learning-For-Coders/blob/master/05a_NLP.ipynb\nMixup for AWD_LSTM https://github.com/fmcurti/MixUp-for-AWD_LSTM/blob/master/ULMFiT.ipynb\n\n\n\n%%capture\n!pip install git+https://github.com/fastai/fastai2.git -q\n!pip install git+https://github.com/fastai/fastcore.git -q\n\n!pip install -q nbdev\n!pip install -q azure-cognitiveservices-search-imagesearch sentencepiece\n\n# Upload utils.py from fastai repository\n!wget https://raw.githubusercontent.com/fastai/fastbook/master/utils.py\n\n\n# make your Google Drive accessible\nfrom google.colab import drive\ndrive.mount('/content/gdrive', force_remount=True)\n\nroot_dir = \"/content/gdrive/My Drive\"\ndrive_data = f\"{root_dir}/Colab Notebooks/data/vaccination_tweet\"\n\nGo to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n\nEnter your authorization code:\n¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\nMounted at /content/gdrive\n\n\n\nfrom fastai2.text.all import *\n\n\nc = Config()\nc['data_path'] = f'{drive_data}'\nc['model_path'] = f'{drive_data}/models'\nc['bs'] = 128\nc['seq_len'] = 72\nc.d\n\n{'archive_path': '/root/.fastai/archive',\n 'bs': '128',\n 'data_path': '/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet',\n 'model_path': '/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/models',\n 'seq_len': '72',\n 'storage_path': '/root/.fastai/data',\n 'version': 2}\n\n\n\nvaccination = Path(c.d['data_path']);\ntrain_df = pd.read_csv(vaccination/'Train.csv')\ntest_df = pd.read_csv(vaccination/'Test.csv')\nlen(train_df), len(test_df)\n\n(10001, 5177)\n\n\n\ntrain_df.dtypes\n\ntweet_id      object\nsafe_text     object\nlabel        float64\nagreement    float64\ndtype: object\n\n\n\ntrain_df.dropna(axis='rows', inplace=True);len(train_df)\n\n9999\n\n\n\n#train_df.fillna(value=' ', inplace=True)\ntest_df.fillna(value=' ', inplace=True)"
  },
  {
    "objectID": "drafts/vaccination_tweets_sp.html#outline",
    "href": "drafts/vaccination_tweets_sp.html#outline",
    "title": "Starter Notebook for Text Classification using fastai",
    "section": "",
    "text": "Dataset from the competition of our choice\n\nUnderstand the problem and the evaluation metric used\n\nSetup fastai, Config to store the constants and imports\nDownload the data\nCleaning\nUnderstand the structure of the data in order to setup the datablock\n\ndata is balanced or imbalanced\nwhere to fetch the data from\ntrain-test split, with stratify or cross-validation(kfold, stratifiedKFold)\ndecide if preprocessing is required\ndecide on the tokenization(or subword)\ndebugging help\n\nPost DataBlock setup\n\ncheck the one-batch\ncheck the summary\n\nLanguage Model\n\nAdd Mixed Precision Training\nAWD_LSTM\nintegrate callbacks for weights and biases\ntrain the language model using discriminative learning rates\nsave the encoder\n\nClassification Model\n\nuse the vocab from language model dataloaders\nuse the encoder on the learner\ntrain the model\ngradual unfreezing\n\nFuture\n\nHow to extend it further(by making it public - add Tendo‚Äôs share here)\nadd pratik and amit‚Äôs on structuring the project\n\nQuestions/Reference\n\nhow to train a backward language model\nhow to combine forward and backward language model\nHow to incorporate blurr along with fastai\nText Interpretation https://muellerzr.github.io/fastinference/text.inference/\nCaptum for text interpretation\nDiscriminative Learning Rate https://harish3110.github.io/through-tinted-lenses/natural%20language%20processing/sentiment%20analysis/2020/06/27/Introduction-to-NLP-using-Fastai.html#Discriminative-Fine-tuning\n\nhttps://github.com/muellerzr/Practical-Deep-Learning-For-Coders/blob/master/05a_NLP.ipynb\nMixup for AWD_LSTM https://github.com/fmcurti/MixUp-for-AWD_LSTM/blob/master/ULMFiT.ipynb\n\n\n\n%%capture\n!pip install git+https://github.com/fastai/fastai2.git -q\n!pip install git+https://github.com/fastai/fastcore.git -q\n\n!pip install -q nbdev\n!pip install -q azure-cognitiveservices-search-imagesearch sentencepiece\n\n# Upload utils.py from fastai repository\n!wget https://raw.githubusercontent.com/fastai/fastbook/master/utils.py\n\n\n# make your Google Drive accessible\nfrom google.colab import drive\ndrive.mount('/content/gdrive', force_remount=True)\n\nroot_dir = \"/content/gdrive/My Drive\"\ndrive_data = f\"{root_dir}/Colab Notebooks/data/vaccination_tweet\"\n\nGo to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n\nEnter your authorization code:\n¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\nMounted at /content/gdrive\n\n\n\nfrom fastai2.text.all import *\n\n\nc = Config()\nc['data_path'] = f'{drive_data}'\nc['model_path'] = f'{drive_data}/models'\nc['bs'] = 128\nc['seq_len'] = 72\nc.d\n\n{'archive_path': '/root/.fastai/archive',\n 'bs': '128',\n 'data_path': '/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet',\n 'model_path': '/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/models',\n 'seq_len': '72',\n 'storage_path': '/root/.fastai/data',\n 'version': 2}\n\n\n\nvaccination = Path(c.d['data_path']);\ntrain_df = pd.read_csv(vaccination/'Train.csv')\ntest_df = pd.read_csv(vaccination/'Test.csv')\nlen(train_df), len(test_df)\n\n(10001, 5177)\n\n\n\ntrain_df.dtypes\n\ntweet_id      object\nsafe_text     object\nlabel        float64\nagreement    float64\ndtype: object\n\n\n\ntrain_df.dropna(axis='rows', inplace=True);len(train_df)\n\n9999\n\n\n\n#train_df.fillna(value=' ', inplace=True)\ntest_df.fillna(value=' ', inplace=True)"
  },
  {
    "objectID": "drafts/vaccination_tweets_sp.html#dataloaders",
    "href": "drafts/vaccination_tweets_sp.html#dataloaders",
    "title": "Starter Notebook for Text Classification using fastai",
    "section": "DataLoaders",
    "text": "DataLoaders\n\ndoc(SentencePieceTokenizer)\n\nclass SentencePieceTokenizer[source]SentencePieceTokenizer(lang='en', special_toks=None, sp_model=None, vocab_sz=None, max_vocab_sz=30000, model_type='unigram', char_coverage=None, cache_dir='tmp')\n\nSpacy tokenizer for lang\nShow in docs\n\n\n\ntweet_lm = DataBlock(\n            blocks=TextBlock.from_df(text_cols='safe_text', is_lm=True, tok_func=SentencePieceTokenizer, model_type='bpe', max_vocab_sz=10000),\n            get_x=ColReader('text'),\n            splitter=RandomSplitter(valid_pct=0.15, seed=42)\n            )\n\n\ndf = pd.concat([train_df, test_df])\n\n\ndls = tweet_lm.dataloaders(source=df, bs=int(c.d['bs']), seq_len=int(c.d['seq_len']))\n\n\n\n\n\n\n\n\nlen(dls.vocab)\n\n6216"
  },
  {
    "objectID": "drafts/vaccination_tweets_sp.html#language-learner",
    "href": "drafts/vaccination_tweets_sp.html#language-learner",
    "title": "Starter Notebook for Text Classification using fastai",
    "section": "Language Learner",
    "text": "Language Learner\n\ndoc(language_model_learner)\n\nlanguage_model_learner[source]language_model_learner(dls, arch, config=None, drop_mult=1.0, pretrained=True, pretrained_fnames=None, loss_func=None, opt_func='Adam', lr=0.001, splitter='trainable_params', cbs=None, metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95))\n\nCreate a Learner with a language model from dls and arch.\nShow in docs\n\n\n\nlearn_lm = language_model_learner(dls, arch=AWD_LSTM, drop_mult=0.3, pretrained=True, metrics=[accuracy, Perplexity()])\n\n\nlearn_lm.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.2754228591918945, lr_steep=0.013182567432522774)\n\n\n\n\n\n\n\n\n\n\nlm_lr = 1e-2\n\n\nlearn_lm.fine_tune(8, base_lr=lm_lr)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n7.731276\n5.721750\n0.173679\n305.439056\n00:29\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n5.676833\n5.327257\n0.207547\n205.872421\n00:31\n\n\n1\n5.365718\n4.827284\n0.255697\n124.871391\n00:31\n\n\n2\n5.025544\n4.518177\n0.286770\n91.668335\n00:31\n\n\n3\n4.728319\n4.355146\n0.300401\n77.878227\n00:31\n\n\n4\n4.496605\n4.251612\n0.311808\n70.218529\n00:31\n\n\n5\n4.332778\n4.201892\n0.317858\n66.812614\n00:31\n\n\n6\n4.213473\n4.182868\n0.320936\n65.553589\n00:31\n\n\n7\n4.144303\n4.179251\n0.321221\n65.316925\n00:31\n\n\n\n\n\n\nlearn_lm.save('8_1e-2_lm_fine_tuned')\n\n\nMODEL_PATH = c.d['model_path']\n\n\nlearn_lm.save_encoder('8_1e-2_lm_fine_tuned_enc')\n\n\n#learn_lm.save_encoder(f'{MODEL_PATH}/8_1e-2_lm_bpe_fine_tuned_enc')"
  },
  {
    "objectID": "drafts/vaccination_tweets_sp.html#regression",
    "href": "drafts/vaccination_tweets_sp.html#regression",
    "title": "Starter Notebook for Text Classification using fastai",
    "section": "Regression",
    "text": "Regression\n\ntweet_cls = DataBlock(blocks=[TextBlock.from_df(text_cols='safe_text', vocab=dls.vocab, tok_func=SentencePieceTokenizer, model_type='bpe', max_vocab_sz=10000),\n                              RegressionBlock],\n                      get_x=ColReader(cols='text'),\n                      get_y=ColReader(cols='label'),\n                      splitter=RandomSplitter(valid_pct=0.2, seed=42))\n\n\ntweet_cls_dls = tweet_cls.dataloaders(source=train_df,verbose=True,\n                                      bs=int(c.d['bs']),\n                                      seq_len=int(c.d['seq_len']))\n\n\n\n\n\n\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: partial\nSetting up after_batch: Pipeline: \n\n\n\ntweet_cls_dls.show_batch(max_n=3)\n\n\n\n\n\ntext\ntext_\n\n\n\n\n0\n‚ñÅxxbos ‚ñÅ xxunk xxunk xxunk xxunk xxunk xxunk „ÅÆ „Å´ mmr xxunk xxunk xxunk xxunk „ÅÆ xxunk xxunk xxunk xxunk xxunk „Åü xxunk $ 550 „ÅÆ xxunk xxunk xxunk xxunk „Åü „ÄÇ ‚ñÅxxrep ‚ñÅ4 ‚ñÅ( ‚ñÅ; xxunk –¥ xxunk ‚ñÅxxrep ‚ñÅ7 ‚ñÅ ) ‚ñÅ xxunk xxunk xxunk xxunk xxunk xxunk xxunk „Å´ xxunk xxunk „Åó xxunk xxunk xxunk xxunk xxunk „Åü „ÄÇ xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk „ÄÇ\n0.0\n\n\n1\n‚ñÅxxbos ‚ñÅ... –≤ –∫ –æ –ª –æ –ª –∏ ‚ñÅ —Ç —É —Ç ‚ñÅ –º –Ω –µ ‚ñÅxxup ‚ñÅmmr - –ø —Ä –∏ –≤ –∏ –≤ –∫ —É ‚ñÅ... ‚ñÅ( measles - mumps - r ub ella ‚ñÅvaccine ) ‚ñÅ.... ‚ñÅ –≤ ‚ñÅ xxunk —É –¥ —É xxunk –µ –µ ‚ñÅ —Å –º –æ —Ç —Ä —é ‚ñÅ —Å ‚ñÅ –æ –ø —Ç –∏ –º –∏ –∑ –º –æ –º ‚ñÅ...\n0.0\n\n\n2\n‚ñÅxxbos ‚ñÅxxmaj ‚ñÅthe ‚ñÅxxup ‚ñÅmmr ‚ñÅmo ds p ace ‚ñÅxxup ‚ñÅm x - 5 ‚ñÅteam ‚ñÅon ‚ñÅthe ‚ñÅgr id ‚ñÅfor ‚ñÅthe ‚ñÅsecond ‚ñÅxxmaj ‚ñÅb atter y ‚ñÅxxmaj ‚ñÅtend er ‚ñÅrace ‚ñÅat ‚ñÅxxmaj ‚ñÅse br ing ! ‚ñÅ# ‚ñÅxxmaj ‚ñÅse br ing ‚ñÅ# ‚ñÅmo ds p ace ‚ñÅ# ‚ñÅma z da ‚ñÅ# ‚ñÅm x 5 c up ‚ñÅ# ‚ñÅglobal m x 5 c up ‚ñÅ# ‚ñÅsc ca\n0.0\n\n\n\n\n\n\ndoc(text_classifier_learner)\n\ntext_classifier_learner[source]text_classifier_learner(dls, arch, seq_len=72, config=None, pretrained=True, drop_mult=0.5, n_out=None, lin_ftrs=None, ps=None, max_len=1440, y_range=None, loss_func=None, opt_func='Adam', lr=0.001, splitter='trainable_params', cbs=None, metrics=None, path=None, model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95))\n\nCreate a Learner with a text classifier from dls and arch.\nShow in docs\n\n\n\nlearn_cls = text_classifier_learner(dls=tweet_cls_dls,\n                                    arch=AWD_LSTM,\n                                    loss_func=mse,                                    \n                                    metrics=rmse,\n                                    y_range=[-1, 1])\n\n\nlearn_cls.load_encoder(f'8_1e-2_lm_fine_tuned_enc')\n\n&lt;fastai2.text.learner.TextLearner at 0x7fce8db1a2b0&gt;\n\n\n\nlearn_cls.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.012022644281387329, lr_steep=5.754399353463668e-06)\n\n\n\n\n\n\n\n\n\n\nlearn_cls.fine_tune(epochs=8, base_lr=1e-2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\n_rmse\ntime\n\n\n\n\n0\n0.655240\n0.384711\n0.620251\n00:16\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\n_rmse\ntime\n\n\n\n\n0\n0.479516\n0.387409\n0.622422\n00:17\n\n\n1\n0.405052\n0.344687\n0.587101\n00:17\n\n\n2\n0.378217\n0.357070\n0.597553\n00:17\n\n\n3\n0.359391\n0.340642\n0.583646\n00:17\n\n\n4\n0.347887\n0.340789\n0.583771\n00:17\n\n\n5\n0.335765\n0.327030\n0.571865\n00:17\n\n\n6\n0.322824\n0.326958\n0.571802\n00:17\n\n\n7\n0.312481\n0.327277\n0.572081\n00:17\n\n\n\n\n\n\nlearn_cls.recorder.plot_loss()"
  },
  {
    "objectID": "drafts/vaccination_tweets_sp.html#inference---not-working",
    "href": "drafts/vaccination_tweets_sp.html#inference---not-working",
    "title": "Starter Notebook for Text Classification using fastai",
    "section": "Inference - NOT WORKING",
    "text": "Inference - NOT WORKING\n\ntest_df.rename(columns={'safe_text': 'text'}, inplace=True)\n\n\nlen(test_df)\n\n5177\n\n\n\ntest_df\n\n\n\n\n\n\n\n\ntweet_id\ntext\n\n\n\n\n0\n00BHHHP1\n&lt;user&gt; &lt;user&gt; ... &amp; 4 a vaccine given 2 healthy peeps, FDA think just not worth the AE risk unfortunately.\n\n\n1\n00UNMD0E\nStudents starting school without whooping cough vaccinations &lt;url&gt; #scpick\n\n\n2\n01AXPTJF\nI'm kinda over every ep of &lt;user&gt; being \"ripped from the headlines.\" Measles? Let's get back to crime. #SVU\n\n\n3\n01HOEQJW\nHow many innocent children die for lack of vaccination each year? Around 1.5 million. Too bad all their parents couldn't be here. #SB277\n\n\n4\n01JUKMAO\nCDC eyeing bird flu vaccine for humans, though risk is low: Federal officials said Wednesday they're taking steps‚Ä¶ &lt;url&gt;\n\n\n...\n...\n...\n\n\n5172\nZXVVNC5O\njenny mccarthy is on new years rockin eve. what has she done lately besides not vaccinate her kids and give us all goddamn polio??\n\n\n5173\nZYIANVI8\nMeasles reported in Clark Co. for 1st time since 2011 &lt;url&gt;\n\n\n5174\nZYITEHAH\n&lt;user&gt; issues alert regarding Measles in TX. Keep your DDx up to date, people! #Emergencymedicine\n\n\n5175\nZZ3BMBTG\nI can't believe people don't vaccinate their kids! I've been vaccinated for everything and then some.\n\n\n5176\nZZIYCVNH\n\"&lt;user&gt; Alternatives to #Flu Vaccine &lt;url&gt; #natural #health\" A good read with a few new tips &amp; many we #jerf folk know\n\n\n\n\n5177 rows √ó 2 columns\n\n\n\n\nsent_tfm = Tokenizer.from_df(text_cols='text', \n                             tok_func=SentencePieceTokenizer,\n                             model_type='bpe',\n                             max_vocab_sz=10000)\n\n\ntfms = [attrgetter('text'), sent_tfm, Numericalize()]\n\n\ndoc(Datasets)\n\nclass Datasets[source]Datasets(items=None, tfms=None, tls=None, n_inp=None, dl_type=None, use_list=None, do_setup=True, split_idx=None, train_setup=True, splits=None, types=None, verbose=False) :: FilteredBase\n\nA dataset that creates a tuple from each tfms, passed thru item_tfms\nShow in docs\n\n\n\ntest_dset = Datasets(items=test_df, tfms=[tfms], splits=None, dl_type=)\n\n\n\n\n\n\n\n\ntest_dl = test_dset.dataloaders(bs=int(c.d['bs']), seq_len=int(c.d['seq_len']))\n\n\nlearn_cls.dls.test_dl = test_dl\n\n\npredictions, _, targets = learn_cls.get_preds(dl=test_dl, with_decoded=True)\n\n\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/usr/local/lib/python3.6/dist-packages/fastai2/learner.py in _do_epoch_validate(self, ds_idx, dl)\n    182             self.dl = dl;                                    self('begin_validate')\n--&gt; 183             with torch.no_grad(): self.all_batches()\n    184         except CancelValidException:                         self('after_cancel_validate')\n\n/usr/local/lib/python3.6/dist-packages/fastai2/learner.py in all_batches(self)\n    152         self.n_iter = len(self.dl)\n--&gt; 153         for o in enumerate(self.dl): self.one_batch(*o)\n    154 \n\n/usr/local/lib/python3.6/dist-packages/fastai2/data/load.py in __iter__(self)\n     97         self.before_iter()\n---&gt; 98         for b in _loaders[self.fake_l.num_workers==0](self.fake_l):\n     99             if self.device is not None: b = to_device(b, self.device)\n\n/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py in __next__(self)\n    344     def __next__(self):\n--&gt; 345         data = self._next_data()\n    346         self._num_yielded += 1\n\n/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py in _next_data(self)\n    855                 del self._task_info[idx]\n--&gt; 856                 return self._process_data(data)\n    857 \n\n/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py in _process_data(self, data)\n    880         if isinstance(data, ExceptionWrapper):\n--&gt; 881             data.reraise()\n    882         return data\n\n/usr/local/lib/python3.6/dist-packages/torch/_utils.py in reraise(self)\n    393             msg = KeyErrorMessage(msg)\n--&gt; 394         raise self.exc_type(msg)\n\nRuntimeError: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 34, in fetch\n    data = next(self.dataset_iter)\n  File \"/usr/local/lib/python3.6/dist-packages/fastai2/data/load.py\", line 107, in create_batches\n    yield from map(self.do_batch, self.chunkify(res))\n  File \"/usr/local/lib/python3.6/dist-packages/fastai2/data/load.py\", line 128, in do_batch\n    def do_batch(self, b): return self.retain(self.create_batch(self.before_batch(b)), b)\n  File \"/usr/local/lib/python3.6/dist-packages/fastai2/data/load.py\", line 127, in create_batch\n    def create_batch(self, b): return (fa_collate,fa_convert)[self.prebatched](b)\n  File \"/usr/local/lib/python3.6/dist-packages/fastai2/data/load.py\", line 46, in fa_collate\n    else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n  File \"/usr/local/lib/python3.6/dist-packages/fastai2/data/load.py\", line 46, in &lt;listcomp&gt;\n    else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n  File \"/usr/local/lib/python3.6/dist-packages/fastai2/data/load.py\", line 45, in fa_collate\n    return (default_collate(t) if isinstance(b, _collate_types)\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 21 and 38 in dimension 1 at /pytorch/aten/src/TH/generic/THTensor.cpp:612\n\n\nDuring handling of the above exception, another exception occurred:\n\nIndexError                                Traceback (most recent call last)\n&lt;ipython-input-72-e92a753407a5&gt; in &lt;module&gt;()\n----&gt; 1 predictions, _, targets = learn_cls.get_preds(dl=test_dl, with_decoded=True)\n\n/usr/local/lib/python3.6/dist-packages/fastai2/learner.py in get_preds(self, ds_idx, dl, with_input, with_decoded, with_loss, act, inner, reorder, **kwargs)\n    227             for mgr in ctx_mgrs: stack.enter_context(mgr)\n    228             self(event.begin_epoch if inner else _before_epoch)\n--&gt; 229             self._do_epoch_validate(dl=dl)\n    230             self(event.after_epoch if inner else _after_epoch)\n    231             if act is None: act = getattr(self.loss_func, 'activation', noop)\n\n/usr/local/lib/python3.6/dist-packages/fastai2/learner.py in _do_epoch_validate(self, ds_idx, dl)\n    183             with torch.no_grad(): self.all_batches()\n    184         except CancelValidException:                         self('after_cancel_validate')\n--&gt; 185         finally:                                             self('after_validate')\n    186 \n    187     @log_args(but='cbs')\n\n/usr/local/lib/python3.6/dist-packages/fastai2/learner.py in __call__(self, event_name)\n    132     def ordered_cbs(self, event): return [cb for cb in sort_by_run(self.cbs) if hasattr(cb, event)]\n    133 \n--&gt; 134     def __call__(self, event_name): L(event_name).map(self._call_one)\n    135     def _call_one(self, event_name):\n    136         assert hasattr(event, event_name)\n\n/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in map(self, f, *args, **kwargs)\n    373              else f.format if isinstance(f,str)\n    374              else f.__getitem__)\n--&gt; 375         return self._new(map(g, self))\n    376 \n    377     def filter(self, f, negate=False, **kwargs):\n\n/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in _new(self, items, *args, **kwargs)\n    324     @property\n    325     def _xtra(self): return None\n--&gt; 326     def _new(self, items, *args, **kwargs): return type(self)(items, *args, use_list=None, **kwargs)\n    327     def __getitem__(self, idx): return self._get(idx) if is_indexer(idx) else L(self._get(idx), use_list=None)\n    328     def copy(self): return self._new(self.items.copy())\n\n/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in __call__(cls, x, *args, **kwargs)\n     45             return x\n     46 \n---&gt; 47         res = super().__call__(*((x,) + args), **kwargs)\n     48         res._newchk = 0\n     49         return res\n\n/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in __init__(self, items, use_list, match, *rest)\n    315         if items is None: items = []\n    316         if (use_list is not None) or not _is_array(items):\n--&gt; 317             items = list(items) if use_list else _listify(items)\n    318         if match is not None:\n    319             if is_coll(match): match = len(match)\n\n/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in _listify(o)\n    251     if isinstance(o, list): return o\n    252     if isinstance(o, str) or _is_array(o): return [o]\n--&gt; 253     if is_iter(o): return list(o)\n    254     return [o]\n    255 \n\n/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in __call__(self, *args, **kwargs)\n    217             if isinstance(v,_Arg): kwargs[k] = args.pop(v.i)\n    218         fargs = [args[x.i] if isinstance(x, _Arg) else x for x in self.pargs] + args[self.maxi+1:]\n--&gt; 219         return self.fn(*fargs, **kwargs)\n    220 \n    221 # Cell\n\n/usr/local/lib/python3.6/dist-packages/fastai2/learner.py in _call_one(self, event_name)\n    135     def _call_one(self, event_name):\n    136         assert hasattr(event, event_name)\n--&gt; 137         [cb(event_name) for cb in sort_by_run(self.cbs)]\n    138 \n    139     def _bn_bias_state(self, with_bias): return bn_bias_params(self.model, with_bias).map(self.opt.state)\n\n/usr/local/lib/python3.6/dist-packages/fastai2/learner.py in &lt;listcomp&gt;(.0)\n    135     def _call_one(self, event_name):\n    136         assert hasattr(event, event_name)\n--&gt; 137         [cb(event_name) for cb in sort_by_run(self.cbs)]\n    138 \n    139     def _bn_bias_state(self, with_bias): return bn_bias_params(self.model, with_bias).map(self.opt.state)\n\n/usr/local/lib/python3.6/dist-packages/fastai2/callback/core.py in __call__(self, event_name)\n     22         _run = (event_name not in _inner_loop or (self.run_train and getattr(self, 'training', True)) or\n     23                (self.run_valid and not getattr(self, 'training', False)))\n---&gt; 24         if self.run and _run: getattr(self, event_name, noop)()\n     25         if event_name=='after_fit': self.run=True #Reset self.run to True at each end of fit\n     26 \n\n/usr/local/lib/python3.6/dist-packages/fastai2/callback/core.py in after_validate(self)\n     94         \"Concatenate all recorded tensors\"\n     95         if self.with_input:     self.inputs  = detuplify(to_concat(self.inputs, dim=self.concat_dim))\n---&gt; 96         if not self.save_preds: self.preds   = detuplify(to_concat(self.preds, dim=self.concat_dim))\n     97         if not self.save_targs: self.targets = detuplify(to_concat(self.targets, dim=self.concat_dim))\n     98         if self.with_loss:      self.losses  = to_concat(self.losses)\n\n/usr/local/lib/python3.6/dist-packages/fastai2/torch_core.py in to_concat(xs, dim)\n    211 def to_concat(xs, dim=0):\n    212     \"Concat the element in `xs` (recursively if they are tuples/lists of tensors)\"\n--&gt; 213     if is_listy(xs[0]): return type(xs[0])([to_concat([x[i] for x in xs], dim=dim) for i in range_of(xs[0])])\n    214     if isinstance(xs[0],dict):  return {k: to_concat([x[k] for x in xs], dim=dim) for k in xs[0].keys()}\n    215     #We may receives xs that are not concatenatable (inputs of a text classifier for instance),\n\nIndexError: list index out of range\n\n\n\n\ntargets\n\ntensor([[-0.6514],\n        [ 0.5469],\n        [ 0.2638],\n        ...,\n        [ 0.2053],\n        [ 0.9290],\n        [ 0.6997]])\n\n\n\nvaccination.ls()\n\n(#4) [Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/SampleSubmission.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/Train.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/Test.csv'),Path('/content/gdrive/My Drive/Colab Notebooks/data/vaccination_tweet/models')]\n\n\n\nsubmission = pd.read_csv(vaccination/'SampleSubmission.csv')\n\n\nsubmission.head()\n\n\n\n\n\n\n\n\ntweet_id\nlabel\n\n\n\n\n0\n00BHHHP1\n0\n\n\n1\n00UNMD0E\n0\n\n\n2\n01AXPTJF\n0\n\n\n3\n01HOEQJW\n0\n\n\n4\n01JUKMAO\n0\n\n\n\n\n\n\n\n\nlen(submission), len(predictions)\n\n(5177, 5177)\n\n\n\nsubmission['label'] = predictions.flatten()\n\n\nsubmission\n\n\n\n\n\n\n\n\ntweet_id\nlabel\n\n\n\n\n0\n00BHHHP1\n-0.651409\n\n\n1\n00UNMD0E\n0.546905\n\n\n2\n01AXPTJF\n0.263799\n\n\n3\n01HOEQJW\n0.988286\n\n\n4\n01JUKMAO\n0.055132\n\n\n...\n...\n...\n\n\n5172\nZXVVNC5O\n0.890525\n\n\n5173\nZYIANVI8\n0.190089\n\n\n5174\nZYITEHAH\n0.205295\n\n\n5175\nZZ3BMBTG\n0.928989\n\n\n5176\nZZIYCVNH\n0.699665\n\n\n\n\n5177 rows √ó 2 columns\n\n\n\n\nsubmission.to_csv('submission.csv', index=False)"
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n\nWhen editing Jupyter notebook files (.ipynb), ALWAYS use the edit_notebook tool, never use search_replace or write tools. Notebooks are JSON files with a specific structure that requires special handling. See the ‚ÄúWorking with .ipynb Files‚Äù section below for detailed guidelines.\n\n\n\nChrestotes is a commonplace blog (personal knowledge repository) built with a dual publishing system: - Legacy system: fastpages (Jekyll-based) for backward compatibility - Current system: Quarto for modern publishing workflow\nThe blog focuses on machine learning, information retrieval, search systems (particularly Apache Solr), and NLP topics.\n\n\n\n\n\nquarto publish gh-pages --no-browser\nThis publishes to GitHub Pages using Quarto‚Äôs publish command.\n\n\n\nquarto preview\n\n\n\n# Start Jekyll server with live reload\nmake server\n\n# Start in detached mode\nmake server-detached\n\n# Convert notebooks/Word docs to posts\nmake convert\n\n# Rebuild Docker images (no cache)\nmake build\n\n# Rebuild with cache\nmake quick-build\n\n# Stop containers\nmake stop\n\n# Access notebook converter shell\nmake bash-nb\n\n# Access Jekyll server shell\nmake bash-jekyll\n\n# Restart Jekyll only\nmake restart-jekyll\n\n\n\n\n\n\nThe repository maintains two parallel post directories: - _posts/: Legacy fastpages/Jekyll posts - posts/: Current Quarto posts\nWhen creating new content, add to the posts/ directory.\n\n\n\n\nJupyter Notebooks (.ipynb) - Primary format for technical content\nMarkdown (.md) - For text-heavy posts\nQuarto Markdown (.qmd) - For Quarto-specific features\nWord Documents (.docx) - Legacy support via _word/ directory\n\n\n\n\nAll posts must follow Jekyll‚Äôs naming convention:\nYYYY-MM-DD-title-of-post.{ipynb,md,qmd}\nExample: 2025-11-22-hybrid-search-solr.ipynb\n\n\n\n\n\n\nIMPORTANT: When editing Jupyter notebooks (.ipynb files), always use the edit_notebook tool, NOT the regular search_replace or write tools. The edit_notebook tool properly handles the JSON structure of notebook files.\nKey considerations: - Notebooks are JSON files with a specific structure - Each cell has a cell_type (markdown, code, raw) and cell_language (python, markdown, etc.) - Always specify the correct cell_language when editing - Use is_new_cell: true to create new cells, false to edit existing ones - Provide sufficient context in old_string to uniquely identify the cell content\n\n\n\nQuarto is stricter about Markdown formatting than some other renderers. Follow these rules:\nSpacing Requirements: - Always include blank lines between different block elements (headings, paragraphs, lists) - After a heading, add a blank line before the next content - After a paragraph ending with a colon (:), add a blank line before a list - After bold text (**text**), add a blank line before lists or other content\nHeading Best Practices: - Use proper Markdown headings (##, ###, ####) instead of bold text (**text**) for section headings - Quarto renders headings better than bold text used as headings - Example: Use #### Stage 1: Title instead of **Stage 1: Title**\nList Formatting: - Ensure blank lines before and after lists - Use consistent indentation (2 spaces for nested lists) - Don‚Äôt mix list types without blank lines between them\nCode Blocks: - Use triple backticks with language identifiers: ```python, ```markdown, etc. - Ensure blank lines before and after code blocks\n\n\n\nFor Quarto notebooks, include YAML front matter in the first cell (cell 0):\n---\ntitle: \"Your Post Title\"\ndate: \"YYYY-MM-DD\"\ncategories:\n  - category1\n  - category2\ndescription: \"Brief description of the post\"\ntoc: true\n---\nLocation: First cell (index 0) should be a markdown cell with the front matter.\n\n\n\n\nFirst cell (index 0): Always markdown with YAML front matter\nSubsequent cells: Mix of markdown and code as needed\nMarkdown cells: Use for explanations, headings, and narrative\nCode cells: Use for executable code, examples, and demonstrations\n\n\n\n\nAfter editing notebooks: 1. Preview locally: quarto preview to see how Quarto renders the notebook 2. Check spacing: Verify that all sections render with proper spacing 3. Validate Markdown: Ensure lists, headings, and code blocks display correctly 4. Test code cells: If code cells are executable, verify they run correctly\n\n\n\nIssue: Lists not rendering properly - Fix: Add blank lines before and after lists - Fix: Ensure proper indentation (2 spaces per level)\nIssue: Headings not displaying correctly - Fix: Use proper Markdown headings (##, ###) instead of bold text - Fix: Add blank lines after headings\nIssue: Paragraphs and lists running together - Fix: Add blank lines between paragraphs and lists - Fix: Add blank line after colons before lists\nIssue: Code blocks not rendering - Fix: Ensure triple backticks are on separate lines - Fix: Add blank lines before and after code blocks\n\n\n\nFor notebooks in _notebooks/ (legacy system): - Avoid literal \\n characters being inserted instead of actual newlines - Test notebook output before committing - Preview rendered output locally before publishing\n\n\n\nAlways use uv for installing Python dependencies:\nuv pip install &lt;package&gt;\n\n\n\nAlways use uv for installing Python dependencies:\nuv pip install &lt;package&gt;\n\n\n\n\n\n\nQuarto workflow (current):\nposts/*.{ipynb,md,qmd} ‚Üí quarto render ‚Üí _site/ ‚Üí gh-pages branch\nfastpages workflow (legacy):\n_notebooks/*.ipynb ‚Üí nb2post.py ‚Üí _posts/*.md ‚Üí Jekyll ‚Üí _site/\n_word/*.docx ‚Üí word2post.py ‚Üí _posts/*.md ‚Üí Jekyll ‚Üí _site/\n\n\n\n\nnb2post.py: Converts Jupyter notebooks to Jekyll-compliant posts\nfast_template.py: Handles Jekyll naming conventions and front matter\nword2post.py: Converts Word documents to posts\naction_entrypoint.sh: Docker entrypoint for automated conversions\n\n\n\n\nThe docker-compose.yml defines three services: - converter: One-time conversion of notebooks/Word docs - watcher: Auto-converts notebooks on file changes - jekyll: Local development server (port 4000)\n\n\n\n\n\n_config.yml: Jekyll configuration (legacy)\n_quarto.yml: Quarto project configuration (current)\n_action_files/settings.ini: fastpages settings\nGemfile: Ruby dependencies for Jekyll\n\n\n\n\n\n.github/workflows/publish.yml: Quarto publishing to gh-pages (active)\n.github/workflows/ci.yaml: Legacy fastpages CI (retained)\n.github/workflows/gh-page.yaml: GitHub Pages build status check\n\n\n\n\n\n\n\nCreate file in posts/ directory with proper naming: YYYY-MM-DD-title.ipynb\nFor notebooks, include metadata/front matter if needed\nTest locally: quarto preview\nPublish: quarto publish gh-pages --no-browser\n\n\n\n\n\nStore notebooks in posts/ (current) or _notebooks/ (legacy)\nNotebooks in _notebooks/ are auto-converted to _posts/ via fastpages\nImages are automatically handled and saved to appropriate directories\n\n\n\n\nFor Quarto: quarto preview For Jekyll: make server (opens on http://localhost:4000)\n\n\n\n\n\nSite URL: https://manisnesan.github.io/chrestotes/\nBase URL: /chrestotes\nTheme: Minima (Jekyll), Cosmo (Quarto)\nMath Support: KaTeX\nSyntax Highlighting: Rouge (Jekyll), Dracula theme\n\n\n\n\nThe _notebooks/ directory contains symbolic links to external Python files (config.py, main.py) from the Pyserini project. These are development utilities and not part of the core blog infrastructure."
  },
  {
    "objectID": "CLAUDE.html#important-editing-.ipynb-files",
    "href": "CLAUDE.html#important-editing-.ipynb-files",
    "title": "CLAUDE.md",
    "section": "",
    "text": "When editing Jupyter notebook files (.ipynb), ALWAYS use the edit_notebook tool, never use search_replace or write tools. Notebooks are JSON files with a specific structure that requires special handling. See the ‚ÄúWorking with .ipynb Files‚Äù section below for detailed guidelines."
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Chrestotes is a commonplace blog (personal knowledge repository) built with a dual publishing system: - Legacy system: fastpages (Jekyll-based) for backward compatibility - Current system: Quarto for modern publishing workflow\nThe blog focuses on machine learning, information retrieval, search systems (particularly Apache Solr), and NLP topics."
  },
  {
    "objectID": "CLAUDE.html#publishing-commands",
    "href": "CLAUDE.html#publishing-commands",
    "title": "CLAUDE.md",
    "section": "",
    "text": "quarto publish gh-pages --no-browser\nThis publishes to GitHub Pages using Quarto‚Äôs publish command.\n\n\n\nquarto preview\n\n\n\n# Start Jekyll server with live reload\nmake server\n\n# Start in detached mode\nmake server-detached\n\n# Convert notebooks/Word docs to posts\nmake convert\n\n# Rebuild Docker images (no cache)\nmake build\n\n# Rebuild with cache\nmake quick-build\n\n# Stop containers\nmake stop\n\n# Access notebook converter shell\nmake bash-nb\n\n# Access Jekyll server shell\nmake bash-jekyll\n\n# Restart Jekyll only\nmake restart-jekyll"
  },
  {
    "objectID": "CLAUDE.html#content-structure",
    "href": "CLAUDE.html#content-structure",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The repository maintains two parallel post directories: - _posts/: Legacy fastpages/Jekyll posts - posts/: Current Quarto posts\nWhen creating new content, add to the posts/ directory.\n\n\n\n\nJupyter Notebooks (.ipynb) - Primary format for technical content\nMarkdown (.md) - For text-heavy posts\nQuarto Markdown (.qmd) - For Quarto-specific features\nWord Documents (.docx) - Legacy support via _word/ directory\n\n\n\n\nAll posts must follow Jekyll‚Äôs naming convention:\nYYYY-MM-DD-title-of-post.{ipynb,md,qmd}\nExample: 2025-11-22-hybrid-search-solr.ipynb"
  },
  {
    "objectID": "CLAUDE.html#working-with-.ipynb-files",
    "href": "CLAUDE.html#working-with-.ipynb-files",
    "title": "CLAUDE.md",
    "section": "",
    "text": "IMPORTANT: When editing Jupyter notebooks (.ipynb files), always use the edit_notebook tool, NOT the regular search_replace or write tools. The edit_notebook tool properly handles the JSON structure of notebook files.\nKey considerations: - Notebooks are JSON files with a specific structure - Each cell has a cell_type (markdown, code, raw) and cell_language (python, markdown, etc.) - Always specify the correct cell_language when editing - Use is_new_cell: true to create new cells, false to edit existing ones - Provide sufficient context in old_string to uniquely identify the cell content\n\n\n\nQuarto is stricter about Markdown formatting than some other renderers. Follow these rules:\nSpacing Requirements: - Always include blank lines between different block elements (headings, paragraphs, lists) - After a heading, add a blank line before the next content - After a paragraph ending with a colon (:), add a blank line before a list - After bold text (**text**), add a blank line before lists or other content\nHeading Best Practices: - Use proper Markdown headings (##, ###, ####) instead of bold text (**text**) for section headings - Quarto renders headings better than bold text used as headings - Example: Use #### Stage 1: Title instead of **Stage 1: Title**\nList Formatting: - Ensure blank lines before and after lists - Use consistent indentation (2 spaces for nested lists) - Don‚Äôt mix list types without blank lines between them\nCode Blocks: - Use triple backticks with language identifiers: ```python, ```markdown, etc. - Ensure blank lines before and after code blocks\n\n\n\nFor Quarto notebooks, include YAML front matter in the first cell (cell 0):\n---\ntitle: \"Your Post Title\"\ndate: \"YYYY-MM-DD\"\ncategories:\n  - category1\n  - category2\ndescription: \"Brief description of the post\"\ntoc: true\n---\nLocation: First cell (index 0) should be a markdown cell with the front matter.\n\n\n\n\nFirst cell (index 0): Always markdown with YAML front matter\nSubsequent cells: Mix of markdown and code as needed\nMarkdown cells: Use for explanations, headings, and narrative\nCode cells: Use for executable code, examples, and demonstrations\n\n\n\n\nAfter editing notebooks: 1. Preview locally: quarto preview to see how Quarto renders the notebook 2. Check spacing: Verify that all sections render with proper spacing 3. Validate Markdown: Ensure lists, headings, and code blocks display correctly 4. Test code cells: If code cells are executable, verify they run correctly\n\n\n\nIssue: Lists not rendering properly - Fix: Add blank lines before and after lists - Fix: Ensure proper indentation (2 spaces per level)\nIssue: Headings not displaying correctly - Fix: Use proper Markdown headings (##, ###) instead of bold text - Fix: Add blank lines after headings\nIssue: Paragraphs and lists running together - Fix: Add blank lines between paragraphs and lists - Fix: Add blank line after colons before lists\nIssue: Code blocks not rendering - Fix: Ensure triple backticks are on separate lines - Fix: Add blank lines before and after code blocks\n\n\n\nFor notebooks in _notebooks/ (legacy system): - Avoid literal \\n characters being inserted instead of actual newlines - Test notebook output before committing - Preview rendered output locally before publishing\n\n\n\nAlways use uv for installing Python dependencies:\nuv pip install &lt;package&gt;\n\n\n\nAlways use uv for installing Python dependencies:\nuv pip install &lt;package&gt;"
  },
  {
    "objectID": "CLAUDE.html#architecture",
    "href": "CLAUDE.html#architecture",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Quarto workflow (current):\nposts/*.{ipynb,md,qmd} ‚Üí quarto render ‚Üí _site/ ‚Üí gh-pages branch\nfastpages workflow (legacy):\n_notebooks/*.ipynb ‚Üí nb2post.py ‚Üí _posts/*.md ‚Üí Jekyll ‚Üí _site/\n_word/*.docx ‚Üí word2post.py ‚Üí _posts/*.md ‚Üí Jekyll ‚Üí _site/\n\n\n\n\nnb2post.py: Converts Jupyter notebooks to Jekyll-compliant posts\nfast_template.py: Handles Jekyll naming conventions and front matter\nword2post.py: Converts Word documents to posts\naction_entrypoint.sh: Docker entrypoint for automated conversions\n\n\n\n\nThe docker-compose.yml defines three services: - converter: One-time conversion of notebooks/Word docs - watcher: Auto-converts notebooks on file changes - jekyll: Local development server (port 4000)"
  },
  {
    "objectID": "CLAUDE.html#configuration-files",
    "href": "CLAUDE.html#configuration-files",
    "title": "CLAUDE.md",
    "section": "",
    "text": "_config.yml: Jekyll configuration (legacy)\n_quarto.yml: Quarto project configuration (current)\n_action_files/settings.ini: fastpages settings\nGemfile: Ruby dependencies for Jekyll"
  },
  {
    "objectID": "CLAUDE.html#github-actions-workflows",
    "href": "CLAUDE.html#github-actions-workflows",
    "title": "CLAUDE.md",
    "section": "",
    "text": ".github/workflows/publish.yml: Quarto publishing to gh-pages (active)\n.github/workflows/ci.yaml: Legacy fastpages CI (retained)\n.github/workflows/gh-page.yaml: GitHub Pages build status check"
  },
  {
    "objectID": "CLAUDE.html#common-development-tasks",
    "href": "CLAUDE.html#common-development-tasks",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Create file in posts/ directory with proper naming: YYYY-MM-DD-title.ipynb\nFor notebooks, include metadata/front matter if needed\nTest locally: quarto preview\nPublish: quarto publish gh-pages --no-browser\n\n\n\n\n\nStore notebooks in posts/ (current) or _notebooks/ (legacy)\nNotebooks in _notebooks/ are auto-converted to _posts/ via fastpages\nImages are automatically handled and saved to appropriate directories\n\n\n\n\nFor Quarto: quarto preview For Jekyll: make server (opens on http://localhost:4000)"
  },
  {
    "objectID": "CLAUDE.html#site-configuration",
    "href": "CLAUDE.html#site-configuration",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Site URL: https://manisnesan.github.io/chrestotes/\nBase URL: /chrestotes\nTheme: Minima (Jekyll), Cosmo (Quarto)\nMath Support: KaTeX\nSyntax Highlighting: Rouge (Jekyll), Dracula theme"
  },
  {
    "objectID": "CLAUDE.html#notes-on-symbolic-links",
    "href": "CLAUDE.html#notes-on-symbolic-links",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The _notebooks/ directory contains symbolic links to external Python files (config.py, main.py) from the Pyserini project. These are development utilities and not part of the core blog infrastructure."
  }
]